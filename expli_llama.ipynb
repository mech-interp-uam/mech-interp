{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretabilidad Mecanicista de Transformers\n",
    "\n",
    "## Introducci√≥n\n",
    "\n",
    "Este documento detalla el proceso para descargar e implementar el modelo **Llama 3.2 1B en fp16** desde **Hugging Face**, extrayendo la *n*-√©sima salida de la capa MLP (*Multi-Layer Perceptron*). Tambi√©n se documentar√° cada paso siguiendo una metodolog√≠a rigurosa.\n",
    "\n",
    "## ¬øQu√© es Llama 3.2 1B?\n",
    "\n",
    "Llama 3.2 es un modelo de lenguaje basado en la arquitectura **Transformer**, desarrollado por Meta. Su tama√±o (1B de par√°metros) lo hace eficiente para tareas de procesamiento del lenguaje natural.\n",
    "\n",
    "## ¬øQu√© es fp16 y por qu√© es importante?\n",
    "\n",
    "**fp16 (Floating Point 16 bits)** es un formato de precisi√≥n reducida que permite acelerar el entrenamiento y la inferencia del modelo, reduciendo el uso de memoria sin perder mucha precisi√≥n.\n",
    "\n",
    "## ¬øQu√© es Hugging Face?\n",
    "\n",
    "**Hugging Face** es una plataforma l√≠der en el desarrollo de modelos de inteligencia artificial, especialmente en el campo del procesamiento del lenguaje natural (NLP). Proporciona una gran variedad de modelos preentrenados, herramientas para el entrenamiento y despliegue de modelos, y una comunidad activa de investigadores y desarrolladores.\n",
    "\n",
    "### ¬øPara qu√© se usa Hugging Face?\n",
    "\n",
    "- **Repositorio de modelos:** Permite descargar y compartir modelos preentrenados.\n",
    "- **Transformers Library:** Proporciona una API para usar modelos de NLP f√°cilmente.\n",
    "- **Hugging Face Hub:** Un espacio para colaborar y almacenar modelos y datasets.\n",
    "- **Inference API:** Para probar modelos sin necesidad de descargarlos localmente.\n",
    "\n",
    "### Conceptos clave que usaremos en Hugging Face\n",
    "\n",
    "1. **Token de autenticaci√≥n:** Necesario para acceder a modelos privados o restringidos.\n",
    "2. **Modelos preentrenados:** Conjuntos de pesos y configuraciones listos para usar.\n",
    "3. **Tokenizer:** Convierte texto en tensores num√©ricos que el modelo puede procesar.\n",
    "4. **Pipeline:** Interfaz sencilla para ejecutar modelos en tareas espec√≠ficas.\n",
    "5. **AutoModel y AutoTokenizer:** Clases que nos permiten cargar modelos y tokenizadores sin necesidad de conocer su arquitectura exacta.\n",
    "\n",
    "---\n",
    "\n",
    "## Acceso al modelo en Hugging Face\n",
    "\n",
    "Para acceder a *Llama 3.2*, hay dos m√©todos principales:\n",
    "\n",
    "### M√©todo 1: Solicitud de acceso manual \n",
    "\n",
    "1. Ir a [Hugging Face](https://huggingface.co/).\n",
    "2. Usar la barra de b√∫squeda para encontrar **Llama 3.2 1B**.\n",
    "3. Llenar el formulario de solicitud de acceso.\n",
    "4. Una vez aprobado, podr√°s acceder a los archivos del modelo en la pesta√±a **Files and versions**.\n",
    "\n",
    "Este m√©todo es √∫til si el modelo tiene restricciones de acceso y no requiere autenticaci√≥n en c√≥digo.\n",
    "\n",
    "### M√©todo 2: Autenticaci√≥n con un token de acceso\n",
    "\n",
    "Si necesitas automatizar el proceso o descargar modelos privados, puedes generar un token de acceso:\n",
    "\n",
    "1. Ve a **Settings > Access Tokens** en Hugging Face.\n",
    "2. Genera un nuevo *token* con permisos de \"read\".\n",
    "3. Usa el siguiente c√≥digo para autenticarte:\n",
    "\n",
    "```python\n",
    "from huggingface_hub import login\n",
    "login(\"TU_TOKEN_AQUI\")  # Reemplaza con tu token\n",
    "```\n",
    "\n",
    "Este m√©todo es m√°s formal y √∫til si trabajas en servidores o con varios modelos en diferentes proyectos.\n",
    "\n",
    "---\n",
    "\n",
    "## Configuraci√≥n del entorno\n",
    "\n",
    "Instalaremos las dependencias necesarias en nuestro entorno de Python:\n",
    "\n",
    "```bash\n",
    "pip install torch torchvision torchaudio transformers huggingface_hub\n",
    "```\n",
    "\n",
    "Verificamos la instalaci√≥n ejecutando:\n",
    "\n",
    "```bash\n",
    "python3 -c \"import torch; print(torch.__version__)\"\n",
    "```\n",
    "\n",
    "Si el comando muestra un n√∫mero de versi√≥n (`2.x.x`), significa que **PyTorch est√° correctamente instalado**.\n",
    "\n",
    "Opcionalmente, podemos crear un entorno virtual (quiz√° esrte paso lo debas hacer primero para poder instalar paquetes, ya deoendera de tu gestor de paquetes que tengas en uso):\n",
    "\n",
    "```bash\n",
    "python -m venv llama_env\n",
    "source llama_env/bin/activate  # En macOS/Linux\n",
    "llama_env\\Scripts\\activate  # En Windows\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Descarga del modelo desde Hugging Face\n",
    "\n",
    "Cargaremos el modelo y el *tokenizer* desde Hugging Face:\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"  # Nombre exacto del modelo\n",
    "\n",
    "# Cargar el tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    print(\"Tokenizer cargado exitosamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar el tokenizer: {e}\")\n",
    "\n",
    "# Cargar el modelo completo\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    print(\"Modelo cargado exitosamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar el modelo: {e}\")\n",
    "```\n",
    "\n",
    "Si el modelo se descarga correctamente, estar√° listo para su an√°lisis y uso en inferencias. En caso de errores, verifica los permisos de acceso en Hugging Face o la correcta instalaci√≥n de las dependencias.\n",
    "\n",
    "---\n",
    "\n",
    "## Estructura del modelo Llama 3.2 1B\n",
    "\n",
    "Llama 3.2 1B sigue la arquitectura **Transformer**, que est√° compuesta por m√∫ltiples bloques de procesamiento de informaci√≥n. Sus principales componentes son:\n",
    "\n",
    "### üîπ 1. **Embeddings**\n",
    "Los embeddings convierten palabras o tokens en vectores num√©ricos de alta dimensi√≥n. Estos vectores son la entrada del modelo y representan el significado de las palabras en un espacio matem√°tico.\n",
    "\n",
    "### üîπ 2. **M√∫ltiples capas de atenci√≥n (Self-Attention)**\n",
    "Cada capa de atenci√≥n analiza las relaciones entre todas las palabras de la oraci√≥n para determinar cu√°les son m√°s relevantes para la predicci√≥n.\n",
    "\n",
    "### üîπ 3. **Capa MLP (Multi-Layer Perceptron)**\n",
    "Despu√©s de cada capa de atenci√≥n, los datos pasan por una **MLP (Red Neuronal de M√∫ltiples Capas)**. Esta capa:\n",
    "   - Refina la informaci√≥n extra√≠da por la atenci√≥n.\n",
    "   - Introduce no linealidad al modelo.\n",
    "   - Generaliza mejor las representaciones del lenguaje.\n",
    "\n",
    "Cada MLP en el Transformer tiene dos capas completamente conectadas con una funci√≥n de activaci√≥n no lineal intermedia.\n",
    "\n",
    "### üîπ 4. **Capa de salida**\n",
    "Finalmente, la capa de salida del modelo convierte la representaci√≥n final en una probabilidad de predicci√≥n sobre el siguiente token en la secuencia.\n",
    "\n",
    "### üîπ 5. **Estructura en profundidad**\n",
    "Llama 3.2 1B tiene varias capas Transformer apiladas, donde cada capa tiene una subcapa de **self-attention** y una subcapa MLP. Las activaciones intermedias dentro de la MLP son esenciales para analizar la informaci√≥n que el modelo est√° aprendiendo en cada paso.\n",
    "\n",
    "---\n",
    "\n",
    "## Guardar y cargar el modelo localmente\n",
    "\n",
    "Para evitar descargar el modelo cada vez que lo necesitemos, podemos almacenarlo localmente:\n",
    "\n",
    "```python\n",
    "# Guardar el modelo localmente\n",
    "model.save_pretrained(\"./llama3_model\")\n",
    "tokenizer.save_pretrained(\"./llama3_model\")\n",
    "\n",
    "# Cargarlo m√°s tarde sin conexi√≥n\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./llama3_model\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./llama3_model\")\n",
    "```\n",
    "\n",
    "Esto optimiza el tiempo y evita depender de internet en cada ejecuci√≥n.\n",
    "\n",
    "---\n",
    "\n",
    "## Generar texto con Llama 3.2\n",
    "\n",
    "Una vez que el modelo est√° cargado, podemos probarlo generando texto:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Definir el modelo\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# Cargar el tokenizer y el modelo\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Verificar si MPS est√° disponible y mover el modelo a GPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Tokenizar la entrada y mover a GPU\n",
    "prompt = \"Explica la importancia de la interpretabilidad en transformers.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generar texto con MPS activado\n",
    "print(\"Generando texto...\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_length=100,  # Reducido para mejorar velocidad\n",
    "        temperature=0.7,  # Controla la aleatoriedad\n",
    "        top_p=0.9  # Controla la diversidad de respuestas\n",
    "    )\n",
    "print(\"Generaci√≥n completada.\")\n",
    "\n",
    "# Decodificar la salida del modelo\n",
    "texto_generado = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Texto generado:\", texto_generado)\n",
    "\n",
    "```\n",
    "\n",
    "Este c√≥digo permite ver c√≥mo Llama 3.2 responde a una consulta en lenguaje natural.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorando la arquitectura interna de Llama 3.2 1B\n",
    "\n",
    "Antes de extraer salidas intermedias, es fundamental comprender c√≥mo est√° estructurado el modelo.\n",
    "\n",
    "## 1Ô∏è‚É£ Ver la configuraci√≥n general del modelo\n",
    "\n",
    "La configuraci√≥n del modelo contiene informaci√≥n clave como:\n",
    "- N√∫mero de capas (`num_hidden_layers`)\n",
    "- Dimensi√≥n de los embeddings (`hidden_size`)\n",
    "- N√∫mero de cabezas de atenci√≥n (`num_attention_heads`)\n",
    "- Tama√±o de la capa intermedia MLP (`intermediate_size`)\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Ver configuraci√≥n general del modelo\n",
    "print(model.config)\n",
    "```\n",
    "\n",
    "### Explicaci√≥n detallada de la configuraci√≥n del modelo:\n",
    "\n",
    "- **_name_or_path**: indica el nombre exacto del modelo que estamos utilizando.\n",
    "- **architectures**: muestra la clase principal usada para la arquitectura, en este caso `LlamaForCausalLM`.\n",
    "- **attention_bias**: especifica si hay un sesgo aplicado en las matrices de atenci√≥n (aqu√≠ es `false`).\n",
    "- **attention_dropout**: nivel de abandono (dropout) aplicado en la atenci√≥n (0.0 significa sin dropout).\n",
    "- **bos_token_id / eos_token_id**: identificadores especiales de inicio y fin de secuencia.\n",
    "- **head_dim**: tama√±o de la dimensi√≥n de cada cabeza de atenci√≥n.\n",
    "- **hidden_act**: funci√≥n de activaci√≥n en la MLP (aqu√≠ `silu`).\n",
    "- **hidden_size**: tama√±o de la representaci√≥n oculta, es decir, la dimensi√≥n del embedding (2048).\n",
    "- **initializer_range**: rango usado para inicializar los pesos.\n",
    "- **intermediate_size**: tama√±o de la capa intermedia de la MLP (8192).\n",
    "- **max_position_embeddings**: el m√°ximo n√∫mero de posiciones que puede procesar el modelo (131072).\n",
    "- **mlp_bias**: indica si la MLP usa sesgos (falso en este caso).\n",
    "- **model_type**: tipo de modelo (`llama`).\n",
    "- **num_attention_heads**: n√∫mero de cabezas de atenci√≥n (32).\n",
    "- **num_hidden_layers**: n√∫mero de capas Transformer (16).\n",
    "- **num_key_value_heads**: n√∫mero de cabezas para valores y llaves (8).\n",
    "- **pretraining_tp**: indica partici√≥n para entrenamiento (1 significa sin particiones).\n",
    "- **rms_norm_eps**: valor epsilon para estabilidad num√©rica en normalizaci√≥n.\n",
    "- **rope_scaling**: contiene par√°metros relacionados con el mecanismo de rotaci√≥n posicional (RoPE).\n",
    "- **rope_theta**: par√°metro adicional para la frecuencia rotacional (500000.0).\n",
    "- **tie_word_embeddings**: indica si las embeddings de entrada y salida est√°n ligadas.\n",
    "- **torch_dtype**: tipo de datos usado (`float32`).\n",
    "- **transformers_version**: versi√≥n de la librer√≠a Transformers usada (4.49.0).\n",
    "- **use_cache**: indica si se utiliza cache para acelerar inferencias.\n",
    "- **vocab_size**: tama√±o del vocabulario (128256 tokens).\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Ver cu√°ntos bloques Transformer tiene el modelo\n",
    "\n",
    "El modelo tiene una lista de capas accesible con `model.model.layers`. Cada elemento es un bloque Transformer completo.\n",
    "\n",
    "```python\n",
    "# Visualizar la lista de bloques Transformer\n",
    "print(model.model.layers)\n",
    "```\n",
    "\n",
    "### Estructura de las capas:\n",
    "\n",
    "La salida muestra una `ModuleList` con 16 bloques (`0-15`), cada uno compuesto por:\n",
    "\n",
    "- **LlamaAttention**: contiene 4 proyecciones lineales:\n",
    "  - `q_proj` (queries), `k_proj` (keys), `v_proj` (values), y `o_proj` (output).\n",
    "- **mlp (LlamaMLP)**: contiene tres capas lineales:\n",
    "  - `gate_proj` y `up_proj` (ambas proyectan de 2048 a 8192 dimensiones), y `down_proj` (reduce de 8192 a 2048).\n",
    "  - Una funci√≥n de activaci√≥n **SiLU()**.\n",
    "- **input_layernorm y post_attention_layernorm**: normalizaciones RMS aplicadas antes y despu√©s de la atenci√≥n.\n",
    "\n",
    "Ejemplo de bloque Transformer impreso:\n",
    "\n",
    "```python\n",
    "ModuleList(\n",
    "  (0-15): 16 x LlamaDecoderLayer(\n",
    "    (self_attn): LlamaAttention(\n",
    "      (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
    "      (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
    "      (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
    "      (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
    "    )\n",
    "    (mlp): LlamaMLP(\n",
    "      (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
    "      (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
    "      (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
    "      (act_fn): SiLU()\n",
    "    )\n",
    "    (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
    "    (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
    "  )\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Analizar un bloque Transformer espec√≠fico\n",
    "\n",
    "Para entender mejor la estructura interna, accedemos al primer bloque (√≠ndice 0):\n",
    "\n",
    "```python\n",
    "# Acceder al primer bloque Transformer\n",
    "primer_bloque = model.model.layers[0]\n",
    "\n",
    "# Ver la estructura interna del primer bloque\n",
    "print(primer_bloque)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ La MLP dentro de un bloque Transformer\n",
    "\n",
    "Cada bloque Transformer contiene un subm√≥dulo `mlp` que es un perceptr√≥n multicapa con dos capas lineales y una activaci√≥n no lineal.\n",
    "\n",
    "```python\n",
    "# Visualizar la estructura de la MLP dentro del primer bloque\n",
    "print(primer_bloque.mlp)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Resumen hasta aqu√≠:\n",
    "- El modelo Llama 3.2 1B tiene varios bloques Transformer.\n",
    "- Cada bloque contiene un m√≥dulo de autoatenci√≥n y una MLP.\n",
    "- La MLP procesa y refina la informaci√≥n extra√≠da por la autoatenci√≥n.\n",
    "- La comprensi√≥n de esta estructura es clave antes de extraer la salida de la MLP.\n",
    "\n",
    "En la siguiente secci√≥n veremos c√≥mo interceptar y extraer la salida de la MLP para la capa que deseemos (la *n*-√©sima salida).\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
