{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretabilidad Mecanicista de Transformers\n",
    "\n",
    "## Introducci贸n\n",
    "\n",
    "Este documento detalla el proceso para descargar e implementar el modelo **Llama 3.2 1B en fp16** desde **Hugging Face**, extrayendo la *n*-茅sima salida de la capa MLP (*Multi-Layer Perceptron*). Tambi茅n se documentar谩 cada paso siguiendo una metodolog铆a rigurosa.\n",
    "\n",
    "## 驴Qu茅 es Llama 3.2 1B?\n",
    "\n",
    "Llama 3.2 es un modelo de lenguaje basado en la arquitectura **Transformer**, desarrollado por Meta. Su tama帽o (1B de par谩metros) lo hace eficiente para tareas de procesamiento del lenguaje natural.\n",
    "\n",
    "## 驴Qu茅 es fp16 y por qu茅 es importante?\n",
    "\n",
    "**fp16 (Floating Point 16 bits)** es un formato de precisi贸n reducida que permite acelerar el entrenamiento y la inferencia del modelo, reduciendo el uso de memoria sin perder mucha precisi贸n.\n",
    "\n",
    "## 驴Qu茅 es Hugging Face?\n",
    "\n",
    "**Hugging Face** es una plataforma l铆der en el desarrollo de modelos de inteligencia artificial, especialmente en el campo del procesamiento del lenguaje natural (NLP). Proporciona una gran variedad de modelos preentrenados, herramientas para el entrenamiento y despliegue de modelos, y una comunidad activa de investigadores y desarrolladores.\n",
    "\n",
    "### 驴Para qu茅 se usa Hugging Face?\n",
    "\n",
    "- **Repositorio de modelos:** Permite descargar y compartir modelos preentrenados.\n",
    "- **Transformers Library:** Proporciona una API para usar modelos de NLP f谩cilmente.\n",
    "- **Hugging Face Hub:** Un espacio para colaborar y almacenar modelos y datasets.\n",
    "- **Inference API:** Para probar modelos sin necesidad de descargarlos localmente.\n",
    "\n",
    "### Conceptos clave que usaremos en Hugging Face\n",
    "\n",
    "1. **Token de autenticaci贸n:** Necesario para acceder a modelos privados o restringidos.\n",
    "2. **Modelos preentrenados:** Conjuntos de pesos y configuraciones listos para usar.\n",
    "3. **Tokenizer:** Convierte texto en tensores num茅ricos que el modelo puede procesar.\n",
    "4. **Pipeline:** Interfaz sencilla para ejecutar modelos en tareas espec铆ficas.\n",
    "5. **AutoModel y AutoTokenizer:** Clases que nos permiten cargar modelos y tokenizadores sin necesidad de conocer su arquitectura exacta.\n",
    "\n",
    "---\n",
    "\n",
    "## Acceso al modelo en Hugging Face\n",
    "\n",
    "Para acceder a *Llama 3.2*, hay dos m茅todos principales:\n",
    "\n",
    "### M茅todo 1: Solicitud de acceso manual \n",
    "\n",
    "1. Ir a [Hugging Face](https://huggingface.co/).\n",
    "2. Usar la barra de b煤squeda para encontrar **Llama 3.2 1B**.\n",
    "3. Llenar el formulario de solicitud de acceso.\n",
    "4. Una vez aprobado, podr谩s acceder a los archivos del modelo en la pesta帽a **Files and versions**.\n",
    "\n",
    "Este m茅todo es 煤til si el modelo tiene restricciones de acceso y no requiere autenticaci贸n en c贸digo.\n",
    "\n",
    "### M茅todo 2: Autenticaci贸n con un token de acceso\n",
    "\n",
    "Si necesitas automatizar el proceso o descargar modelos privados, puedes generar un token de acceso:\n",
    "\n",
    "1. Ve a **Settings > Access Tokens** en Hugging Face.\n",
    "2. Genera un nuevo *token* con permisos de \"read\".\n",
    "3. Usa el siguiente c贸digo para autenticarte:\n",
    "\n",
    "```python\n",
    "from huggingface_hub import login\n",
    "login(\"TU_TOKEN_AQUI\")  # Reemplaza con tu token\n",
    "```\n",
    "\n",
    "Este m茅todo es m谩s formal y 煤til si trabajas en servidores o con varios modelos en diferentes proyectos.\n",
    "\n",
    "---\n",
    "\n",
    "## Configuraci贸n del entorno\n",
    "\n",
    "Instalaremos las dependencias necesarias en nuestro entorno de Python:\n",
    "\n",
    "```bash\n",
    "pip install torch torchvision torchaudio transformers huggingface_hub\n",
    "```\n",
    "\n",
    "Verificamos la instalaci贸n ejecutando:\n",
    "\n",
    "```bash\n",
    "python3 -c \"import torch; print(torch.__version__)\"\n",
    "```\n",
    "\n",
    "Si el comando muestra un n煤mero de versi贸n (`2.x.x`), significa que **PyTorch est谩 correctamente instalado**.\n",
    "\n",
    "Opcionalmente, podemos crear un entorno virtual (quiz谩 esrte paso lo debas hacer primero para poder instalar paquetes, ya deoendera de tu gestor de paquetes que tengas en uso):\n",
    "\n",
    "```bash\n",
    "python -m venv llama_env\n",
    "source llama_env/bin/activate  # En macOS/Linux\n",
    "llama_env\\Scripts\\activate  # En Windows\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Descarga del modelo desde Hugging Face\n",
    "\n",
    "Cargaremos el modelo y el *tokenizer* desde Hugging Face:\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"  # Nombre exacto del modelo\n",
    "\n",
    "# Cargar el tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    print(\"Tokenizer cargado exitosamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar el tokenizer: {e}\")\n",
    "\n",
    "# Cargar el modelo completo\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    print(\"Modelo cargado exitosamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar el modelo: {e}\")\n",
    "```\n",
    "\n",
    "Si el modelo se descarga correctamente, estar谩 listo para su an谩lisis y uso en inferencias. En caso de errores, verifica los permisos de acceso en Hugging Face o la correcta instalaci贸n de las dependencias.\n",
    "\n",
    "---\n",
    "\n",
    "## Estructura del modelo Llama 3.2 1B\n",
    "\n",
    "Llama 3.2 1B sigue la arquitectura **Transformer**, que est谩 compuesta por m煤ltiples bloques de procesamiento de informaci贸n. Sus principales componentes son:\n",
    "\n",
    "###  1. **Embeddings**\n",
    "Los embeddings convierten palabras o tokens en vectores num茅ricos de alta dimensi贸n. Estos vectores son la entrada del modelo y representan el significado de las palabras en un espacio matem谩tico.\n",
    "\n",
    "###  2. **M煤ltiples capas de atenci贸n (Self-Attention)**\n",
    "Cada capa de atenci贸n analiza las relaciones entre todas las palabras de la oraci贸n para determinar cu谩les son m谩s relevantes para la predicci贸n.\n",
    "\n",
    "###  3. **Capa MLP (Multi-Layer Perceptron)**\n",
    "Despu茅s de cada capa de atenci贸n, los datos pasan por una **MLP (Red Neuronal de M煤ltiples Capas)**. Esta capa:\n",
    "   - Refina la informaci贸n extra铆da por la atenci贸n.\n",
    "   - Introduce no linealidad al modelo.\n",
    "   - Generaliza mejor las representaciones del lenguaje.\n",
    "\n",
    "Cada MLP en el Transformer tiene dos capas completamente conectadas con una funci贸n de activaci贸n no lineal intermedia.\n",
    "\n",
    "###  4. **Capa de salida**\n",
    "Finalmente, la capa de salida del modelo convierte la representaci贸n final en una probabilidad de predicci贸n sobre el siguiente token en la secuencia.\n",
    "\n",
    "###  5. **Estructura en profundidad**\n",
    "Llama 3.2 1B tiene varias capas Transformer apiladas, donde cada capa tiene una subcapa de **self-attention** y una subcapa MLP. Las activaciones intermedias dentro de la MLP son esenciales para analizar la informaci贸n que el modelo est谩 aprendiendo en cada paso.\n",
    "\n",
    "---\n",
    "\n",
    "## Guardar y cargar el modelo localmente\n",
    "\n",
    "Para evitar descargar el modelo cada vez que lo necesitemos, podemos almacenarlo localmente:\n",
    "\n",
    "```python\n",
    "# Guardar el modelo localmente\n",
    "model.save_pretrained(\"./llama3_model\")\n",
    "tokenizer.save_pretrained(\"./llama3_model\")\n",
    "\n",
    "# Cargarlo m谩s tarde sin conexi贸n\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./llama3_model\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./llama3_model\")\n",
    "```\n",
    "\n",
    "Esto optimiza el tiempo y evita depender de internet en cada ejecuci贸n.\n",
    "\n",
    "---\n",
    "\n",
    "## Generar texto con Llama 3.2\n",
    "\n",
    "Una vez que el modelo est谩 cargado, podemos probarlo generando texto:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Definir el modelo\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# Cargar el tokenizer y el modelo\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Verificar si MPS est谩 disponible y mover el modelo a GPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Tokenizar la entrada y mover a GPU\n",
    "prompt = \"Explica la importancia de la interpretabilidad en transformers.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generar texto con MPS activado\n",
    "print(\"Generando texto...\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_length=100,  # Reducido para mejorar velocidad\n",
    "        temperature=0.7,  # Controla la aleatoriedad\n",
    "        top_p=0.9  # Controla la diversidad de respuestas\n",
    "    )\n",
    "print(\"Generaci贸n completada.\")\n",
    "\n",
    "# Decodificar la salida del modelo\n",
    "texto_generado = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Texto generado:\", texto_generado)\n",
    "\n",
    "```\n",
    "\n",
    "Este c贸digo permite ver c贸mo Llama 3.2 responde a una consulta en lenguaje natural.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorando la arquitectura interna de Llama 3.2 1B\n",
    "\n",
    "Antes de extraer salidas intermedias, es fundamental comprender c贸mo est谩 estructurado el modelo.\n",
    "\n",
    "## 1锔 Ver la configuraci贸n general del modelo\n",
    "\n",
    "La configuraci贸n del modelo contiene informaci贸n clave como:\n",
    "- N煤mero de capas (`num_hidden_layers`)\n",
    "- Dimensi贸n de los embeddings (`hidden_size`)\n",
    "- N煤mero de cabezas de atenci贸n (`num_attention_heads`)\n",
    "- Tama帽o de la capa intermedia MLP (`intermediate_size`)\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Ver configuraci贸n general del modelo\n",
    "print(model.config)\n",
    "```\n",
    "\n",
    "### Explicaci贸n detallada de la configuraci贸n del modelo:\n",
    "\n",
    "- **_name_or_path**: indica el nombre exacto del modelo que estamos utilizando.\n",
    "- **architectures**: muestra la clase principal usada para la arquitectura, en este caso `LlamaForCausalLM`.\n",
    "- **attention_bias**: especifica si hay un sesgo aplicado en las matrices de atenci贸n (aqu铆 es `false`).\n",
    "- **attention_dropout**: nivel de abandono (dropout) aplicado en la atenci贸n (0.0 significa sin dropout).\n",
    "- **bos_token_id / eos_token_id**: identificadores especiales de inicio y fin de secuencia.\n",
    "- **head_dim**: tama帽o de la dimensi贸n de cada cabeza de atenci贸n.\n",
    "- **hidden_act**: funci贸n de activaci贸n en la MLP (aqu铆 `silu`).\n",
    "- **hidden_size**: tama帽o de la representaci贸n oculta, es decir, la dimensi贸n del embedding (2048).\n",
    "- **initializer_range**: rango usado para inicializar los pesos.\n",
    "- **intermediate_size**: tama帽o de la capa intermedia de la MLP (8192).\n",
    "- **max_position_embeddings**: el m谩ximo n煤mero de posiciones que puede procesar el modelo (131072).\n",
    "- **mlp_bias**: indica si la MLP usa sesgos (falso en este caso).\n",
    "- **model_type**: tipo de modelo (`llama`).\n",
    "- **num_attention_heads**: n煤mero de cabezas de atenci贸n (32).\n",
    "- **num_hidden_layers**: n煤mero de capas Transformer (16).\n",
    "- **num_key_value_heads**: n煤mero de cabezas para valores y llaves (8).\n",
    "- **pretraining_tp**: indica partici贸n para entrenamiento (1 significa sin particiones).\n",
    "- **rms_norm_eps**: valor epsilon para estabilidad num茅rica en normalizaci贸n.\n",
    "- **rope_scaling**: contiene par谩metros relacionados con el mecanismo de rotaci贸n posicional (RoPE).\n",
    "- **rope_theta**: par谩metro adicional para la frecuencia rotacional (500000.0).\n",
    "- **tie_word_embeddings**: indica si las embeddings de entrada y salida est谩n ligadas.\n",
    "- **torch_dtype**: tipo de datos usado (`float32`).\n",
    "- **transformers_version**: versi贸n de la librer铆a Transformers usada (4.49.0).\n",
    "- **use_cache**: indica si se utiliza cache para acelerar inferencias.\n",
    "- **vocab_size**: tama帽o del vocabulario (128256 tokens).\n",
    "\n",
    "---\n",
    "\n",
    "## 2锔 Ver cu谩ntos bloques Transformer tiene el modelo\n",
    "\n",
    "El modelo tiene una lista de capas accesible con `model.model.layers`. Cada elemento es un bloque Transformer completo.\n",
    "\n",
    "```python\n",
    "# Visualizar la lista de bloques Transformer\n",
    "print(model.model.layers)\n",
    "```\n",
    "\n",
    "### Estructura de las capas:\n",
    "\n",
    "La salida muestra una `ModuleList` con 16 bloques (`0-15`), cada uno compuesto por:\n",
    "\n",
    "- **LlamaAttention**: contiene 4 proyecciones lineales:\n",
    "  - `q_proj` (queries), `k_proj` (keys), `v_proj` (values), y `o_proj` (output).\n",
    "- **mlp (LlamaMLP)**: contiene tres capas lineales:\n",
    "  - `gate_proj` y `up_proj` (ambas proyectan de 2048 a 8192 dimensiones), y `down_proj` (reduce de 8192 a 2048).\n",
    "  - Una funci贸n de activaci贸n **SiLU()**.\n",
    "- **input_layernorm y post_attention_layernorm**: normalizaciones RMS aplicadas antes y despu茅s de la atenci贸n.\n",
    "\n",
    "Ejemplo de bloque Transformer impreso:\n",
    "\n",
    "```python\n",
    "ModuleList(\n",
    "  (0-15): 16 x LlamaDecoderLayer(\n",
    "    (self_attn): LlamaAttention(\n",
    "      (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
    "      (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
    "      (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
    "      (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
    "    )\n",
    "    (mlp): LlamaMLP(\n",
    "      (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
    "      (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
    "      (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
    "      (act_fn): SiLU()\n",
    "    )\n",
    "    (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
    "    (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
    "  )\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3锔 Analizar un bloque Transformer espec铆fico\n",
    "\n",
    "Para entender mejor la estructura interna, accedemos al primer bloque (铆ndice 0):\n",
    "\n",
    "```python\n",
    "# Acceder al primer bloque Transformer\n",
    "primer_bloque = model.model.layers[0]\n",
    "\n",
    "# Ver la estructura interna del primer bloque\n",
    "print(primer_bloque)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4锔 La MLP dentro de un bloque Transformer\n",
    "\n",
    "Cada bloque Transformer contiene un subm贸dulo `mlp` que es un perceptr贸n multicapa con dos capas lineales y una activaci贸n no lineal.\n",
    "\n",
    "```python\n",
    "# Visualizar la estructura de la MLP dentro del primer bloque\n",
    "print(primer_bloque.mlp)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    
