{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretabilidad Mecanicista de Transformers\n",
    "\n",
    "## Introducci贸n\n",
    "\n",
    "Este documento detalla el proceso para descargar e implementar el modelo **Llama 3.2 1B en fp16** desde **Hugging Face**, extrayendo la *n*-茅sima salida de la capa MLP (*Multi-Layer Perceptron*). Tambi茅n se documentar谩 cada paso siguiendo una metodolog铆a rigurosa.\n",
    "\n",
    "## 驴Qu茅 es Llama 3.2 1B?\n",
    "\n",
    "Llama 3.2 es un modelo de lenguaje basado en la arquitectura **Transformer**, desarrollado por Meta. Su tama帽o (1B de par谩metros) lo hace eficiente para tareas de procesamiento del lenguaje natural.\n",
    "\n",
    "## 驴Qu茅 es fp16 y por qu茅 es importante?\n",
    "\n",
    "**fp16 (Floating Point 16 bits)** es un formato de precisi贸n reducida que permite acelerar el entrenamiento y la inferencia del modelo, reduciendo el uso de memoria sin perder mucha precisi贸n.\n",
    "\n",
    "## 驴Qu茅 es Hugging Face?\n",
    "\n",
    "**Hugging Face** es una plataforma l铆der en el desarrollo de modelos de inteligencia artificial, especialmente en el campo del procesamiento del lenguaje natural (NLP). Proporciona una gran variedad de modelos preentrenados, herramientas para el entrenamiento y despliegue de modelos, y una comunidad activa de investigadores y desarrolladores.\n",
    "\n",
    "### 驴Para qu茅 se usa Hugging Face?\n",
    "\n",
    "- **Repositorio de modelos:** Permite descargar y compartir modelos preentrenados.\n",
    "- **Transformers Library:** Proporciona una API para usar modelos de NLP f谩cilmente.\n",
    "- **Hugging Face Hub:** Un espacio para colaborar y almacenar modelos y datasets.\n",
    "- **Inference API:** Para probar modelos sin necesidad de descargarlos localmente.\n",
    "\n",
    "### Conceptos clave que usaremos en Hugging Face\n",
    "\n",
    "1. **Token de autenticaci贸n:** Necesario para acceder a modelos privados o restringidos.\n",
    "2. **Modelos preentrenados:** Conjuntos de pesos y configuraciones listos para usar.\n",
    "3. **Tokenizer:** Convierte texto en tensores num茅ricos que el modelo puede procesar.\n",
    "4. **Pipeline:** Interfaz sencilla para ejecutar modelos en tareas espec铆ficas.\n",
    "5. **AutoModel y AutoTokenizer:** Clases que nos permiten cargar modelos y tokenizadores sin necesidad de conocer su arquitectura exacta.\n",
    "\n",
    "---\n",
    "\n",
    "## Acceso al modelo en Hugging Face\n",
    "\n",
    "Para acceder a *Llama 3.2*, hay dos m茅todos principales:\n",
    "\n",
    "### M茅todo 1: Solicitud de acceso manual \n",
    "\n",
    "1. Ir a [Hugging Face](https://huggingface.co/).\n",
    "2. Usar la barra de b煤squeda para encontrar **Llama 3.2 1B**.\n",
    "3. Llenar el formulario de solicitud de acceso.\n",
    "4. Una vez aprobado, podr谩s acceder a los archivos del modelo en la pesta帽a **Files and versions**.\n",
    "\n",
    "Este m茅todo es 煤til si el modelo tiene restricciones de acceso y no requiere autenticaci贸n en c贸digo.\n",
    "\n",
    "### M茅todo 2: Autenticaci贸n con un token de acceso\n",
    "\n",
    "Si necesitas automatizar el proceso o descargar modelos privados, puedes generar un token de acceso:\n",
    "\n",
    "1. Ve a **Settings > Access Tokens** en Hugging Face.\n",
    "2. Genera un nuevo *token* con permisos de \"read\".\n",
    "3. Usa el siguiente c贸digo para autenticarte:\n",
    "\n",
    "```python\n",
    "from huggingface_hub import login\n",
    "login(\"TU_TOKEN_AQUI\")  # Reemplaza con tu token\n",
    "```\n",
    "\n",
    "Este m茅todo es m谩s formal y 煤til si trabajas en servidores o con varios modelos en diferentes proyectos.\n",
    "\n",
    "---\n",
    "\n",
    "## Configuraci贸n del entorno\n",
    "\n",
    "Instalaremos las dependencias necesarias en nuestro entorno de Python:\n",
    "\n",
    "```bash\n",
    "pip install torch torchvision torchaudio transformers huggingface_hub\n",
    "```\n",
    "\n",
    "Verificamos la instalaci贸n ejecutando:\n",
    "\n",
    "```bash\n",
    "python3 -c \"import torch; print(torch.__version__)\"\n",
    "```\n",
    "\n",
    "Si el comando muestra un n煤mero de versi贸n (`2.x.x`), significa que **PyTorch est谩 correctamente instalado**.\n",
    "\n",
    "Opcionalmente, podemos crear un entorno virtual (quiz谩 esrte paso lo debas hacer primero para poder instalar paquetes, ya deoendera de tu gestor de paquetes que tengas en uso):\n",
    "\n",
    "```bash\n",
    "python -m venv llama_env\n",
    "source llama_env/bin/activate  # En macOS/Linux\n",
    "llama_env\\Scripts\\activate  # En Windows\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Descarga del modelo desde Hugging Face\n",
    "\n",
    "Cargaremos el modelo y el *tokenizer* desde Hugging Face:\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"  # Nombre exacto del modelo\n",
    "\n",
    "# Cargar el tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    print(\"Tokenizer cargado exitosamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar el tokenizer: {e}\")\n",
    "\n",
    "# Cargar el modelo completo\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    print(\"Modelo cargado exitosamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar el modelo: {e}\")\n",
    "```\n",
    "\n",
    "Si el modelo se descarga correctamente, estar谩 listo para su an谩lisis y uso en inferencias. En caso de errores, verifica los permisos de acceso en Hugging Face o la correcta instalaci贸n de las dependencias.\n",
    "\n",
    "---\n",
    "\n",
    "## Estructura del modelo Llama 3.2 1B\n",
    "\n",
    "Llama 3.2 1B sigue la arquitectura **Transformer**, que est谩 compuesta por m煤ltiples bloques de procesamiento de informaci贸n. Sus principales componentes son:\n",
    "\n",
    "###  1. **Embeddings**\n",
    "Los embeddings convierten palabras o tokens en vectores num茅ricos de alta dimensi贸n. Estos vectores son la entrada del modelo y representan el significado de las palabras en un espacio matem谩tico.\n",
    "\n",
    "###  2. **M煤ltiples capas de atenci贸n (Self-Attention)**\n",
    "Cada capa de atenci贸n analiza las relaciones entre todas las palabras de la oraci贸n para determinar cu谩les son m谩s relevantes para la predicci贸n.\n",
    "\n",
    "###  3. **Capa MLP (Multi-Layer Perceptron)**\n",
    "Despu茅s de cada capa de atenci贸n, los datos pasan por una **MLP (Red Neuronal de M煤ltiples Capas)**. Esta capa:\n",
    "   - Refina la informaci贸n extra铆da por la atenci贸n.\n",
    "   - Introduce no linealidad al modelo.\n",
    "   - Generaliza mejor las representaciones del lenguaje.\n",
    "\n",
    "Cada MLP en el Transformer tiene dos capas completamente conectadas con una funci贸n de activaci贸n no lineal intermedia.\n",
    "\n",
    "###  4. **Capa de salida**\n",
    "Finalmente, la capa de salida del modelo convierte la representaci贸n final en una probabilidad de predicci贸n sobre el siguiente token en la secuencia.\n",
    "\n",
    "###  5. **Estructura en profundidad**\n",
    "Llama 3.2 1B tiene varias capas Transformer apiladas, donde cada capa tiene una subcapa de **self-attention** y una subcapa MLP. Las activaciones intermedias dentro de la MLP son esenciales para analizar la informaci贸n que el modelo est谩 aprendiendo en cada paso.\n",
    "\n",
    "---\n",
    "\n",
    "## Guardar y cargar el modelo localmente\n",
    "\n",
    "Para evitar descargar el modelo cada vez que lo necesitemos, podemos almacenarlo localmente:\n",
    "\n",
    "```python\n",
    "# Guardar el modelo localmente\n",
    "model.save_pretrained(\"./llama3_model\")\n",
    "tokenizer.save_pretrained(\"./llama3_model\")\n",
    "\n",
    "# Cargarlo m谩s tarde sin conexi贸n\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./llama3_model\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./llama3_model\")\n",
    "```\n",
    "\n",
    "Esto optimiza el tiempo y evita depender de internet en cada ejecuci贸n.\n",
    "\n",
    "---\n",
    "\n",
    "## Generar texto con Llama 3.2\n",
    "\n",
    "Una vez que el modelo est谩 cargado, podemos probarlo generando texto:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Definir el modelo\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# Cargar el tokenizer y el modelo\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Verificar si MPS est谩 disponible y mover el modelo a GPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Tokenizar la entrada y mover a GPU\n",
    "prompt = \"Explica la importancia de la interpretabilidad en transformers.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generar texto con MPS activado\n",
    "print(\"Generando texto...\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_length=100,  # Reducido para mejorar velocidad\n",
    "        temperature=0.7,  # Controla la aleatoriedad\n",
    "        top_p=0.9  # Controla la diversidad de respuestas\n",
    "    )\n",
    "print(\"Generaci贸n completada.\")\n",
    "\n",
    "# Decodificar la salida del modelo\n",
    "texto_generado = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Texto generado:\", texto_generado)\n",
    "\n",
    "```\n",
    "\n",
    "Este c贸digo permite ver c贸mo Llama 3.2 responde a una consulta en lenguaje natural.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorando la arquitectura interna de Llama 3.2 1B\n",
    "\n",
    "Antes de extraer salidas intermedias, es fundamental comprender c贸mo est谩 estructurado el modelo.\n",
    "\n",
    "## 1锔 Ver la configuraci贸n general del modelo\n",
    "\n",
    "La configuraci贸n del modelo contiene informaci贸n clave como:\n",
    "- N煤mero de capas (`num_hidden_layers`)\n",
    "- Dimensi贸n de los embeddings (`hidden_size`)\n",
    "- N煤mero de cabezas de atenci贸n (`num_attention_heads`)\n",
    "- Tama帽o de la capa intermedia MLP (`intermediate_size`)\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Ver configuraci贸n general del modelo\n",
    "print(model.config)\n",
    "```\n",
    "\n",
    "### Explicaci贸n detallada de la configuraci贸n del modelo:\n",
    "\n",
    "- **_name_or_path**: indica el nombre exacto del modelo que estamos utilizando.\n",
    "- **architectures**: muestra la clase principal usada para la arquitectura, en este caso `LlamaForCausalLM`.\n",
    "- **attention_bias**: especifica si hay un sesgo aplicado en las matrices de atenci贸n (aqu铆 es `false`).\n",
    "- **attention_dropout**: nivel de abandono (dropout) aplicado en la atenci贸n (0.0 significa sin dropout).\n",
    "- **bos_token_id / eos_token_id**: identificadores especiales de inicio y fin de secuencia.\n",
    "- **head_dim**: tama帽o de la dimensi贸n de cada cabeza de atenci贸n.\n",
    "- **hidden_act**: funci贸n de activaci贸n en la MLP (aqu铆 `silu`).\n",
    "- **hidden_size**: tama帽o de la representaci贸n oculta, es decir, la dimensi贸n del embedding (2048).\n",
    "- **initializer_range**: rango usado para inicializar los pesos.\n",
    "- **intermediate_size**: tama帽o de la capa intermedia de la MLP (8192).\n",
    "- **max_position_embeddings**: el m谩ximo n煤mero de posiciones que puede procesar el modelo (131072).\n",
    "- **mlp_bias**: indica si la MLP usa sesgos (falso en este caso).\n",
    "- **model_type**: tipo de modelo (`llama`).\n",
    "- **num_attention_heads**: n煤mero de cabezas de atenci贸n (32).\n",
    "- **num_hidden_layers**: n煤mero de capas Transformer (16).\n",
    "- **num_key_value_heads**: n煤mero de cabezas para valores y llaves (8).\n",
    "- **pretraining_tp**: indica partici贸n para entrenamiento (1 significa sin particiones).\n",
    "- **rms_norm_eps**: valor epsilon para estabilidad num茅rica en normalizaci贸n.\n",
    "- **rope_scaling**: contiene par谩metros relacionados con el mecanismo de rotaci贸n posicional (RoPE).\n",
    "- **rope_theta**: par谩metro adicional para la frecuencia rotacional (500000.0).\n",
    "- **tie_word_embeddings**: indica si las embeddings de entrada y salida est谩n ligadas.\n",
    "- **torch_dtype**: tipo de datos usado (`float32`).\n",
    "- **transformers_version**: versi贸n de la librer铆a Transformers usada (4.49.0).\n",
    "- **use_cache**: indica si se utiliza cache para acelerar inferencias.\n",
    "- **vocab_size**: tama帽o del vocabulario (128256 tokens).\n",
    "\n",
    "---\n",
    "\n",
    "## 2锔 Ver cu谩ntos bloques Transformer tiene el modelo\n",
    "\n",
    "El modelo tiene una lista de capas accesible con `model.model.layers`. Cada elemento es un bloque Transformer completo.\n",
    "\n",
    "```python\n",
    "# Visualizar la lista de bloques Transformer\n",
    "print(model.model.layers)\n",
    "```\n",
    "\n",
    "### Explicaci贸n l铆nea por l铆nea del resultado:\n",
    "\n",
    "```plaintext\n",
    "ModuleList(\n",
    "  (0-15): 16 x LlamaDecoderLayer(\n",
    "```\n",
    "- `ModuleList`: indica que es una lista de m贸dulos (capas).\n",
    "- `(0-15): 16 x LlamaDecoderLayer`: el modelo tiene 16 capas numeradas de la 0 a la 15, cada una es un `LlamaDecoderLayer`.\n",
    "\n",
    "```plaintext\n",
    "    (self_attn): LlamaAttention(\n",
    "```\n",
    "- Cada capa contiene un m贸dulo de autoatenci贸n (`self_attn`).\n",
    "\n",
    "```plaintext\n",
    "      (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
    "```\n",
    "- `q_proj`: capa lineal que proyecta la consulta (query) desde 2048 a 2048 dimensiones.\n",
    "\n",
    "```plaintext\n",
    "      (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
    "```\n",
    "- `k_proj`: proyecci贸n de la clave (key) de 2048 dimensiones a 512.\n",
    "\n",
    "```plaintext\n",
    "      (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
    "```\n",
    "- `v_proj`: proyecci贸n del valor (value) de 2048 a 512 dimensiones.\n",
    "\n",
    "```plaintext\n",
    "      (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
    "```\n",
    "- `o_proj`: proyecta la salida de vuelta a 2048 dimensiones.\n",
    "\n",
    "```plaintext\n",
    "    )\n",
    "```\n",
    "- Fin del m贸dulo de atenci贸n.\n",
    "\n",
    "```plaintext\n",
    "    (mlp): LlamaMLP(\n",
    "```\n",
    "- Comienza la descripci贸n del m贸dulo MLP (multi-layer perceptron).\n",
    "\n",
    "```plaintext\n",
    "      (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
    "```\n",
    "- `gate_proj`: primera capa lineal que expande la dimensi贸n de 2048 a 8192.\n",
    "\n",
    "```plaintext\n",
    "      (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
    "```\n",
    "- `up_proj`: otra proyecci贸n de expansi贸n.\n",
    "\n",
    "```plaintext\n",
    "      (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
    "```\n",
    "- `down_proj`: reduce nuevamente de 8192 dimensiones a 2048.\n",
    "\n",
    "```plaintext\n",
    "      (act_fn): SiLU()\n",
    "```\n",
    "- `act_fn`: la funci贸n de activaci贸n usada es SiLU (Sigmoid Linear Unit).\n",
    "\n",
    "```plaintext\n",
    "    )\n",
    "```\n",
    "- Fin del m贸dulo MLP.\n",
    "\n",
    "```plaintext\n",
    "    (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
    "```\n",
    "- `input_layernorm`: normalizaci贸n de entrada.\n",
    "\n",
    "```plaintext\n",
    "    (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
    "```\n",
    "- `post_attention_layernorm`: normalizaci贸n despu茅s del bloque de atenci贸n.\n",
    "\n",
    "```plaintext\n",
    "  )\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 3锔 Analizar un bloque Transformer espec铆fico\n",
    "\n",
    "Para entender mejor la estructura interna, accedemos al primer bloque (铆ndice 0):\n",
    "\n",
    "```python\n",
    "# Acceder al primer bloque Transformer\n",
    "primer_bloque = model.model.layers[0]\n",
    "\n",
    "# Ver la estructura interna del primer bloque\n",
    "print(primer_bloque)\n",
    "```\n",
    "Al imprimir el contenido de un bloque Transformer, observamos la estructura de un **LlamaDecoderLayer**, que es la unidad b谩sica repetida en el modelo.\n",
    "\n",
    "# Estructura del `LlamaDecoderLayer`\n",
    "\n",
    "```python\n",
    "LlamaDecoderLayer(\n",
    "  (self_attn): LlamaAttention(\n",
    "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
    "    (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
    "    (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
    "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
    "  )\n",
    "  (mlp): LlamaMLP(\n",
    "    (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
    "    (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
    "    (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
    "    (act_fn): SiLU()\n",
    "  )\n",
    "  (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
    "  (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
    ")\n",
    "```\n",
    "\n",
    "# Desglose de cada componente:\n",
    "\n",
    "#  **(self_attn): LlamaAttention**\n",
    "Este bloque es el mecanismo de atenci贸n, compuesto por:\n",
    "- **q_proj**: Proyecci贸n lineal de las queries.\n",
    "- **k_proj**: Proyecci贸n lineal de las keys (notar que reduce de 2048 a 512 dimensiones).\n",
    "- **v_proj**: Proyecci贸n lineal de los values (tambi茅n reduce de 2048 a 512 dimensiones).\n",
    "- **o_proj**: Proyecci贸n lineal para combinar el resultado de la atenci贸n (vuelve a 2048).\n",
    "\n",
    "###  **(mlp): LlamaMLP**\n",
    "Es la red neuronal de m煤ltiples capas, compuesta por:\n",
    "- **gate_proj**: Proyecci贸n lineal que lleva de 2048 a 8192 dimensiones.\n",
    "- **up_proj**: Otra proyecci贸n lineal de 2048 a 8192.\n",
    "- **down_proj**: Reduce de 8192 de vuelta a 2048 dimensiones.\n",
    "- **act_fn**: La funci贸n de activaci贸n no lineal **SiLU()**.\n",
    "\n",
    "###  **Normalizaciones**\n",
    "- **input_layernorm**: Normalizaci贸n RMS antes de la atenci贸n, estabiliza la entrada.\n",
    "- **post_attention_layernorm**: Otra normalizaci贸n RMS aplicada despu茅s del bloque de atenci贸n.\n",
    "\n",
    "## Observaciones importantes:\n",
    "- Las proyecciones lineales con `bias=False` indican que no hay t茅rmino constante agregado.\n",
    "- La reducci贸n de dimensionalidad en las proyecciones de keys y values ayuda a ahorrar memoria y computaci贸n.\n",
    "- La MLP amplifica la representaci贸n (multiplicando por 4 el tama帽o del vector) y luego la comprime, lo cual ayuda a capturar relaciones complejas.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 4锔 La MLP dentro de un bloque Transformer\n",
    "\n",
    "Cada bloque Transformer contiene un subm贸dulo `mlp` que es un perceptr贸n multicapa con dos capas lineales y una activaci贸n no lineal.\n",
    "\n",
    "```python\n",
    "# Visualizar la estructura de la MLP dentro del primer bloque\n",
    "print(primer_bloque.mlp)\n",
    "```\n",
    "\n",
    "Al imprimir el contenido de la MLP dentro de un bloque Transformer, vemos la estructura siguiente:\n",
    "\n",
    "```python\n",
    "LlamaMLP(\n",
    "  (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
    "  (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
    "  (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
    "  (act_fn): SiLU()\n",
    ")\n",
    "```\n",
    "\n",
    "# Descripci贸n de cada componente\n",
    "\n",
    "  **gate_proj**\n",
    "- Es una capa lineal que transforma un vector de dimensi贸n 2048 a 8192.\n",
    "- El nombre *gate* indica que se utiliza junto con una funci贸n de activaci贸n para controlar qu茅 informaci贸n pasa y qu茅 se bloquea.\n",
    "- No tiene sesgo (`bias=False`), lo que significa que solo es una multiplicaci贸n lineal.\n",
    "\n",
    "  **up_proj**\n",
    "- Tambi茅n proyecta de 2048 a 8192 dimensiones.\n",
    "- Funciona en conjunto con `gate_proj` para ampliar la representaci贸n interna.\n",
    "\n",
    " **act_fn: SiLU()**\n",
    "- La funci贸n de activaci贸n es **SiLU** (*Sigmoid Linear Unit*), que introduce no linealidad en la transformaci贸n.\n",
    "- Esta funci贸n es suave y se ha demostrado eficaz en modelos grandes.\n",
    "\n",
    " **down_proj**\n",
    "- Una vez ampliada y transformada la representaci贸n, esta capa la reduce de nuevo de 8192 a 2048 dimensiones.\n",
    "- La compresi贸n permite que la informaci贸n relevante pase, eliminando redundancia y permitiendo un procesamiento eficiente.\n",
    "\n",
    " 驴Por qu茅 estas proyecciones?  \n",
    "La MLP dentro de un Transformer act煤a como una red que:\n",
    "1. Expande la representaci贸n (de 2048 a 8192).\n",
    "2. Aplica una activaci贸n no lineal.\n",
    "3. Comprime nuevamente a 2048 dimensiones.\n",
    "\n",
    "Este proceso permite que la red capture relaciones m谩s complejas y refinadas que no podr铆an obtenerse solo mediante capas de atenci贸n.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interceptando la salida de la MLP en la capa n-茅sima\n",
    "\n",
    "Para entender qu茅 informaci贸n est谩 procesando el modelo, vamos a interceptar la salida de la MLP en una capa espec铆fica.\n",
    "\n",
    "### 驴C贸mo hacerlo?\n",
    "\n",
    "Utilizaremos *hooks* de PyTorch.  \n",
    "Un *hook* es una funci贸n que se ejecuta autom谩ticamente cada vez que pasa informaci贸n por una capa espec铆fica.  \n",
    "As铆 podremos capturar la salida intermedia sin modificar el modelo.\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 1锔: Definir una funci贸n hook\n",
    "\n",
    "Un hook es una funci贸n que se ejecuta cada vez que la capa procesa informaci贸n.\n",
    "Esto nos permitir谩 interceptar y guardar la salida intermedia de la MLP.\n",
    "\n",
    "```python\n",
    "# Diccionario para guardar las activaciones\n",
    "activaciones_mlp = {}\n",
    "\n",
    "# Funci贸n hook para almacenar la salida\n",
    "def guardar_salida_mlp(layer_num):\n",
    "    def hook(module, input, output):\n",
    "        activaciones_mlp[f'capa_{layer_num}'] = output.detach().cpu()\n",
    "    return hook\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 2锔: Registrar el hook en la capa deseada\n",
    "\n",
    "Elegimos el n煤mero de capa `n` en la cual queremos interceptar la salida y registramos el hook.\n",
    "\n",
    "```python\n",
    "n = 5  # Cambia este n煤mero por la capa que deseas interceptar\n",
    "handle = model.model.layers[n].mlp.register_forward_hook(guardar_salida_mlp(n))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 3锔: Ejecutar una inferencia para activar el hook\n",
    "\n",
    "Realizamos una inferencia normal; el hook capturar谩 la salida autom谩ticamente.\n",
    "\n",
    "```python\n",
    "prompt = \"La interpretabilidad en transformers es fundamental.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = model(**inputs)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 4锔: Visualizar la activaci贸n interceptada\n",
    "\n",
    "Revisamos qu茅 se ha capturado y el tama帽o de la salida.\n",
    "\n",
    "```python\n",
    "print(f\"Salida interceptada en la capa {n}:\")\n",
    "print(activaciones_mlp[f'capa_{n}'])\n",
    "print(f\"Forma de la salida: {activaciones_mlp[f'capa_{n}'].shape}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 5锔: Eliminar el hook\n",
    "\n",
    "Eliminamos el hook para liberar recursos y evitar que siga interceptando salidas.\n",
    "\n",
    "```python\n",
    "handle.remove()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicaci贸n matem谩tica de la salida de la MLP\n",
    "\n",
    "Dentro de cada bloque Transformer, la MLP es un mapeo no lineal que transforma la representaci贸n obtenida despu茅s de la atenci贸n.\n",
    "\n",
    "Formalmente, si la entrada a la MLP es un vector $x \\in \\mathbb{R}^d$, la MLP realiza la siguiente operaci贸n:\n",
    "\n",
    "$$\n",
    "\\text{MLP}(x) = W_2(\\sigma(W_1 x + b_1)) + b_2\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $W_1 \\in \\mathbb{R}^{d_{inter} \\times d}$ es la matriz de pesos de la primera capa lineal.\n",
    "- $b_1$ es el sesgo (en Llama 3.2, puede ser nulo).\n",
    "- $\\sigma$ es la funci贸n de activaci贸n no lineal (en este modelo es SiLU).\n",
    "- $W_2 \\in \\mathbb{R}^{d \\times d_{inter}}$ es la segunda matriz de pesos.\n",
    "- $b_2$ es el segundo vector de sesgo.\n",
    "- $d$ es la dimensi贸n de entrada/salida y $d_{inter}$ es la dimensi贸n intermedia.\n",
    "\n",
    "**驴Qu茅 representa la salida de la MLP?**\n",
    "- La salida es un vector en $\\mathbb{R}^d$ que contiene una versi贸n refinada de la informaci贸n.\n",
    "- Cada dimensi贸n puede interpretarse como una activaci贸n relacionada con una caracter铆stica interna de la representaci贸n.\n",
    "\n",
    "## An谩lisis de las activaciones capturadas\n",
    "\n",
    "Una vez que interceptamos la salida de la MLP, obtenemos un tensor de forma:\n",
    "\n",
    "$$\n",
    "(\\text{batch size}, \\text{sequence length}, d)\n",
    "$$\n",
    "\n",
    "Para interpretarlo:\n",
    "- La dimensi贸n $d$ es la dimensi贸n de activaci贸n (ejemplo: 2048).\n",
    "- Cada posici贸n de la secuencia tiene un vector de activaciones.\n",
    "\n",
    "## Visualizaci贸n b谩sica de las activaciones\n",
    "\n",
    "### Mostrar el tama帽o de las activaciones\n",
    "```python\n",
    "# Ver forma de las activaciones\n",
    "print(activaciones_mlp[f'capa_{n}'].shape)\n",
    "```\n",
    "\n",
    "### Graficar histograma de valores\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "activaciones = activaciones_mlp[f'capa_{n}'].flatten().numpy()\n",
    "\n",
    "plt.hist(activaciones, bins=100, density=True)\n",
    "plt.title(f\"Distribuci贸n de activaciones de la capa {n}\")\n",
    "plt.xlabel(\"Valor de activaci贸n\")\n",
    "plt.ylabel(\"Frecuencia relativa\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Calcular media y varianza de las activaciones\n",
    "```python\n",
    "print(\"Media de las activaciones:\", activaciones.mean())\n",
    "print(\"Varianza de las activaciones:\", activaciones.var())\n",
    "```\n",
    "\n",
    "## Interpretaci贸n\n",
    "- Una media cercana a 0 y varianza controlada indican una activaci贸n bien normalizada.\n",
    "- Si hay valores extremos, pueden representar \"picos\" de activaci贸n, es decir, dimensiones dominantes que el modelo usa para tomar decisiones.\n",
    "\n",
    "En la siguiente secci贸n, aplicaremos reducci贸n de dimensionalidad para visualizar estas representaciones en 2D o 3D.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
