{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretabilidad Mecanicista de Transformers\n",
    "\n",
    "## Introducci√≥n\n",
    "\n",
    "Este documento detalla el proceso para descargar e implementar el modelo **Llama 3.2 1B en fp16** desde **Hugging Face**, extrayendo la *n*-√©sima salida de la capa MLP (*Multi-Layer Perceptron*). Tambi√©n se documentar√° cada paso siguiendo una metodolog√≠a rigurosa.\n",
    "\n",
    "## ¬øQu√© es Llama 3.2 1B?\n",
    "\n",
    "Llama 3.2 es un modelo de lenguaje basado en la arquitectura **Transformer**, desarrollado por Meta. Su tama√±o (1B de par√°metros) lo hace eficiente para tareas de procesamiento del lenguaje natural.\n",
    "\n",
    "## ¬øQu√© es fp16 y por qu√© es importante?\n",
    "\n",
    "**fp16 (Floating Point 16 bits)** es un formato de precisi√≥n reducida que permite acelerar el entrenamiento y la inferencia del modelo, reduciendo el uso de memoria sin perder mucha precisi√≥n.\n",
    "\n",
    "## ¬øQu√© es Hugging Face?\n",
    "\n",
    "**Hugging Face** es una plataforma l√≠der en el desarrollo de modelos de inteligencia artificial, especialmente en el campo del procesamiento del lenguaje natural (NLP). Proporciona una gran variedad de modelos preentrenados, herramientas para el entrenamiento y despliegue de modelos, y una comunidad activa de investigadores y desarrolladores.\n",
    "\n",
    "### ¬øPara qu√© se usa Hugging Face?\n",
    "\n",
    "- **Repositorio de modelos:** Permite descargar y compartir modelos preentrenados.\n",
    "- **Transformers Library:** Proporciona una API para usar modelos de NLP f√°cilmente.\n",
    "- **Hugging Face Hub:** Un espacio para colaborar y almacenar modelos y datasets.\n",
    "- **Inference API:** Para probar modelos sin necesidad de descargarlos localmente.\n",
    "\n",
    "### Conceptos clave que usaremos en Hugging Face\n",
    "\n",
    "1. **Token de autenticaci√≥n:** Necesario para acceder a modelos privados o restringidos.\n",
    "2. **Modelos preentrenados:** Conjuntos de pesos y configuraciones listos para usar.\n",
    "3. **Tokenizer:** Convierte texto en tensores num√©ricos que el modelo puede procesar.\n",
    "4. **Pipeline:** Interfaz sencilla para ejecutar modelos en tareas espec√≠ficas.\n",
    "5. **AutoModel y AutoTokenizer:** Clases que nos permiten cargar modelos y tokenizadores sin necesidad de conocer su arquitectura exacta.\n",
    "\n",
    "---\n",
    "\n",
    "## Acceso al modelo en Hugging Face\n",
    "\n",
    "Para acceder a *Llama 3.2*, hay dos m√©todos principales:\n",
    "\n",
    "### M√©todo 1: Solicitud de acceso manual \n",
    "\n",
    "1. Ir a [Hugging Face](https://huggingface.co/).\n",
    "2. Usar la barra de b√∫squeda para encontrar **Llama 3.2 1B**.\n",
    "3. Llenar el formulario de solicitud de acceso.\n",
    "4. Una vez aprobado, podr√°s acceder a los archivos del modelo en la pesta√±a **Files and versions**.\n",
    "\n",
    "Este m√©todo es √∫til si el modelo tiene restricciones de acceso y no requiere autenticaci√≥n en c√≥digo.\n",
    "\n",
    "### M√©todo 2: Autenticaci√≥n con un token de acceso\n",
    "\n",
    "Si necesitas automatizar el proceso o descargar modelos privados, puedes generar un token de acceso:\n",
    "\n",
    "1. Ve a **Settings > Access Tokens** en Hugging Face.\n",
    "2. Genera un nuevo *token* con permisos de \"read\".\n",
    "3. Usa el siguiente c√≥digo para autenticarte:\n",
    "\n",
    "```python\n",
    "from huggingface_hub import login\n",
    "login(\"TU_TOKEN_AQUI\")  # Reemplaza con tu token\n",
    "```\n",
    "\n",
    "Este m√©todo es m√°s formal y √∫til si trabajas en servidores o con varios modelos en diferentes proyectos.\n",
    "\n",
    "---\n",
    "\n",
    "## Configuraci√≥n del entorno\n",
    "\n",
    "Instalaremos las dependencias necesarias en nuestro entorno de Python:\n",
    "\n",
    "```bash\n",
    "pip install torch torchvision torchaudio transformers huggingface_hub\n",
    "```\n",
    "\n",
    "Verificamos la instalaci√≥n ejecutando:\n",
    "\n",
    "```bash\n",
    "python3 -c \"import torch; print(torch.__version__)\"\n",
    "```\n",
    "\n",
    "Si el comando muestra un n√∫mero de versi√≥n (`2.x.x`), significa que **PyTorch est√° correctamente instalado**.\n",
    "\n",
    "Opcionalmente, podemos crear un entorno virtual (quiz√° esrte paso lo debas hacer primero para poder instalar paquetes, ya deoendera de tu gestor de paquetes que tengas en uso):\n",
    "\n",
    "```bash\n",
    "python -m venv llama_env\n",
    "source llama_env/bin/activate  # En macOS/Linux\n",
    "llama_env\\Scripts\\activate  # En Windows\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Descarga del modelo desde Hugging Face\n",
    "\n",
    "Cargaremos el modelo y el *tokenizer* desde Hugging Face:\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"  # Nombre exacto del modelo\n",
    "\n",
    "# Cargar el tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    print(\"Tokenizer cargado exitosamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar el tokenizer: {e}\")\n",
    "\n",
    "# Cargar el modelo completo\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    print(\"Modelo cargado exitosamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar el modelo: {e}\")\n",
    "```\n",
    "\n",
    "Si el modelo se descarga correctamente, estar√° listo para su an√°lisis y uso en inferencias. En caso de errores, verifica los permisos de acceso en Hugging Face o la correcta instalaci√≥n de las dependencias.\n",
    "\n",
    "---\n",
    "\n",
    "## Estructura del modelo Llama 3.2 1B\n",
    "\n",
    "Llama 3.2 1B sigue la arquitectura **Transformer**, que est√° compuesta por m√∫ltiples bloques de procesamiento de informaci√≥n. Sus principales componentes son:\n",
    "\n",
    "### üîπ 1. **Embeddings**\n",
    "Los embeddings convierten palabras o tokens en vectores num√©ricos de alta dimensi√≥n. Estos vectores son la entrada del modelo y representan el significado de las palabras en un espacio matem√°tico.\n",
    "\n",
    "### üîπ 2. **M√∫ltiples capas de atenci√≥n (Self-Attention)**\n",
    "Cada capa de atenci√≥n analiza las relaciones entre todas las palabras de la oraci√≥n para determinar cu√°les son m√°s relevantes para la predicci√≥n.\n",
    "\n",
    "### üîπ 3. **Capa MLP (Multi-Layer Perceptron)**\n",
    "Despu√©s de cada capa de atenci√≥n, los datos pasan por una **MLP (Red Neuronal de M√∫ltiples Capas)**. Esta capa:\n",
    "   - Refina la informaci√≥n extra√≠da por la atenci√≥n.\n",
    "   - Introduce no linealidad al modelo.\n",
    "   - Generaliza mejor las representaciones del lenguaje.\n",
    "\n",
    "Cada MLP en el Transformer tiene dos capas completamente conectadas con una funci√≥n de activaci√≥n no lineal intermedia.\n",
    "\n",
    "### üîπ 4. **Capa de salida**\n",
    "Finalmente, la capa de salida del modelo convierte la representaci√≥n final en una probabilidad de predicci√≥n sobre el siguiente token en la secuencia.\n",
    "\n",
    "### üîπ 5. **Estructura en profundidad**\n",
    "Llama 3.2 1B tiene varias capas Transformer apiladas, donde cada capa tiene una subcapa de **self-attention** y una subcapa MLP. Las activaciones intermedias dentro de la MLP son esenciales para analizar la informaci√≥n que el modelo est√° aprendiendo en cada paso.\n",
    "\n",
    "---\n",
    "\n",
    "## Guardar y cargar el modelo localmente\n",
    "\n",
    "Para evitar descargar el modelo cada vez que lo necesitemos, podemos almacenarlo localmente:\n",
    "\n",
    "```python\n",
    "# Guardar el modelo localmente\n",
    "model.save_pretrained(\"./llama3_model\")\n",
    "tokenizer.save_pretrained(\"./llama3_model\")\n",
    "\n",
    "# Cargarlo m√°s tarde sin conexi√≥n\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./llama3_model\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./llama3_model\")\n",
    "```\n",
    "\n",
    "Esto optimiza el tiempo y evita depender de internet en cada ejecuci√≥n.\n",
    "\n",
    "---\n",
    "\n",
    "## Generar texto con Llama 3.2\n",
    "\n",
    "Una vez que el modelo est√° cargado, podemos probarlo generando texto:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Definir el modelo\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# Cargar el tokenizer y el modelo\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Verificar si MPS est√° disponible y mover el modelo a GPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Tokenizar la entrada y mover a GPU\n",
    "prompt = \"Explica la importancia de la interpretabilidad en transformers.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generar texto con MPS activado\n",
    "print(\"Generando texto...\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_length=100,  # Reducido para mejorar velocidad\n",
    "        temperature=0.7,  # Controla la aleatoriedad\n",
    "        top_p=0.9  # Controla la diversidad de respuestas\n",
    "    )\n",
    "print(\"Generaci√≥n completada.\")\n",
    "\n",
    "# Decodificar la salida del modelo\n",
    "texto_generado = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Texto generado:\", texto_generado)\n",
    "\n",
    "```\n",
    "\n",
    "Este c√≥digo permite ver c√≥mo Llama 3.2 responde a una consulta en lenguaje natural.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorando la arquitectura interna de Llama 3.2 1B\n",
    "\n",
    "Antes de extraer salidas intermedias, es fundamental comprender c√≥mo est√° estructurado el modelo.\n",
    "\n",
    "## 1Ô∏è‚É£ Ver la configuraci√≥n general del modelo\n",
    "\n",
    "La configuraci√≥n del modelo contiene informaci√≥n clave como:\n",
    "- N√∫mero de capas (`num_hidden_layers`)\n",
    "- Dimensi√≥n de los embeddings (`hidden_size`)\n",
    "- N√∫mero de cabezas de atenci√≥n (`num_attention_heads`)\n",
    "- Tama√±o de la capa intermedia MLP (`intermediate_size`)\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Ver configuraci√≥n general del modelo\n",
    "print(model.config)\n",
    "```\n",
    "\n",
    "### Explicaci√≥n detallada de la configuraci√≥n del modelo:\n",
    "\n",
    "- **_name_or_path**: indica el nombre exacto del modelo que estamos utilizando.\n",
    "- **architectures**: muestra la clase principal usada para la arquitectura, en este caso `LlamaForCausalLM`.\n",
    "- **attention_bias**: especifica si hay un sesgo aplicado en las matrices de atenci√≥n (aqu√≠ es `false`).\n",
    "- **attention_dropout**: nivel de abandono (dropout) aplicado en la atenci√≥n (0.0 significa sin dropout).\n",
    "- **bos_token_id / eos_token_id**: identificadores especiales de inicio y fin de secuencia.\n",
    "- **head_dim**: tama√±o de la dimensi√≥n de cada cabeza de atenci√≥n.\n",
    "- **hidden_act**: funci√≥n de activaci√≥n en la MLP (aqu√≠ `silu`).\n",
    "- **hidden_size**: tama√±o de la representaci√≥n oculta, es decir, la dimensi√≥n del embedding (2048).\n",
    "- **initializer_range**: rango usado para inicializar los pesos.\n",
    "- **intermediate_size**: tama√±o de la capa intermedia de la MLP (8192).\n",
    "- **max_position_embeddings**: el m√°ximo n√∫mero de posiciones que puede procesar el modelo (131072).\n",
    "- **mlp_bias**: indica si la MLP usa sesgos (falso en este caso).\n",
    "- **model_type**: tipo de modelo (`llama`).\n",
    "- **num_attention_heads**: n√∫mero de cabezas de atenci√≥n (32).\n",
    "- **num_hidden_layers**: n√∫mero de capas Transformer (16).\n",
    "- **num_key_value_heads**: n√∫mero de cabezas para valores y llaves (8).\n",
    "- **pretraining_tp**: indica partici√≥n para entrenamiento (1 significa sin particiones).\n",
    "- **rms_norm_eps**: valor epsilon para estabilidad num√©rica en normalizaci√≥n.\n",
    "- **rope_scaling**: contiene par√°metros relacionados con el mecanismo de rotaci√≥n posicional (RoPE).\n",
    "- **rope_theta**: par√°metro adicional para la frecuencia rotacional (500000.0).\n",
    "- **tie_word_embeddings**: indica si las embeddings de entrada y salida est√°n ligadas.\n",
    "- **torch_dtype**: tipo de datos usado (`float32`).\n",
    "- **transformers_version**: versi√≥n de la librer√≠a Transformers usada (4.49.0).\n",
    "- **use_cache**: indica si se utiliza cache para acelerar inferencias.\n",
    "- **vocab_size**: tama√±o del vocabulario (128256 tokens).\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Ver cu√°ntos bloques Transformer tiene el modelo\n",
    "\n",
    "El modelo tiene una lista de capas accesible con `model.model.layers`. Cada elemento es un bloque Transformer completo.\n",
    "\n",
    "```python\n",
    "# Visualizar la lista de bloques Transformer\n",
    "print(model.model.layers)\n",
    "```\n",
    "\n",
    "### Explicaci√≥n l√≠nea por l√≠nea del resultado:\n",
    "\n",
    "```plaintext\n",
    "ModuleList(\n",
    "  (0-15): 16 x LlamaDecoderLayer(\n",
    "```\n",
    "- `ModuleList`: indica que es una lista de m√≥dulos (capas).\n",
    "- `(0-15): 16 x LlamaDecoderLayer`: el modelo tiene 16 capas numeradas de la 0 a la 15, cada una es un `LlamaDecoderLayer`.\n",
    "\n",
    "```plaintext\n",
    "    (self_attn): LlamaAttention(\n",
    "```\n",
    "- Cada capa contiene un m√≥dulo de autoatenci√≥n (`self_attn`).\n",
    "\n",
    "```plaintext\n",
    "      (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
    "```\n",
    "- `q_proj`: capa lineal que proyecta la consulta (query) desde 2048 a 2048 dimensiones.\n",
    "\n",
    "```plaintext\n",
    "      (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
    "```\n",
    "- `k_proj`: proyecci√≥n de la clave (key) de 2048 dimensiones a 512.\n",
    "\n",
    "```plaintext\n",
    "      (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
    "```\n",
    "- `v_proj`: proyecci√≥n del valor (value) de 2048 a 512 dimensiones.\n",
    "\n",
    "```plaintext\n",
    "      (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
    "```\n",
    "- `o_proj`: proyecta la salida de vuelta a 2048 dimensiones.\n",
    "\n",
    "```plaintext\n",
    "    )\n",
    "```\n",
    "- Fin del m√≥dulo de atenci√≥n.\n",
    "\n",
    "```plaintext\n",
    "    (mlp): LlamaMLP(\n",
    "```\n",
    "- Comienza la descripci√≥n del m√≥dulo MLP (multi-layer perceptron).\n",
    "\n",
    "```plaintext\n",
    "      (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
    "```\n",
    "- `gate_proj`: primera capa lineal que expande la dimensi√≥n de 2048 a 8192.\n",
    "\n",
    "```plaintext\n",
    "      (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
    "```\n",
    "- `up_proj`: otra proyecci√≥n de expansi√≥n.\n",
    "\n",
    "```plaintext\n",
    "      (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
    "```\n",
    "- `down_proj`: reduce nuevamente de 8192 dimensiones a 2048.\n",
    "\n",
    "```plaintext\n",
    "      (act_fn): SiLU()\n",
    "```\n",
    "- `act_fn`: la funci√≥n de activaci√≥n usada es SiLU (Sigmoid Linear Unit).\n",
    "\n",
    "```plaintext\n",
    "    )\n",
    "```\n",
    "- Fin del m√≥dulo MLP.\n",
    "\n",
    "```plaintext\n",
    "    (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
    "```\n",
    "- `input_layernorm`: normalizaci√≥n de entrada.\n",
    "\n",
    "```plaintext\n",
    "    (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
    "```\n",
    "- `post_attention_layernorm`: normalizaci√≥n despu√©s del bloque de atenci√≥n.\n",
    "\n",
    "```plaintext\n",
    "  )\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 3Ô∏è‚É£ Analizar un bloque Transformer espec√≠fico\n",
    "\n",
    "Para entender mejor la estructura interna, accedemos al primer bloque (√≠ndice 0):\n",
    "\n",
    "```python\n",
    "# Acceder al primer bloque Transformer\n",
    "primer_bloque = model.model.layers[0]\n",
    "\n",
    "# Ver la estructura interna del primer bloque\n",
    "print(primer_bloque)\n",
    "```\n",
    "Al imprimir el contenido de un bloque Transformer, observamos la estructura de un **LlamaDecoderLayer**, que es la unidad b√°sica repetida en el modelo.\n",
    "\n",
    "# Estructura del `LlamaDecoderLayer`\n",
    "\n",
    "```python\n",
    "LlamaDecoderLayer(\n",
    "  (self_attn): LlamaAttention(\n",
    "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
    "    (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
    "    (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
    "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
    "  )\n",
    "  (mlp): LlamaMLP(\n",
    "    (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
    "    (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
    "    (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
    "    (act_fn): SiLU()\n",
    "  )\n",
    "  (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
    "  (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
    ")\n",
    "```\n",
    "\n",
    "# Desglose de cada componente:\n",
    "\n",
    "# üî∏ **(self_attn): LlamaAttention**\n",
    "Este bloque es el mecanismo de atenci√≥n, compuesto por:\n",
    "- **q_proj**: Proyecci√≥n lineal de las queries.\n",
    "- **k_proj**: Proyecci√≥n lineal de las keys (notar que reduce de 2048 a 512 dimensiones).\n",
    "- **v_proj**: Proyecci√≥n lineal de los values (tambi√©n reduce de 2048 a 512 dimensiones).\n",
    "- **o_proj**: Proyecci√≥n lineal para combinar el resultado de la atenci√≥n (vuelve a 2048).\n",
    "\n",
    "### üî∏ **(mlp): LlamaMLP**\n",
    "Es la red neuronal de m√∫ltiples capas, compuesta por:\n",
    "- **gate_proj**: Proyecci√≥n lineal que lleva de 2048 a 8192 dimensiones.\n",
    "- **up_proj**: Otra proyecci√≥n lineal de 2048 a 8192.\n",
    "- **down_proj**: Reduce de 8192 de vuelta a 2048 dimensiones.\n",
    "- **act_fn**: La funci√≥n de activaci√≥n no lineal **SiLU()**.\n",
    "\n",
    "### üî∏ **Normalizaciones**\n",
    "- **input_layernorm**: Normalizaci√≥n RMS antes de la atenci√≥n, estabiliza la entrada.\n",
    "- **post_attention_layernorm**: Otra normalizaci√≥n RMS aplicada despu√©s del bloque de atenci√≥n.\n",
    "\n",
    "## Observaciones importantes:\n",
    "- Las proyecciones lineales con `bias=False` indican que no hay t√©rmino constante agregado.\n",
    "- La reducci√≥n de dimensionalidad en las proyecciones de keys y values ayuda a ahorrar memoria y computaci√≥n.\n",
    "- La MLP amplifica la representaci√≥n (multiplicando por 4 el tama√±o del vector) y luego la comprime, lo cual ayuda a capturar relaciones complejas.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ La MLP dentro de un bloque Transformer\n",
    "\n",
    "Cada bloque Transformer contiene un subm√≥dulo `mlp` que es un perceptr√≥n multicapa con dos capas lineales y una activaci√≥n no lineal.\n",
    "\n",
    "```python\n",
    "# Visualizar la estructura de la MLP dentro del primer bloque\n",
    "print(primer_bloque.mlp)\n",
    "```\n",
    "\n",
    "Al imprimir el contenido de la MLP dentro de un bloque Transformer, vemos la estructura siguiente:\n",
    "\n",
    "```python\n",
    "LlamaMLP(\n",
    "  (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
    "  (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
    "  (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
    "  (act_fn): SiLU()\n",
    ")\n",
    "```\n",
    "\n",
    "# Descripci√≥n de cada componente\n",
    "\n",
    "  **gate_proj**\n",
    "- Es una capa lineal que transforma un vector de dimensi√≥n 2048 a 8192.\n",
    "- El nombre *gate* indica que se utiliza junto con una funci√≥n de activaci√≥n para controlar qu√© informaci√≥n pasa y qu√© se bloquea.\n",
    "- No tiene sesgo (`bias=False`), lo que significa que solo es una multiplicaci√≥n lineal.\n",
    "\n",
    "  **up_proj**\n",
    "- Tambi√©n proyecta de 2048 a 8192 dimensiones.\n",
    "- Funciona en conjunto con `gate_proj` para ampliar la representaci√≥n interna.\n",
    "\n",
    " **act_fn: SiLU()**\n",
    "- La funci√≥n de activaci√≥n es **SiLU** (*Sigmoid Linear Unit*), que introduce no linealidad en la transformaci√≥n.\n",
    "- Esta funci√≥n es suave y se ha demostrado eficaz en modelos grandes.\n",
    "\n",
    " **down_proj**\n",
    "- Una vez ampliada y transformada la representaci√≥n, esta capa la reduce de nuevo de 8192 a 2048 dimensiones.\n",
    "- La compresi√≥n permite que la informaci√≥n relevante pase, eliminando redundancia y permitiendo un procesamiento eficiente.\n",
    "\n",
    " ¬øPor qu√© estas proyecciones?  \n",
    "La MLP dentro de un Transformer act√∫a como una red que:\n",
    "1. Expande la representaci√≥n (de 2048 a 8192).\n",
    "2. Aplica una activaci√≥n no lineal.\n",
    "3. Comprime nuevamente a 2048 dimensiones.\n",
    "\n",
    "Este proceso permite que la red capture relaciones m√°s complejas y refinadas que no podr√≠an obtenerse solo mediante capas de atenci√≥n.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interceptando la salida de la MLP en la capa n-√©sima\n",
    "\n",
    "Para entender qu√© informaci√≥n est√° procesando el modelo, vamos a interceptar la salida de la MLP en una capa espec√≠fica.\n",
    "\n",
    "### ¬øC√≥mo hacerlo?\n",
    "\n",
    "Utilizaremos *hooks* de PyTorch.  \n",
    "Un *hook* es una funci√≥n que se ejecuta autom√°ticamente cada vez que pasa informaci√≥n por una capa espec√≠fica.  \n",
    "As√≠ podremos capturar la salida intermedia sin modificar el modelo.\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 1Ô∏è‚É£: Definir una funci√≥n hook\n",
    "\n",
    "Un hook es una funci√≥n que se ejecuta cada vez que la capa procesa informaci√≥n.\n",
    "Esto nos permitir√° interceptar y guardar la salida intermedia de la MLP.\n",
    "\n",
    "```python\n",
    "# Diccionario para guardar las activaciones\n",
    "activaciones_mlp = {}\n",
    "\n",
    "# Funci√≥n hook para almacenar la salida\n",
    "def guardar_salida_mlp(layer_num):\n",
    "    def hook(module, input, output):\n",
    "        activaciones_mlp[f'capa_{layer_num}'] = output.detach().cpu()\n",
    "    return hook\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 2Ô∏è‚É£: Registrar el hook en la capa deseada\n",
    "\n",
    "Elegimos el n√∫mero de capa `n` en la cual queremos interceptar la salida y registramos el hook.\n",
    "\n",
    "```python\n",
    "n = 5  # Cambia este n√∫mero por la capa que deseas interceptar\n",
    "handle = model.model.layers[n].mlp.register_forward_hook(guardar_salida_mlp(n))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 3Ô∏è‚É£: Ejecutar una inferencia para activar el hook\n",
    "\n",
    "Realizamos una inferencia normal; el hook capturar√° la salida autom√°ticamente.\n",
    "\n",
    "```python\n",
    "prompt = \"La interpretabilidad en transformers es fundamental.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = model(**inputs)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 4Ô∏è‚É£: Visualizar la activaci√≥n interceptada\n",
    "\n",
    "Revisamos qu√© se ha capturado y el tama√±o de la salida.\n",
    "\n",
    "```python\n",
    "print(f\"Salida interceptada en la capa {n}:\")\n",
    "print(activaciones_mlp[f'capa_{n}'])\n",
    "print(f\"Forma de la salida: {activaciones_mlp[f'capa_{n}'].shape}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 5Ô∏è‚É£: Eliminar el hook\n",
    "\n",
    "Eliminamos el hook para liberar recursos y evitar que siga interceptando salidas.\n",
    "\n",
    "```python\n",
    "handle.remove()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicaci√≥n matem√°tica de la salida de la MLP\n",
    "\n",
    "Dentro de cada bloque Transformer, la MLP es un mapeo no lineal que transforma la representaci√≥n obtenida despu√©s de la atenci√≥n.\n",
    "\n",
    "Formalmente, si la entrada a la MLP es un vector $x \\in \\mathbb{R}^d$, la MLP realiza la siguiente operaci√≥n:\n",
    "\n",
    "$$\n",
    "\\text{MLP}(x) = W_2(\\sigma(W_1 x + b_1)) + b_2\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $W_1 \\in \\mathbb{R}^{d_{inter} \\times d}$ es la matriz de pesos de la primera capa lineal.\n",
    "- $b_1$ es el sesgo (en Llama 3.2, puede ser nulo).\n",
    "- $\\sigma$ es la funci√≥n de activaci√≥n no lineal (en este modelo es SiLU).\n",
    "- $W_2 \\in \\mathbb{R}^{d \\times d_{inter}}$ es la segunda matriz de pesos.\n",
    "- $b_2$ es el segundo vector de sesgo.\n",
    "- $d$ es la dimensi√≥n de entrada/salida y $d_{inter}$ es la dimensi√≥n intermedia.\n",
    "\n",
    "**¬øQu√© representa la salida de la MLP?**\n",
    "- La salida es un vector en $\\mathbb{R}^d$ que contiene una versi√≥n refinada de la informaci√≥n.\n",
    "- Cada dimensi√≥n puede interpretarse como una activaci√≥n relacionada con una caracter√≠stica interna de la representaci√≥n.\n",
    "\n",
    "## An√°lisis de las activaciones capturadas\n",
    "\n",
    "Una vez que interceptamos la salida de la MLP, obtenemos un tensor de forma:\n",
    "\n",
    "$$\n",
    "(\\text{batch size}, \\text{sequence length}, d)\n",
    "$$\n",
    "\n",
    "Para interpretarlo:\n",
    "- La dimensi√≥n $d$ es la dimensi√≥n de activaci√≥n (ejemplo: 2048).\n",
    "- Cada posici√≥n de la secuencia tiene un vector de activaciones.\n",
    "\n",
    "## Visualizaci√≥n b√°sica de las activaciones\n",
    "\n",
    "### Mostrar el tama√±o de las activaciones\n",
    "```python\n",
    "# Ver forma de las activaciones\n",
    "print(activaciones_mlp[f'capa_{n}'].shape)\n",
    "```\n",
    "\n",
    "### Graficar histograma de valores\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "activaciones = activaciones_mlp[f'capa_{n}'].flatten().numpy()\n",
    "\n",
    "plt.hist(activaciones, bins=100, density=True)\n",
    "plt.title(f\"Distribuci√≥n de activaciones de la capa {n}\")\n",
    "plt.xlabel(\"Valor de activaci√≥n\")\n",
    "plt.ylabel(\"Frecuencia relativa\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Calcular media y varianza de las activaciones\n",
    "```python\n",
    "print(\"Media de las activaciones:\", activaciones.mean())\n",
    "print(\"Varianza de las activaciones:\", activaciones.var())\n",
    "```\n",
    "\n",
    "## Interpretaci√≥n\n",
    "- Una media cercana a 0 y varianza controlada indican una activaci√≥n bien normalizada.\n",
    "- Si hay valores extremos, pueden representar \"picos\" de activaci√≥n, es decir, dimensiones dominantes que el modelo usa para tomar decisiones.\n",
    "\n",
    "En la siguiente secci√≥n, aplicaremos reducci√≥n de dimensionalidad para visualizar estas representaciones en 2D o 3D.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
