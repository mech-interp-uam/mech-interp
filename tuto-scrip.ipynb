{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cómo usar Vast.ai y subir archivos desde tu computadora local.\n",
    "\n",
    "\n",
    "### 1. Crear y configurar una instancia en Vast.ai\n",
    "\n",
    "#### 1.1 : Entrar a la plataforma\n",
    "- Dirígete al sitio oficial: [https://vast.ai](https://vast.ai)\n",
    "- Crea una cuenta si no tienes una. Inicia sesión.\n",
    "- Completa el formulario de registro con tu correo electrónico y una \n",
    "contraseña segura.​\n",
    "- Después de registrarte, accede a tu cuenta.​\n",
    "- Dirígete a la sección de facturación para añadir créditos a tu cuenta. \n",
    "Puedes hacerlo mediante tarjeta de crédito o criptomonedas. \n",
    "El depósito mínimo es de $5 USD. ​\n",
    "- Para conectarte a las instancias mediante SSH, necesitas generar una clave SSH \n",
    "y añadirla a tu cuenta de Vast.ai.\n",
    "- En Windows: Abre PowerShell y genera una clave SSH ejecutando: ​ \n",
    "ssh-keygen -t rsa\n",
    "- En tu cuenta de Vast.ai, ve a la sección \"Account\" y selecciona \"Keys\". \n",
    "Pega tu clave pública en el campo correspondiente y guárdala. ​\n",
    "\n",
    "#### 1.2 En Mac o Linux:\n",
    "- Abre la terminal.\n",
    "- Genera una clave SSH: ssh-keygen -t rsa\n",
    "- Presiona Enter para aceptar la ubicación predeterminada y deja la contraseña \n",
    "en blanco si lo deseas.\n",
    "- Copia la clave pública al portapapeles:\n",
    "- macOS:​ pbcopy < ~/.ssh/id_rsa.pub\n",
    "- Linux:​ xclip -sel clip < ~/.ssh/id_rsa.pub\n",
    "- En tu cuenta de Vast.ai, ve a la sección \"Account\" y selecciona \"Keys\". \n",
    "Pega tu clave pública en el campo correspondiente y guárdala.​\n",
    "\n",
    "#### 1.3 : Buscar y filtrar una máquina\n",
    "Haz clic en el botón **\"Create\"** para comenzar a buscar una instancia. \n",
    "Establece los siguientes filtros en la parte izquierda de la interfaz:\n",
    "\n",
    "- **CUDA Version** `>= 11.8` (necesaria para modelos LLaMA recientes)\n",
    "- **VRAM** `>= 22 GB` (se recomienda A5000, A6000, RTX 3090 o A100)\n",
    "- **Internet Bandwidth** `>= 100 Mbps`\n",
    "- **Disk Space** `>= 50 GB` (dependiendo del tamaño del dataset y los \n",
    "outputs que planeas generar)\n",
    "\n",
    "En la opción **Image**, selecciona:\n",
    "```\n",
    "pytorch/pytorch:2.1.0-cuda11.8-cudnn8-runtime\n",
    "```\n",
    "Esta imagen ya viene con PyTorch y CUDA configurado, ideal para nuestros \n",
    "propósitos.\n",
    "\n",
    "#### 1.4 : Habilitar puertos y opciones\n",
    "Asegúrate de que estén seleccionadas las siguientes opciones:\n",
    "- `Public IP` \n",
    "- `SSH Port` (para poder conectarte remotamente desde tu terminal)\n",
    "- (Opcional) `Jupyter Port` si planeas abrir notebooks directamente en \n",
    "el navegador.\n",
    "\n",
    "Haz clic en **\"Rent\"** para activar la máquina. Despues en la parte izquierda \n",
    "has clic en instancces y espera que termine de cargar.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Conexión por SSH a la instancia de Vast.ai\n",
    "\n",
    "#### 2.1\n",
    "Una vez que la instancia esté activa, la consola de Vast.ai te mostrará:\n",
    "- IP pública (por ejemplo: `198.51.100.10`)\n",
    "- Puerto SSH (por ejemplo: `22400`)\n",
    "\n",
    "Abre tu terminal local (en Mac/Linux) y escribe:\n",
    "```bash\n",
    "ssh -p 22400 root@198.51.100.10\n",
    "```\n",
    "Este comando inicia sesión en la máquina remota usando el puerto correcto. \n",
    "Reemplaza `22400` y `198.51.100.10` con los valores reales que aparecen en \n",
    "tu consola de Vast.\n",
    "\n",
    "Al conectarte, estarás en la carpeta base, normalmente `/root/` o `/workspace/` \n",
    "dependiendo de la imagen.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Subir archivos desde tu computadora local (Mac o Linux) al servidor de Vast.ai\n",
    "\n",
    "#### 3.1\n",
    "Para transferir archivos usaremos el comando `scp` (secure copy), \n",
    "que funciona desde cualquier terminal. o simplemente los arrastramos y esperamos\n",
    "que carguen.\n",
    "\n",
    "#### 3.2 Sintaxis general:\n",
    "```bash\n",
    "scp -P PUERTO archivo_local usuario@IP:/ruta/remota/destino/\n",
    "```\n",
    "#### Ejemplo práctico:\n",
    "Supongamos que quieres subir estos dos archivos desde tu Mac:\n",
    "- `extraer_mlp8_hf.py`\n",
    "- `textos_oscar_1porciento.jsonl`\n",
    "\n",
    "Y que la IP del servidor es `198.51.100.10` y el puerto SSH es `22400`. \n",
    "Entonces en tu terminal escribe:\n",
    "\n",
    "```bash\n",
    "scp -P 22400 extraer_mlp8_hf.py root@198.51.100.10:/workspace/\n",
    "scp -P 22400 textos_oscar_1porciento.jsonl root@198.51.100.10:/workspace/\n",
    "```\n",
    "\n",
    "Esto hará que los archivos se copien desde tu máquina local al servidor remoto, \n",
    "específicamente a la carpeta `/workspace/`, que es donde suelen montarse \n",
    "los entornos de trabajo en esa imagen de PyTorch.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Verifica que los archivos llegaron correctamente\n",
    "\n",
    "Una vez conectado por SSH, puedes verificar que los archivos fueron subidos \n",
    "correctamente con:\n",
    "```bash\n",
    "ls /workspace/\n",
    "```\n",
    "Deberías ver `extraer_mlp8_hf.py` y `textos_oscar_1porciento.jsonl` listados.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicación del script `extraer_mlp8_hf.py` para ejecutarlo en Vast.ai\n",
    "\n",
    "Aquí se  explica línea por línea el funcionamiento del script \n",
    "`extraer_mlp8_hf.py`, destinado a extraer activaciones de la capa MLP 8 del \n",
    "modelo LLaMA 3.2 y subirlas automáticamente a Hugging Face. Además, se detalla \n",
    "cómo ejecutar este script en un servidor rentado en Vast.ai.\n",
    "\n",
    "\n",
    "### 1. Preparativos previos en Vast.ai\n",
    "\n",
    "Antes de correr este script:\n",
    "- Asegúrate de tener el archivo `extraer_mlp8_hf.py` y \n",
    "`textos_oscar_1porciento.jsonl` subidos en la ruta `/workspace/` de tu \n",
    "servidor.\n",
    "- Estés dentro del entorno base con Python, PyTorch y Transformers instalados.\n",
    "\n",
    "Puedes correr el script desde terminal con:\n",
    "```bash\n",
    "cd /workspace\n",
    "python extraer_mlp8_hf.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Explicación del script\n",
    "\n",
    "```python\n",
    "from huggingface_hub import login, HfApi\n",
    "```\n",
    "Importa:\n",
    "- `login`: para autenticarte con Hugging Face.\n",
    "- `HfApi`: para usar métodos de subida de archivos.\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "```\n",
    "Importa:\n",
    "- `AutoTokenizer`: para convertir textos en tokens.\n",
    "- `AutoModelForCausalLM`: modelo autoregresivo tipo LLaMA.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import shutil\n",
    "```\n",
    "Importa librerías necesarias para procesamiento tensorial, manejo de archivos \n",
    "y compresión ZIP.\n",
    "\n",
    "```python\n",
    "# === CONFIGURACIÓN ===\n",
    "login(\"TU_TOKEN_AQUI\")\n",
    "```\n",
    "Autentica el script en Hugging Face mediante tu token personal (este con\n",
    "permiso write).\n",
    "\n",
    "```python\n",
    "repo_id = \"naraca/mi-dataset-activaciones-llama3_2\"\n",
    "```\n",
    "Define el repositorio donde se subirán los `.zip` generados.\n",
    "\n",
    "```python\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "```\n",
    "Nombre del modelo base.\n",
    "\n",
    "```python\n",
    "jsonl_path = \"textos_oscar_1porciento.jsonl\"\n",
    "```\n",
    "Ruta del archivo con textos para procesar.\n",
    "\n",
    "```python\n",
    "batch_size = 32\n",
    "max_length = 256\n",
    "layer_idx = 8\n",
    "zip_every = 1000\n",
    "```\n",
    "Configura:\n",
    "- Tamaño del batch\n",
    "- Longitud máxima de tokens\n",
    "- Índice de la capa MLP a extraer\n",
    "- Cada cuántos tensores se hace un `.zip`\n",
    "\n",
    "```python\n",
    "api = HfApi()\n",
    "```\n",
    "Instancia de `HfApi` para subir archivos a Hugging Face.\n",
    "\n",
    "```python\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "```\n",
    "Carga el tokenizador y asigna el token de padding como el token de fin de \n",
    "secuencia (para evitar errores en modelos como LLaMA).\n",
    "\n",
    "```python\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    use_auth_token=True\n",
    ")\n",
    "model.eval()\n",
    "```\n",
    "Carga el modelo preentrenado en precisión baja (`float16`) y lo mapea \n",
    "automáticamente a CPU o GPU disponibles. Se pone en modo evaluación (`eval`).\n",
    "\n",
    "```python\n",
    "with open(jsonl_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "```\n",
    "Carga el archivo `.jsonl` como una lista de líneas para iterarlas más adelante.\n",
    "\n",
    "```python\n",
    "mlp_outputs = []\n",
    "counter = 0\n",
    "```\n",
    "Inicializa una lista vacía para almacenar tensores y un contador de chunks.\n",
    "\n",
    "```python\n",
    "def capture_mlp_output(module, input, output):\n",
    "    mlp_outputs.append(output.detach().cpu())\n",
    "```\n",
    "Función `hook` que captura la salida de la capa MLP, la desconecta del grafo \n",
    "de cómputo y la envía a la RAM (CPU).\n",
    "\n",
    "```python\n",
    "mlp_module = model.model.layers[layer_idx].mlp\n",
    "hook_handle = mlp_module.register_forward_hook(capture_mlp_output)\n",
    "```\n",
    "Registra el hook para capturar activaciones de la capa MLP 8.\n",
    "\n",
    "```python\n",
    "for i in range(0, len(lines), batch_size):\n",
    "```\n",
    "Iteración principal en bloques del tamaño `batch_size`.\n",
    "\n",
    "```python\n",
    "    batch_texts = [json.loads(l)[\"text\"] for l in lines[i:i+batch_size]]\n",
    "    inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, \n",
    "    truncation=True, max_length=max_length).to(model.device)\n",
    "```\n",
    "Toma un batch de textos, los tokeniza con truncamiento y padding, y los pasa \n",
    "al dispositivo del modelo.\n",
    "\n",
    "```python\n",
    "    with torch.no_grad():\n",
    "        model(**inputs)\n",
    "```\n",
    "Ejecuta el modelo sin guardar gradientes (modo inferencia).\n",
    "\n",
    "```python\n",
    "    if len(mlp_outputs) >= zip_every:\n",
    "        chunk = mlp_outputs[:zip_every]\n",
    "        chunk_path = f\"mlp8_chunk_{counter}.pt\"\n",
    "        torch.save(chunk, chunk_path)\n",
    "```\n",
    "Cuando se acumulan suficientes tensores (`zip_every`), se guardan en un \n",
    "archivo `.pt`.\n",
    "\n",
    "```python\n",
    "        zip_path = f\"mlp8_chunk_{counter}.zip\"\n",
    "        with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
    "            zipf.write(chunk_path)\n",
    "```\n",
    "Se crea un archivo `.zip` que contiene el `.pt`.\n",
    "\n",
    "```python\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=zip_path,\n",
    "            path_in_repo=zip_path,\n",
    "            repo_id=repo_id,\n",
    "            repo_type=\"dataset\"\n",
    "        )\n",
    "```\n",
    "Sube automáticamente el archivo `.zip` al repositorio de Hugging Face indicado.\n",
    "\n",
    "```python\n",
    "        os.remove(chunk_path)\n",
    "        os.remove(zip_path)\n",
    "        mlp_outputs = mlp_outputs[zip_every:]\n",
    "        counter += 1\n",
    "```\n",
    "Elimina archivos locales y actualiza el buffer y contador.\n",
    "\n",
    "```python\n",
    "hook_handle.remove()\n",
    "```\n",
    "Al terminar todo el proceso, se desconecta el hook para liberar recursos.\n",
    "\n",
    "\n",
    "\n",
    "Este script permite automatizar completamente la extracción, almacenamiento y publicación de activaciones MLP8 desde un dataset de textos en un entorno GPU remoto (como Vast.ai).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Errores comunes, recuperación y recomendaciones al ejecutar el script `extraer_mlp8_hf.py`\n",
    "\n",
    "\n",
    "- Posibles errores comunes al correr el script en Vast.ai.\n",
    "- Cómo recuperar el procesamiento si se interrumpe.\n",
    "- Cómo verificar que los archivos subidos sean válidos.\n",
    "- Recomendaciones adicionales para evitar fallos.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Errores comunes y cómo solucionarlos\n",
    "\n",
    "####  `RuntimeError: CUDA out of memory`\n",
    "**Causa:** El batch actual no cabe en la memoria de la GPU.  \n",
    "**Solución:** Reduce el `batch_size` en el script, por ejemplo de `32` a `16` \n",
    "o `8`.\n",
    "\n",
    "#### `Token indices sequence length is longer than the specified maximum`\n",
    "**Causa:** Algún texto excede `max_length`.  \n",
    "**Solución:** Asegúrate de tener `truncation=True` en el \n",
    "`tokenizer(...)`. Esto ya está hecho correctamente en el script.\n",
    "\n",
    "####  `KeyError: 'text'`\n",
    "**Causa:** El archivo `.jsonl` no tiene un campo `\"text\"` en alguna línea.  \n",
    "**Solución:** Abre el archivo y verifica que cada línea sea un JSON válido \n",
    "con la clave `\"text\"`.\n",
    "\n",
    "####  `ConnectionError` al subir a Hugging Face\n",
    "**Causa:** Falla temporal de red.  \n",
    "**Solución:** Asegúrate de que la instancia tiene conexión. Si es intermitente,\n",
    " puedes usar `try-except` para reintentos.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. ¿Qué hacer si se interrumpe el script?\n",
    "\n",
    "No necesitas volver a empezar desde cero. Para reiniciar el script de forma \n",
    "inteligente:\n",
    "\n",
    "1. **Verifica cuántos `.zip` ya se subieron** a tu repositorio en Hugging Face.\n",
    "2. **Modifica el contador manualmente** al reiniciar el script:\n",
    "   ```python\n",
    "   counter = N  # donde N es el siguiente número de chunk\n",
    "   ```\n",
    "3. **Ajusta la lista `lines = lines[M:]`** para saltarte los textos ya \n",
    "procesados.\n",
    "4. **También puedes guardar un registro (`log.txt`)** con las líneas procesadas \n",
    "   para más seguridad.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Verificar los archivos subidos en Hugging Face\n",
    "\n",
    "Puedes comprobar desde la interfaz de tu repositorio en:\n",
    "```\n",
    "https://huggingface.co/datasets/usuario/repositorio_name\n",
    "```\n",
    "Haz clic sobre cualquier `.zip`, descárgalo y luego localmente:\n",
    "\n",
    "```python\n",
    "import torch, zipfile\n",
    "with zipfile.ZipFile(\"mlp8_chunk_0.zip\") as zf:\n",
    "    zf.extractall()\n",
    "chunk = torch.load(\"mlp8_chunk_0.pt\")\n",
    "print(type(chunk), len(chunk))  # Debe ser una lista de tensores\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Recomendaciones adicionales\n",
    "\n",
    "* Usa `torch_dtype=torch.float16` para eficiencia.  \n",
    "* Si la conexión de red es inestable, guarda los `.zip` y súbelos al final \n",
    "  manualmente.  \n",
    "* Elimina archivos locales inmediatamente después de subirlos para ahorrar \n",
    "  espacio en disco.  \n",
    "* Usa `with torch.no_grad():` para evitar gastos innecesarios de memoria.\n",
    "\n",
    "---\n",
    "\n",
    "Este complemento ayudará a mantener un proceso robusto, eficiente y \n",
    "recuperable si ocurre algún fallo inesperado.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
