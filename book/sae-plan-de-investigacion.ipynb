{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de un SAE sobre las salidad y activaciones de la MLP intermedia de llama3.2 1B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Costo de entrenamiento\n",
    "\n",
    "Generalmente, el costo computacional de un LLM está dominado por la evaluación\n",
    "de sus MLPs (referencia a el seminario en una universidad de un ex empleado\n",
    "de anthropic)\n",
    "\n",
    "Una aprocimación simple del costo de entrenamiento de un modelo es\n",
    "\n",
    "$$\n",
    "  6ND \n",
    "$$\n",
    "esto en términos de FLOPs (operaciones de punto flotante).\n",
    "(citar a chinchilla scaling laws)\n",
    "\n",
    "donde $N$ es el número de parámetros y $D$ la cantidad de muestras en el\n",
    "conjunto de entrenamiento.\n",
    "\n",
    "Esto se debe a que, generalmente, cada parámetro actua en una multiplicaciónl\n",
    "y en una suma de punto flotante, dandonos un costo de $2ND$ tan solo en el\n",
    "forward pass. Tipicamente, el costo de el backward pass es el doble del forward\n",
    "pass, haciendo que su costo sea $4ND$. Sumando tenemos el resultado previamente\n",
    "mencionado.\n",
    "\n",
    "Sea $N_l$ el número de parámetros de llama.\n",
    "Como nosotros vamos a entrenar un autoencoder sobre un MLP a la mitad de llama,\n",
    "solo necesitamos evaluar esa primera mitad. Además, no correremos el backward\n",
    "pass sobre los parámetros de llama, pues no buscamos modificarlos, es decir, los\n",
    "mantendrémos fijos. Por esto, tenemos que el FLOPs realizados por tal mitad del\n",
    "modelo llama es\n",
    "\n",
    "$$\n",
    "  N_l D\n",
    "$$\n",
    "\n",
    "En cuanto al SAE, solo considerando el costo de aplicar sus matrices, tenemos\n",
    "\n",
    "$$\n",
    "  6 (2d_\\text{in}d_\\text{sae})D\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de gemmascope, entre todos los SAEs que entrenaron, los más pequeños\n",
    "entrenados en las salidas de las MLPs, se entrenaron en 4 mil millones de\n",
    "vectores de activaciones, con la dimencion de los vectores en el stream\n",
    "recidual (y por lo tanto, de la salida de las capas MLP), es 2048, con\n",
    "$d_\\text{sae} = 2028 * 8$.\n",
    "\n",
    "Deseamos encontrar hiperparámetros para entrenar SAEs, para esto:\n",
    "- Usamos una primera aproximación razonable modificando los hiperparámetros para\n",
    "  los autoencoders más pequeños entrenados en salidas de las MLPs de gemma 2\n",
    "  2 B\n",
    "- Ajustamos una power law en base a 2 entrenamientos de SAEs más pequeñas,\n",
    "  usando el mismo learning rate.\n",
    "- Ajustamos una power law para el learning rate con los hiperparámetos optimos\n",
    "  que estimó el paso anterior.\n",
    "\n",
    "Si ignoramos la relación posicional de los tokens y asumimos una distribución\n",
    "uniforme, tenemos que la entropía por token es\n",
    "\n",
    "$$\n",
    "  \\log_2 (\\text{tamaño del vocabulario})\n",
    "$$\n",
    "ya que el vocabulario de gemma2 2B es 256000 y el de llama es 128000, obtenemos\n",
    "que el número de tokens equivalente sería 4.2 mil millones. Ya que nuestro\n",
    "modelo es la mitad de tamaño de gemma2 2B, una primera cantidad de datos\n",
    "razonable para entrenar nuesto sae más grande es $2.1B$\n",
    "\n",
    "En el caso de llama3.2 1B, eso resultaría en\n",
    "\n",
    "$$\n",
    "  N_l D = (2.1 \\times 10^9)(1.2 \\times 10^9) \\approx 2.5 \\times 10^{18}\n",
    "$$\n",
    "Una RTX 4090 puede realisar cada segundo un máximo de $165 \\times 10^{12}$\n",
    "operaciones con tensores de 16 bits y acumulador de 32 bits (referencia\n",
    "al reporte técnico v1.0.1), luego, estimamos 4.2 horas de entrenamiento\n",
    "tan solo considerando la computación en el modelo llama.\n",
    "\n",
    "Ahora, para estimar las horas-RTX4090 para el autoencoder, en el caso de\n",
    "entrenarlo en la salida de la MLP intermedia, tendríamos\n",
    "\n",
    "$$\n",
    "    6(2.1 \\times 10^9)(2048^2)(8)(2) = 8.5 \\times 10^{17}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import jaxtyping\n",
    "import dataclasses\n",
    "from tqdm import tqdm\n",
    "from torch.nn import functional as F\n",
    "from torch.linalg import vector_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Step(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, threshold):\n",
    "        ctx.save_for_backward(x, threshold)\n",
    "        return (x > threshold).to(x.dtype)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        bandwidth = 0.001\n",
    "        x, threshold = ctx.saved_tensors\n",
    "\n",
    "        grad_threshold = torch.where(\n",
    "            abs(F.relu(x) - threshold) < bandwidth/2,\n",
    "            -1.0/bandwidth, 0)\n",
    "        \n",
    "        return torch.zeros_like(x), grad_threshold * grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sae(nn.Module):\n",
    "    def __init__(self, d_in, d_sae, use_pre_enc_bias=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.enc = nn.Linear(d_in, d_sae, dtype=torch.float16)\n",
    "        self.dec = nn.Linear(d_sae, d_in, dtype=torch.float16)\n",
    "        with torch.no_grad():\n",
    "            # normalize each of the d_sae dictonary vectors\n",
    "            self.dec.weight /= vector_norm(self.dec.weight, dim=0, keepdim=True)\n",
    "            self.enc.weight.copy_(self.dec.weight.clone().t())\n",
    "            self.enc.bias.copy_(torch.zeros_like(self.enc.bias))\n",
    "            self.dec.bias.copy_(torch.zeros_like(self.dec.bias))\n",
    "        self.log_threshold = nn.Parameter(\n",
    "            torch.log(torch.full((d_sae,), 0.001, dtype=torch.float16)))\n",
    "        self.use_pre_enc_bias = use_pre_enc_bias\n",
    "        def project_out_parallel_grad(dim, tensor):\n",
    "            @torch.no_grad\n",
    "            def hook(grad_in):\n",
    "                # norm along dim=dim of the tensor is assumed to be 1 as we\n",
    "                # are going to normalize it after every grad update\n",
    "                dot = (tensor * grad_in).sum(dim=dim, keepdim=True)\n",
    "                return grad_in - dot * tensor\n",
    "            return hook\n",
    "\n",
    "        self.dec.weight.register_hook(\n",
    "            project_out_parallel_grad(0, self.dec.weight))\n",
    "                \n",
    "\n",
    "    def forward(self,\n",
    "        x,\n",
    "        return_mask=False,\n",
    "        return_l0=True,\n",
    "        return_reconstruction_loss=True,\n",
    "    ):\n",
    "        \"We compute this much here so that compile() can do its magic\"\n",
    "        # as per train_gpt2.py on karpathy's llm.c repo, there are performance\n",
    "        # reasons not to return stuff\n",
    "        d = {}\n",
    "        original_input = x\n",
    "        if self.use_pre_enc_bias:\n",
    "            x = x - self.dec.bias\n",
    "        \n",
    "        x = self.enc(x)\n",
    "        threshold = torch.exp(self.log_threshold)\n",
    "        s = Step.apply(x, threshold)\n",
    "        if return_mask:\n",
    "            d['mask'] = s\n",
    "        if return_l0:\n",
    "            d['l0'] = s.float().mean(0).sum(-1)\n",
    "        if not return_reconstruction_loss:\n",
    "            return d\n",
    "        x = x*s\n",
    "        x = self.dec(x)\n",
    "        # print(((((x - original_input).float())**2) == 0).sum().to(int))\n",
    "        # d['reconstruction'] = ((x - original_input)**2).mean(0, dtype=torch.float32).sum()\n",
    "        d['reconstruction'] = (torch.linalg.vector_norm(x - original_input, dim=1, dtype=torch.float32)**2).mean()\n",
    "\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_in = 1024\n",
    "d_sae = d_in*8\n",
    "sae = Sae(d_in, d_sae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(13.011992, dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = 512\n",
    "x = torch.randn(b, d_in)\n",
    "# x /= x.norm(dim=1, keepdim=True).mean(dim=1, keepdim=True)\n",
    "x /= torch.linalg.vector_norm(x, dim=1, keepdim=True).mean(dim=1, keepdim=True)\n",
    "x = x.to(torch.float16)\n",
    "d = sae(x)\n",
    "d['reconstruction'].detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection = []\n",
    "# for i in range(20):\n",
    "#     if not (i % 10):\n",
    "#         print(i)\n",
    "#     x = torch.randn(b, d_in)\n",
    "#     # x /= x.norm(dim=1, keepdim=True).mean(dim=1, keepdim=True)\n",
    "#     x /= torch.linalg.vector_norm(x, dim=1, keepdim=True).mean(dim=1, keepdim=True)\n",
    "#     x = x.half()\n",
    "#     sae = Sae(d_in, d_sae)\n",
    "#     if not sae.enc.weight.detach().isfinite().all():\n",
    "#         print(\"enc blew up\")\n",
    "#     if not sae.dec.weight.detach().isfinite().all():\n",
    "#         print(\"dec blew up\")\n",
    "#     d = sae(x)\n",
    "#     l0, reconstruction = d['l0'].detach(), d['reconstruction'].detach()\n",
    "#     if not l0.isfinite().all():\n",
    "#         print(\"l0 blew up\")\n",
    "#     if not reconstruction.isfinite().all():\n",
    "#         print(\"reconstruction blew up\")\n",
    "#     collection.append((l0, reconstruction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_schedule_with_warmup(\n",
    "    current_step: int,\n",
    "    warmup_steps: int,\n",
    "    total_steps: int\n",
    "    ):\n",
    "    if current_step < warmup_steps:\n",
    "        return (1 + current_step) / warmup_steps\n",
    "    progress = (current_step - warmup_steps) / (total_steps - warmup_steps)\n",
    "    return 0.5 * (1 + math.cos(math.pi * progress))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsity_schedule(step, warmup_steps, max_sparsity_coeff):\n",
    "    if step >= warmup_steps:\n",
    "        return max_sparsity\n",
    "    return step / warmup_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_train_loader(batch, d_in, total_steps):\n",
    "    for _ in range(total_steps):\n",
    "        x = torch.randn(batch, d_in)\n",
    "        x = torch.randn(b, d_in)\n",
    "        x /= vector_norm(x, dim=1, keepdim=True)\n",
    "        yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "steps = 2**16\n",
    "max_lr = 7e-5\n",
    "d_in = 2048\n",
    "d_sae = 2048*8\n",
    "model = Sae(d_in, d_sae)\n",
    "warmup_steps=1000\n",
    "total_steps=2000 #for now\n",
    "batch = 1024\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n",
    "max_sparsity_coeff = 1000\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lr_lambda=lambda step: cosine_schedule_with_warmup(step, warmup_steps, total_steps)\n",
    ")\n",
    "train_loader = fake_train_loader(batch, d_in, total_steps)\n",
    "for step, x in enumerate(tqdm(train_loader)):\n",
    "    optimizer.zero_grad()\n",
    "    d = model(x.to(torch.float16))\n",
    "    reconstruction_loss, l0 = d['reconstruction'], d['l0']\n",
    "    sparsity_coefficient = sparsity_schedule(step, warmup_steps, max_sparsity_coeff)\n",
    "    loss = reconstruction_loss + sparsity_coefficient * l0\n",
    "    # log losses, compute stats, etc\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # TODO: sparsity_coefficient scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # normalize\n",
    "    with torch.no_grad():\n",
    "        model.dec.weight /= vector_norm(model.dec.weight, dim=0, keepdim=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
