{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretabilidad Mecanicista de Transformers\n",
    "\n",
    "## Introducci√≥n\n",
    "\n",
    "Este documento detalla el proceso para descargar e implementar el modelo **Llama 3.2 1B en fp16** desde **Hugging Face**, extrayendo la *n*-√©sima salida de la capa MLP (*Multi-Layer Perceptron*). Tambi√©n se documentar√° cada paso siguiendo una metodolog√≠a.\n",
    "\n",
    "## ¬øQu√© es Llama 3.2 1B?\n",
    "\n",
    "Llama 3.2 es un modelo de lenguaje basado en la arquitectura **Transformer**, desarrollado por Meta. Su tama√±o (1B de par√°metros) lo hace eficiente para tareas de procesamiento del lenguaje natural.\n",
    "\n",
    "## ¬øQu√© es fp16 y por qu√© es importante?\n",
    "\n",
    "**fp16 (Floating Point 16 bits)** es un formato de precisi√≥n reducida que permite acelerar el entrenamiento y la inferencia del modelo, reduciendo el uso de memoria sin perder mucha precisi√≥n.\n",
    "\n",
    "## ¬øQu√© es Hugging Face?\n",
    "\n",
    "**Hugging Face** es una plataforma l√≠der en el desarrollo de modelos de inteligencia artificial, especialmente en el campo del procesamiento del lenguaje natural (NLP). Proporciona una gran variedad de modelos preentrenados, herramientas para el entrenamiento y despliegue de modelos, y una comunidad activa de investigadores y desarrolladores.\n",
    "\n",
    "### ¬øPara qu√© se usa Hugging Face?\n",
    "\n",
    "- **Repositorio de modelos:** Permite descargar y compartir modelos preentrenados.\n",
    "- **Transformers Library:** Proporciona una API para usar modelos de NLP f√°cilmente.\n",
    "- **Hugging Face Hub:** Un espacio para colaborar y almacenar modelos y datasets.\n",
    "- **Inference API:** Para probar modelos sin necesidad de descargarlos localmente.\n",
    "\n",
    "### Conceptos clave que usaremos en Hugging Face\n",
    "\n",
    "1. **Token de autenticaci√≥n:** Necesario para acceder a modelos privados o restringidos.\n",
    "2. **Modelos preentrenados:** Conjuntos de pesos y configuraciones listos para usar.\n",
    "3. **Tokenizer:** Convierte texto en tensores num√©ricos que el modelo puede procesar.\n",
    "4. **Pipeline:** Interfaz sencilla para ejecutar modelos en tareas espec√≠ficas.\n",
    "5. **AutoModel y AutoTokenizer:** Clases que nos permiten cargar modelos y tokenizadores sin necesidad de conocer su arquitectura exacta.\n",
    "\n",
    "---\n",
    "\n",
    "## Acceso al modelo en Hugging Face\n",
    "\n",
    "Para acceder a *Llama 3.2*, hay dos m√©todos principales:\n",
    "\n",
    "### M√©todo 1: Solicitud de acceso manual \n",
    "\n",
    "1. Ir a [Hugging Face](https://huggingface.co/).\n",
    "2. Usar la barra de b√∫squeda para encontrar **Llama 3.2 1B**.\n",
    "3. Llenar el formulario de solicitud de acceso.\n",
    "4. Una vez aprobado, podr√°s acceder a los archivos del modelo en la pesta√±a **Files and versions**.\n",
    "\n",
    "Este m√©todo es √∫til si el modelo tiene restricciones de acceso y no requiere autenticaci√≥n en c√≥digo.\n",
    "\n",
    "### M√©todo 2: Autenticaci√≥n con un token de acceso\n",
    "\n",
    "Si necesitas automatizar el proceso o descargar modelos privados, puedes generar un token de acceso:\n",
    "\n",
    "1. Ve a **Settings > Access Tokens** en Hugging Face.\n",
    "2. Genera un nuevo *token* con permisos de \"read\".\n",
    "3. Usa el siguiente c√≥digo para autenticarte:\n",
    "\n",
    "```python\n",
    "from huggingface_hub import login\n",
    "login(\"TU_TOKEN_AQUI\")  # Reemplaza con tu token\n",
    "```\n",
    "\n",
    "Este m√©todo es m√°s formal y √∫til si trabajas en servidores o con varios modelos en diferentes proyectos.\n",
    "\n",
    "---\n",
    "\n",
    "## Configuraci√≥n del entorno\n",
    "\n",
    "Instalaremos las dependencias necesarias en nuestro entorno de Python:\n",
    "\n",
    "```bash\n",
    "pip install torch torchvision torchaudio transformers huggingface_hub\n",
    "```\n",
    "\n",
    "Verificamos la instalaci√≥n ejecutando:\n",
    "\n",
    "```bash\n",
    "python3 -c \"import torch; print(torch.__version__)\"\n",
    "```\n",
    "\n",
    "Si el comando muestra un n√∫mero de versi√≥n (`2.x.x`), significa que **PyTorch est√° correctamente instalado**.\n",
    "\n",
    "Opcionalmente, podemos crear un entorno virtual (quiz√° esrte paso lo debas hacer primero para poder instalar paquetes, ya deoendera de tu gestor de paquetes que tengas en uso):\n",
    "\n",
    "```bash\n",
    "python -m venv llama_env\n",
    "source llama_env/bin/activate  # En macOS/Linux\n",
    "llama_env\\Scripts\\activate  # En Windows\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Descarga del modelo desde Hugging Face\n",
    "\n",
    "Cargaremos el modelo y el *tokenizer* desde Hugging Face:\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"  # Nombre exacto del modelo\n",
    "\n",
    "# Cargar el tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    print(\"Tokenizer cargado exitosamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar el tokenizer: {e}\")\n",
    "\n",
    "# Cargar el modelo completo\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    print(\"Modelo cargado exitosamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar el modelo: {e}\")\n",
    "```\n",
    "\n",
    "Si el modelo se descarga correctamente, estar√° listo para su an√°lisis y uso en inferencias. En caso de errores, verifica los permisos de acceso en Hugging Face o la correcta instalaci√≥n de las dependencias.\n",
    "\n",
    "---\n",
    "\n",
    "## Estructura del modelo Llama 3.2 1B\n",
    "\n",
    "Llama 3.2 1B sigue la arquitectura **Transformer**, que est√° compuesta por m√∫ltiples bloques de procesamiento de informaci√≥n. Sus principales componentes son:\n",
    "\n",
    "###  1. **Embeddings**\n",
    "Los embeddings convierten palabras o tokens en vectores num√©ricos de alta dimensi√≥n. Estos vectores son la entrada del modelo y representan el significado de las palabras en un espacio matem√°tico.\n",
    "\n",
    "###  2. **M√∫ltiples capas de atenci√≥n (Self-Attention)**\n",
    "Cada capa de atenci√≥n analiza las relaciones entre todas las palabras de la oraci√≥n para determinar cu√°les son m√°s relevantes para la predicci√≥n.\n",
    "\n",
    "###  3. **Capa MLP (Multi-Layer Perceptron)**\n",
    "Despu√©s de cada capa de atenci√≥n, los datos pasan por una **MLP (Red Neuronal de M√∫ltiples Capas)**. Esta capa:\n",
    "   - Refina la informaci√≥n extra√≠da por la atenci√≥n.\n",
    "   - Introduce no linealidad al modelo.\n",
    "   - Generaliza mejor las representaciones del lenguaje.\n",
    "\n",
    "Cada MLP en el Transformer tiene dos capas completamente conectadas con una funci√≥n de activaci√≥n no lineal intermedia.\n",
    "\n",
    "###  4. **Capa de salida**\n",
    "Finalmente, la capa de salida del modelo convierte la representaci√≥n final en una probabilidad de predicci√≥n sobre el siguiente token en la secuencia.\n",
    "\n",
    "### üîπ 5. **Estructura en profundidad**\n",
    "Llama 3.2 1B tiene varias capas Transformer apiladas, donde cada capa tiene una subcapa de **self-attention** y una subcapa MLP. Las activaciones intermedias dentro de la MLP son esenciales para analizar la informaci√≥n que el modelo est√° aprendiendo en cada paso.\n",
    "\n",
    "---\n",
    "\n",
    "## Guardar y cargar el modelo localmente\n",
    "\n",
    "Para evitar descargar el modelo cada vez que lo necesitemos, podemos almacenarlo localmente:\n",
    "\n",
    "```python\n",
    "# Guardar el modelo localmente\n",
    "model.save_pretrained(\"./llama3_model\")\n",
    "tokenizer.save_pretrained(\"./llama3_model\")\n",
    "\n",
    "# Cargarlo m√°s tarde sin conexi√≥n\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./llama3_model\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./llama3_model\")\n",
    "```\n",
    "\n",
    "Esto optimiza el tiempo y evita depender de internet en cada ejecuci√≥n.\n",
    "\n",
    "---\n",
    "\n",
    "## Generar texto con Llama 3.2\n",
    "\n",
    "Una vez que el modelo est√° cargado, podemos probarlo generando texto:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Definir el modelo\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# Cargar el tokenizer y el modelo\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Verificar si MPS est√° disponible y mover el modelo a GPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Tokenizar la entrada y mover a GPU\n",
    "prompt = \"Explica la importancia de la interpretabilidad en transformers.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generar texto con MPS activado\n",
    "print(\"Generando texto...\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_length=100,  # Reducido para mejorar velocidad\n",
    "        temperature=0.7,  # Controla la aleatoriedad\n",
    "        top_p=0.9  # Controla la diversidad de respuestas\n",
    "    )\n",
    "print(\"Generaci√≥n completada.\")\n",
    "\n",
    "# Decodificar la salida del modelo\n",
    "texto_generado = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Texto generado:\", texto_generado)\n",
    "\n",
    "```\n",
    "\n",
    "Este c√≥digo permite ver c√≥mo Llama 3.2 responde a una consulta en lenguaje natural.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorando la arquitectura interna de Llama 3.2 1B\n",
    "\n",
    "Antes de extraer salidas intermedias, es fundamental comprender c√≥mo est√° estructurado el modelo.\n",
    "\n",
    "## 1Ô∏è Ver la configuraci√≥n general del modelo\n",
    "\n",
    "La configuraci√≥n del modelo contiene informaci√≥n clave como:\n",
    "- N√∫mero de capas (`num_hidden_layers`)\n",
    "- Dimensi√≥n de los embeddings (`hidden_size`)\n",
    "- N√∫mero de cabezas de atenci√≥n (`num_attention_heads`)\n",
    "- Tama√±o de la capa intermedia MLP (`intermediate_size`)\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Ver configuraci√≥n general del modelo\n",
    "print(model.config)\n",
    "```\n",
    "\n",
    "### Explicaci√≥n detallada de la configuraci√≥n del modelo:\n",
    "\n",
    "- **_name_or_path**: indica el nombre exacto del modelo que estamos utilizando.\n",
    "- **architectures**: muestra la clase principal usada para la arquitectura, en este caso `LlamaForCausalLM`.\n",
    "- **attention_bias**: especifica si hay un sesgo aplicado en las matrices de atenci√≥n (aqu√≠ es `false`).\n",
    "- **attention_dropout**: nivel de abandono (dropout) aplicado en la atenci√≥n (0.0 significa sin dropout).\n",
    "- **bos_token_id / eos_token_id**: identificadores especiales de inicio y fin de secuencia.\n",
    "- **head_dim**: tama√±o de la dimensi√≥n de cada cabeza de atenci√≥n.\n",
    "- **hidden_act**: funci√≥n de activaci√≥n en la MLP (aqu√≠ `silu`).\n",
    "- **hidden_size**: tama√±o de la representaci√≥n oculta, es decir, la dimensi√≥n del embedding (2048).\n",
    "- **initializer_range**: rango usado para inicializar los pesos.\n",
    "- **intermediate_size**: tama√±o de la capa intermedia de la MLP (8192).\n",
    "- **max_position_embeddings**: el m√°ximo n√∫mero de posiciones que puede procesar el modelo (131072).\n",
    "- **mlp_bias**: indica si la MLP usa sesgos (falso en este caso).\n",
    "- **model_type**: tipo de modelo (`llama`).\n",
    "- **num_attention_heads**: n√∫mero de cabezas de atenci√≥n (32).\n",
    "- **num_hidden_layers**: n√∫mero de capas Transformer (16).\n",
    "- **num_key_value_heads**: n√∫mero de cabezas para valores y llaves (8).\n",
    "- **pretraining_tp**: indica partici√≥n para entrenamiento (1 significa sin particiones).\n",
    "- **rms_norm_eps**: valor epsilon para estabilidad num√©rica en normalizaci√≥n.\n",
    "- **rope_scaling**: contiene par√°metros relacionados con el mecanismo de rotaci√≥n posicional (RoPE).\n",
    "- **rope_theta**: par√°metro adicional para la frecuencia rotacional (500000.0).\n",
    "- **tie_word_embeddings**: indica si las embeddings de entrada y salida est√°n ligadas.\n",
    "- **torch_dtype**: tipo de datos usado (`float32`).\n",
    "- **transformers_version**: versi√≥n de la librer√≠a Transformers usada (4.49.0).\n",
    "- **use_cache**: indica si se utiliza cache para acelerar inferencias.\n",
    "- **vocab_size**: tama√±o del vocabulario (128256 tokens).\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è Ver cu√°ntos bloques Transformer tiene el modelo\n",
    "\n",
    "El modelo tiene una lista de capas accesible con `model.model.layers`. Cada elemento es un bloque Transformer completo.\n",
    "\n",
    "```python\n",
    "# Visualizar la lista de bloques Transformer\n",
    "print(model.model.layers)\n",
    "```\n",
    "\n",
    "### Explicaci√≥n l√≠nea por l√≠nea del resultado:\n",
    "\n",
    "```plaintext\n",
    "ModuleList(\n",
    "  (0-15): 16 x LlamaDecoderLayer(\n",
    "```\n",
    "- `ModuleList`: indica que es una lista de m√≥dulos (capas).\n",
    "- `(0-15): 16 x LlamaDecoderLayer`: el modelo tiene 16 capas numeradas de la 0 a la 15, cada una es un `LlamaDecoderLayer`.\n",
    "\n",
    "```plaintext\n",
    "    (self_attn): LlamaAttention(\n",
    "```\n",
    "- Cada capa contiene un m√≥dulo de autoatenci√≥n (`self_attn`).\n",
    "\n",
    "```plaintext\n",
    "      (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
    "```\n",
    "- `q_proj`: capa lineal que proyecta la consulta (query) desde 2048 a 2048 dimensiones.\n",
    "\n",
    "```plaintext\n",
    "      (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
    "```\n",
    "- `k_proj`: proyecci√≥n de la clave (key) de 2048 dimensiones a 512.\n",
    "\n",
    "```plaintext\n",
    "      (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
    "```\n",
    "- `v_proj`: proyecci√≥n del valor (value) de 2048 a 512 dimensiones.\n",
    "\n",
    "```plaintext\n",
    "      (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
    "```\n",
    "- `o_proj`: proyecta la salida de vuelta a 2048 dimensiones.\n",
    "\n",
    "```plaintext\n",
    "    )\n",
    "```\n",
    "- Fin del m√≥dulo de atenci√≥n.\n",
    "\n",
    "```plaintext\n",
    "    (mlp): LlamaMLP(\n",
    "```\n",
    "- Comienza la descripci√≥n del m√≥dulo MLP (multi-layer perceptron).\n",
    "\n",
    "```plaintext\n",
    "      (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
    "```\n",
    "- `gate_proj`: primera capa lineal que expande la dimensi√≥n de 2048 a 8192.\n",
    "\n",
    "```plaintext\n",
    "      (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
    "```\n",
    "- `up_proj`: otra proyecci√≥n de expansi√≥n.\n",
    "\n",
    "```plaintext\n",
    "      (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
    "```\n",
    "- `down_proj`: reduce nuevamente de 8192 dimensiones a 2048.\n",
    "\n",
    "```plaintext\n",
    "      (act_fn): SiLU()\n",
    "```\n",
    "- `act_fn`: la funci√≥n de activaci√≥n usada es SiLU (Sigmoid Linear Unit).\n",
    "\n",
    "```plaintext\n",
    "    )\n",
    "```\n",
    "- Fin del m√≥dulo MLP.\n",
    "\n",
    "```plaintext\n",
    "    (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
    "```\n",
    "- `input_layernorm`: normalizaci√≥n de entrada.\n",
    "\n",
    "```plaintext\n",
    "    (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
    "```\n",
    "- `post_attention_layernorm`: normalizaci√≥n despu√©s del bloque de atenci√≥n.\n",
    "\n",
    "```plaintext\n",
    "  )\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 3Ô∏è Analizar un bloque Transformer espec√≠fico\n",
    "\n",
    "Para entender mejor la estructura interna, accedemos al primer bloque (√≠ndice 0):\n",
    "\n",
    "```python\n",
    "# Acceder al primer bloque Transformer\n",
    "primer_bloque = model.model.layers[0]\n",
    "\n",
    "# Ver la estructura interna del primer bloque\n",
    "print(primer_bloque)\n",
    "```\n",
    "Al imprimir el contenido de un bloque Transformer, observamos la estructura de un **LlamaDecoderLayer**, que es la unidad b√°sica repetida en el modelo.\n",
    "\n",
    "# Estructura del `LlamaDecoderLayer`\n",
    "\n",
    "```python\n",
    "LlamaDecoderLayer(\n",
    "  (self_attn): LlamaAttention(\n",
    "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
    "    (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
    "    (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
    "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
    "  )\n",
    "  (mlp): LlamaMLP(\n",
    "    (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
    "    (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
    "    (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
    "    (act_fn): SiLU()\n",
    "  )\n",
    "  (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
    "  (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
    ")\n",
    "```\n",
    "\n",
    "# Desglose de cada componente:\n",
    "\n",
    "#  **(self_attn): LlamaAttention**\n",
    "Este bloque es el mecanismo de atenci√≥n, compuesto por:\n",
    "- **q_proj**: Proyecci√≥n lineal de las queries.\n",
    "- **k_proj**: Proyecci√≥n lineal de las keys (notar que reduce de 2048 a 512 dimensiones).\n",
    "- **v_proj**: Proyecci√≥n lineal de los values (tambi√©n reduce de 2048 a 512 dimensiones).\n",
    "- **o_proj**: Proyecci√≥n lineal para combinar el resultado de la atenci√≥n (vuelve a 2048).\n",
    "\n",
    "###  **(mlp): LlamaMLP**\n",
    "Es la red neuronal de m√∫ltiples capas, compuesta por:\n",
    "- **gate_proj**: Proyecci√≥n lineal que lleva de 2048 a 8192 dimensiones.\n",
    "- **up_proj**: Otra proyecci√≥n lineal de 2048 a 8192.\n",
    "- **down_proj**: Reduce de 8192 de vuelta a 2048 dimensiones.\n",
    "- **act_fn**: La funci√≥n de activaci√≥n no lineal **SiLU()**.\n",
    "\n",
    "###  **Normalizaciones**\n",
    "- **input_layernorm**: Normalizaci√≥n RMS antes de la atenci√≥n, estabiliza la entrada.\n",
    "- **post_attention_layernorm**: Otra normalizaci√≥n RMS aplicada despu√©s del bloque de atenci√≥n.\n",
    "\n",
    "## Observaciones importantes:\n",
    "- Las proyecciones lineales con `bias=False` indican que no hay t√©rmino constante agregado.\n",
    "- La reducci√≥n de dimensionalidad en las proyecciones de keys y values ayuda a ahorrar memoria y computaci√≥n.\n",
    "- La MLP amplifica la representaci√≥n (multiplicando por 4 el tama√±o del vector) y luego la comprime, lo cual ayuda a capturar relaciones complejas.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è La MLP dentro de un bloque Transformer\n",
    "\n",
    "Cada bloque Transformer contiene un subm√≥dulo `mlp` que es un perceptr√≥n multicapa con dos capas lineales y una activaci√≥n no lineal.\n",
    "\n",
    "```python\n",
    "# Visualizar la estructura de la MLP dentro del primer bloque\n",
    "print(primer_bloque.mlp)\n",
    "```\n",
    "\n",
    "Al imprimir el contenido de la MLP dentro de un bloque Transformer, vemos la estructura siguiente:\n",
    "\n",
    "```python\n",
    "LlamaMLP(\n",
    "  (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
    "  (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
    "  (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
    "  (act_fn): SiLU()\n",
    ")\n",
    "```\n",
    "\n",
    "# Descripci√≥n de cada componente\n",
    "\n",
    "  **gate_proj**\n",
    "- Es una capa lineal que transforma un vector de dimensi√≥n 2048 a 8192.\n",
    "- El nombre *gate* indica que se utiliza junto con una funci√≥n de activaci√≥n para controlar qu√© informaci√≥n pasa y qu√© se bloquea.\n",
    "- No tiene sesgo (`bias=False`), lo que significa que solo es una multiplicaci√≥n lineal.\n",
    "\n",
    "  **up_proj**\n",
    "- Tambi√©n proyecta de 2048 a 8192 dimensiones.\n",
    "- Funciona en conjunto con `gate_proj` para ampliar la representaci√≥n interna.\n",
    "\n",
    " **act_fn: SiLU()**\n",
    "- La funci√≥n de activaci√≥n es **SiLU** (*Sigmoid Linear Unit*), que introduce no linealidad en la transformaci√≥n.\n",
    "- Esta funci√≥n es suave y se ha demostrado eficaz en modelos grandes.\n",
    "\n",
    " **down_proj**\n",
    "- Una vez ampliada y transformada la representaci√≥n, esta capa la reduce de nuevo de 8192 a 2048 dimensiones.\n",
    "- La compresi√≥n permite que la informaci√≥n relevante pase, eliminando redundancia y permitiendo un procesamiento eficiente.\n",
    "\n",
    " ¬øPor qu√© estas proyecciones?  \n",
    "La MLP dentro de un Transformer act√∫a como una red que:\n",
    "1. Expande la representaci√≥n (de 2048 a 8192).\n",
    "2. Aplica una activaci√≥n no lineal.\n",
    "3. Comprime nuevamente a 2048 dimensiones.\n",
    "\n",
    "Este proceso permite que la red capture relaciones m√°s complejas y refinadas que no podr√≠an obtenerse solo mediante capas de atenci√≥n.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interceptando la salida de la MLP en la capa n-√©sima\n",
    "\n",
    "Para entender qu√© informaci√≥n est√° procesando el modelo, vamos a interceptar la salida de la MLP en una capa espec√≠fica.\n",
    "\n",
    "### ¬øC√≥mo hacerlo?\n",
    "\n",
    "Utilizaremos *hooks* de PyTorch.  \n",
    "Un *hook* es una funci√≥n que se ejecuta autom√°ticamente cada vez que pasa informaci√≥n por una capa espec√≠fica.  \n",
    "As√≠ podremos capturar la salida intermedia sin modificar el modelo.\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 1Ô∏è: Definir una funci√≥n hook\n",
    "\n",
    "Un hook es una funci√≥n que se ejecuta cada vez que la capa procesa informaci√≥n.\n",
    "Esto nos permitir√° interceptar y guardar la salida intermedia de la MLP.\n",
    "\n",
    "```python\n",
    "# Diccionario para guardar las activaciones\n",
    "activaciones_mlp = {}\n",
    "\n",
    "# Funci√≥n hook para almacenar la salida\n",
    "def guardar_salida_mlp(layer_num):\n",
    "    def hook(module, input, output):\n",
    "        activaciones_mlp[f'capa_{layer_num}'] = output.detach().cpu()\n",
    "    return hook\n",
    "```\n",
    "**Explicaci√≥n:**\n",
    "- `activaciones_mlp` es un diccionario donde almacenamos la salida de la MLP.\n",
    "- `guardar_salida_mlp(layer_num)` devuelve una funci√≥n interna (hook) que captura el `output` de la MLP cuando se ejecuta el `forward`.\n",
    "- `output.detach().cpu()` desconecta el tensor del grafo de c√≥mputo y lo pasa a la CPU para ahorrar memoria.\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 2Ô∏è: Registrar el hook en la capa deseada\n",
    "\n",
    "Elegimos el n√∫mero de capa `n` en la cual queremos interceptar la salida y registramos el hook.\n",
    "\n",
    "```python\n",
    "n = 5  # este numero lo cambiamos a gusto para interceptar la capa que se desee\n",
    "handle = model.model.layers[n].mlp.register_forward_hook(guardar_salida_mlp(n))\n",
    "```\n",
    "**Explicaci√≥n:**\n",
    "- Se elige la capa `n` donde quieres capturar la salida.\n",
    "- `register_forward_hook()` vincula la funci√≥n hook a la capa MLP de ese bloque.\n",
    "---\n",
    "\n",
    "### Paso 3Ô∏è: Ejecutar una inferencia para activar el hook\n",
    "\n",
    "Realizamos una inferencia normal; el hook capturar√° la salida autom√°ticamente.\n",
    "# si no tienes definido donde correras el modelo, definirlo ahora con torch.device(\"mps\" if torch.backends.mps.is_available() else 2cpu\")\n",
    "```python\n",
    "prompt = \"La interpretabilidad en transformers es fundamental.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = model(**inputs)\n",
    "```\n",
    "**Explicaci√≥n:**\n",
    "- Definimos el dispositivo para que el modelo y las entradas trabajen en la misma plataforma.\n",
    "- El modelo ejecuta un `forward` sobre el `prompt`, y gracias al hook, se almacena autom√°ticamente la salida de la MLP de la capa interceptada.\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 4Ô∏è: Visualizar la activaci√≥n interceptada\n",
    "\n",
    "Revisamos qu√© se ha capturado y el tama√±o de la salida.\n",
    "\n",
    "```python\n",
    "print(f\"Salida interceptada en la capa {n}:\")\n",
    "print(activaciones_mlp[f'capa_{n}'])\n",
    "print(f\"Forma de la salida: {activaciones_mlp[f'capa_{n}'].shape}\")\n",
    "```\n",
    "**Explicaci√≥n de la salida:**\n",
    "- La salida es un tensor tridimensional de forma `(1, secuencia, dimensiones)`.\n",
    "- En este caso: `torch.Size([1, 11, 2048])` significa:\n",
    "  - 1: solo un ejemplo procesado.\n",
    "  - 11: tokens que forman la secuencia.\n",
    "  - 2048: dimensi√≥n de la representaci√≥n interna que produce la MLP.\n",
    "- Cada fila representa las activaciones (caracter√≠sticas) para un token en la capa interceptada.\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 5: Eliminar el hook\n",
    "\n",
    "Eliminamos el hook para liberar recursos y evitar que siga interceptando salidas.\n",
    "\n",
    "```python\n",
    "handle.remove()\n",
    "**Explicaci√≥n:**\n",
    "- El hook consume recursos y permanece activo mientras no se elimine.\n",
    "- Siempre es buena pr√°ctica eliminarlo despu√©s de haber capturado la salida deseada.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificando que la extracci√≥n de la salida MLP fue exitosa\n",
    "\n",
    "Despu√©s de interceptar la salida de la MLP y ejecutar una inferencia, es importante comprobar que la captura fue realizada correctamente.\n",
    "\n",
    "#### 1Ô∏è. Revisar las claves del diccionario\n",
    "El diccionario `activaciones_mlp` debe contener una clave con el nombre `capa_n`.\n",
    "\n",
    "```python\n",
    "print(activaciones_mlp.keys())\n",
    "```\n",
    "Salida esperada:\n",
    "```python\n",
    "dict_keys(['capa_5'])  # Si n=5\n",
    "```\n",
    "\n",
    "#### 2Ô∏è. Comprobar el tama√±o de la activaci√≥n capturada\n",
    "\n",
    "```python\n",
    "print(activaciones_mlp[f'capa_{n}'].shape)\n",
    "```\n",
    "La forma t√≠pica ser√°:\n",
    "- `(batch_size, sequence_length, hidden_size)`\n",
    "- Por ejemplo: `(1, 15, 2048)` si el batch size es 1, el prompt tiene 15 tokens y el `hidden_size` es 2048.\n",
    "\n",
    "#### 3Ô∏è. Visualizar valores ejemplo\n",
    "\n",
    "```python\n",
    "print(activaciones_mlp[f'capa_{n}'][0, 0, :10])  # Primer token, primeras 10 activaciones\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicaci√≥n matem√°tica y an√°lisis de la salida de la MLP\n",
    "\n",
    "Dentro de cada bloque Transformer, la MLP es un mapeo no lineal que transforma la representaci√≥n obtenida despu√©s de la atenci√≥n.\n",
    "\n",
    "### Formula general de la MLP\n",
    "\n",
    "Si la entrada a la MLP es un vector $x \\in \\mathbb{R}^d$, la MLP realiza la siguiente operaci√≥n:\n",
    "\n",
    "$$\n",
    "\\text{MLP}(x) = W_2(\\sigma(W_1 x + b_1)) + b_2\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $W_1 \\in \\mathbb{R}^{d_{inter} \\times d}$ es la matriz de pesos de la primera capa lineal (expansi√≥n).\n",
    "- $b_1$ es el vector de sesgo (en Llama 3.2 puede ser nulo).\n",
    "- $\\sigma$ es la funci√≥n de activaci√≥n no lineal (SiLU).\n",
    "- $W_2 \\in \\mathbb{R}^{d \\times d_{inter}}$ es la matriz de compresi√≥n.\n",
    "- $b_2$ es el vector de sesgo final.\n",
    "- $d$ es la dimensi√≥n de entrada/salida (ejemplo: 2048).\n",
    "- $d_{inter}$ es la dimensi√≥n intermedia expandida (ejemplo: 8192).\n",
    "\n",
    "### Desglose paso a paso:\n",
    "\n",
    "#### 1Ô∏è‚É£ Expansi√≥n lineal:\n",
    "$$\n",
    "z = W_1 x + b_1\n",
    "$$\n",
    "Se expande la representaci√≥n de $d$ a $d_{inter}$.\n",
    "\n",
    "#### 2Ô∏è‚É£ Activaci√≥n SiLU:\n",
    "$$\n",
    "z_{act} = \\text{SiLU}(z) = z \\cdot \\text{sigmoid}(z)\n",
    "$$\n",
    "La activaci√≥n SiLU introduce no linealidad suave, favoreciendo la estabilidad en modelos grandes.\n",
    "\n",
    "#### 3Ô∏è‚É£ Compresi√≥n lineal:\n",
    "$$\n",
    "y = W_2 z_{act} + b_2\n",
    "$$\n",
    "Se reduce de nuevo la representaci√≥n a la dimensi√≥n original $d$.\n",
    "\n",
    "### Correspondencia con el c√≥digo del modelo\n",
    "En el bloque `LlamaMLP`, estas operaciones corresponden a:\n",
    "- `gate_proj` y `up_proj`: expansi√≥n.\n",
    "- `act_fn = SiLU()`: activaci√≥n.\n",
    "- `down_proj`: compresi√≥n.\n",
    "\n",
    "---\n",
    "\n",
    "## An√°lisis de las activaciones capturadas\n",
    "\n",
    "Una vez que interceptamos la salida de la MLP, obtenemos un tensor de forma:\n",
    "$$\n",
    "(\\text{batch size}, \\text{sequence length}, d)\n",
    "$$\n",
    "\n",
    "Interpretaci√≥n:\n",
    "- La dimensi√≥n $d$ es la dimensi√≥n de activaci√≥n (ejemplo: 2048).\n",
    "- Cada posici√≥n de la secuencia tiene un vector de activaciones, representando la respuesta del modelo para ese token en esa capa.\n",
    "\n",
    "## Visualizaci√≥n b√°sica de las activaciones\n",
    "\n",
    "### Mostrar el tama√±o de las activaciones\n",
    "```python\n",
    "# Ver forma de las activaciones\n",
    "print(activaciones_mlp[f'capa_{n}'].shape)\n",
    "```\n",
    "\n",
    "### Graficar histograma de valores\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "activaciones = activaciones_mlp[f'capa_{n}'].flatten().numpy()\n",
    "\n",
    "plt.hist(activaciones, bins=100, density=True)\n",
    "plt.title(f\"Distribuci√≥n de activaciones de la capa {n}\")\n",
    "plt.xlabel(\"Valor de activaci√≥n\")\n",
    "plt.ylabel(\"Frecuencia relativa\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Calcular media y varianza de las activaciones\n",
    "```python\n",
    "print(\"Media de las activaciones:\", activaciones.mean())\n",
    "print(\"Varianza de las activaciones:\", activaciones.var())\n",
    "```\n",
    "\n",
    "## Interpretaci√≥n de la distribuci√≥n\n",
    "- Una media cercana a 0 y varianza controlada indican una activaci√≥n normalizada y estable.\n",
    "- Valores extremos en la cola del histograma representan \"picos\" de activaci√≥n.\n",
    "- Estos picos suelen corresponder a dimensiones que el modelo usa para tomar decisiones clave.\n",
    "\n",
    "En la siguiente secci√≥n, se aplicar√° reducci√≥n de dimensionalidad (PCA o t-SNE) para visualizar estas representaciones en un espacio 2D o 3D y analizar agrupamientos sem√°nticos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizaci√≥n avanzada de las activaciones de la MLP\n",
    "\n",
    "### Reducci√≥n de dimensionalidad con PCA\n",
    "\n",
    "Las activaciones que hemos capturado son vectores en un espacio de alta dimensi√≥n ($d=2048$). Para poder interpretarlas visualmente, usaremos **An√°lisis de Componentes Principales (PCA)**, que permite proyectar estos datos a un espacio de menor dimensi√≥n (usualmente 2D o 3D), preservando la mayor cantidad posible de varianza.\n",
    "\n",
    "---\n",
    "\n",
    "###  Explicaci√≥n matem√°tica de PCA (breve)\n",
    "\n",
    "Dado un conjunto de datos $X$ con $m$ muestras y $d$ dimensiones, PCA:\n",
    "1. Centra los datos restando la media.\n",
    "2. Calcula la matriz de covarianza $C = X^T X / m$.\n",
    "3. Obtiene los vectores propios y valores propios de $C$.\n",
    "4. Ordena esos vectores propios y selecciona los primeros $k$ para proyectar los datos en un subespacio de dimensi√≥n $k$.\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 1Ô∏è: Preparar los datos para PCA\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Extraemos el tensor de activaciones (ejemplo: capa n)\n",
    "activaciones_tensor = activaciones_mlp[f'capa_{n}']  # Shape: (batch_size, seq_length, hidden_dim)\n",
    "\n",
    "# Convertimos el tensor en un arreglo 2D: (batch_size * seq_length, hidden_dim)\n",
    "activaciones_2d = activaciones_tensor.reshape(-1, activaciones_tensor.shape[-1]).numpy()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 2Ô∏è: Aplicar PCA\n",
    "\n",
    "```python\n",
    "# Aplicamos PCA para reducir a 2 dimensiones\n",
    "pca = PCA(n_components=2)\n",
    "activaciones_pca = pca.fit_transform(activaciones_2d)\n",
    "\n",
    "# Informaci√≥n sobre varianza explicada\n",
    "print(\"Varianza explicada por cada componente:\", pca.explained_variance_ratio_)\n",
    "```\n",
    "\n",
    "**Salida esperada:**\n",
    "- Un array con dos valores que indican qu√© porcentaje de la varianza original es explicada por cada componente.\n",
    "- Ejemplo:\n",
    "```text\n",
    "Varianza explicada por cada componente: [0.35 0.20]\n",
    "```\n",
    "Esto significar√≠a que el primer componente retiene el 35% de la varianza y el segundo el 20%.\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 3Ô∏è: Graficar las activaciones proyectadas\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(activaciones_pca[:, 0], activaciones_pca[:, 1], alpha=0.3, s=10)\n",
    "plt.title(f\"Proyecci√≥n PCA de las activaciones de la capa {n}\")\n",
    "plt.xlabel(\"Componente principal 1\")\n",
    "plt.ylabel(\"Componente principal 2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Salida esperada:**\n",
    "- Un gr√°fico de dispersi√≥n (scatter plot) donde cada punto representa una posici√≥n de token proyectada a 2D.\n",
    "- Pueden aparecer nubes densas o grupos de puntos, lo que indica que las activaciones tienen patrones estructurados.\n",
    "\n",
    "---\n",
    "\n",
    "En la siguiente secci√≥n explicaremos c√≥mo interpretar estas agrupaciones y qu√© significan en el contexto de la interpretabilidad mecanicista de transformers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretaci√≥n de las agrupaciones observadas en PCA\n",
    "\n",
    "Una vez que hemos proyectado las activaciones en 2D, podemos analizar visualmente los patrones:\n",
    "\n",
    "### ¬øQu√© representan los grupos o nubes de puntos?\n",
    "- Cada punto en el gr√°fico corresponde a un vector de activaci√≥n para un token espec√≠fico dentro del lote de entrada.\n",
    "- Grupos densos indican que hay tokens o posiciones que comparten representaciones internas similares.\n",
    "- Puntos alejados o dispersos pueden representar outliers o tokens con roles especiales (por ejemplo, delimitadores o s√≠mbolos que la red trata de manera particular).\n",
    "\n",
    "---\n",
    "\n",
    "### Ejemplos de interpretaci√≥n:\n",
    "- Si observamos una nube central muy densa con algunas ramas o salidas laterales, esto indica que la mayor√≠a de las activaciones est√°n cerca de un espacio com√∫n, pero ciertos tokens activan dimensiones espec√≠ficas.\n",
    "- Si aparecen subgrupos claramente diferenciados, puede ser evidencia de cl√∫sters sem√°nticos: grupos de tokens con funciones similares.\n",
    "\n",
    "---\n",
    "\n",
    "## Visualizaci√≥n avanzada con t-SNE \n",
    "\n",
    "El algoritmo **t-SNE** (t-distributed Stochastic Neighbor Embedding) es una t√©cnica de reducci√≥n de dimensionalidad no lineal, ideal para visualizar agrupaciones y relaciones locales.\n",
    "\n",
    "### Paso 1Ô∏è: Aplicar t-SNE\n",
    "\n",
    "```python\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Reducimos primero con PCA a 50 dimensiones para mayor eficiencia\n",
    "pca_50 = PCA(n_components=50)\n",
    "activaciones_pca50 = pca_50.fit_transform(activaciones_2d)\n",
    "\n",
    "# Ahora aplicamos t-SNE\n",
    "print(\"Ejecutando t-SNE... (puede tardar unos segundos)\")\n",
    "tsne = TSNE(n_components=2, perplexity=30, learning_rate=200, n_iter=1000, verbose=1)\n",
    "activaciones_tsne = tsne.fit_transform(activaciones_pca50)\n",
    "```\n",
    "\n",
    "### Paso 2Ô∏è: Graficar la proyecci√≥n t-SNE\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(activaciones_tsne[:, 0], activaciones_tsne[:, 1], alpha=0.4, s=10, cmap=\"viridis\")\n",
    "plt.title(f\"Visualizaci√≥n t-SNE de las activaciones de la capa {n}\")\n",
    "plt.xlabel(\"Dimensi√≥n 1\")\n",
    "plt.ylabel(\"Dimensi√≥n 2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Interpretaci√≥n de t-SNE\n",
    "- t-SNE permite observar relaciones locales: puntos cercanos en el gr√°fico representan activaciones similares.\n",
    "- Clusters bien definidos son evidencia de estructuras internas aprendidas por la red.\n",
    "- Si observamos brazos o ramificaciones, pueden indicar caminos de transformaci√≥n de informaci√≥n desde representaciones generales a espec√≠ficas.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparando activaciones de la MLP entre diferentes prompts\n",
    "\n",
    "Para entender a√∫n m√°s la interpretabilidad mecanicista, es √∫til comparar c√≥mo cambian las activaciones de la MLP al variar el texto de entrada.\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 1Ô∏è: Definir diferentes prompts\n",
    "\n",
    "```python\n",
    "prompts = [\n",
    "    \"La inteligencia artificial est√° transformando la educaci√≥n.\",\n",
    "    \"La biolog√≠a estudia los seres vivos y su entorno.\",\n",
    "    \"Las matem√°ticas son el lenguaje del universo.\",\n",
    "    \"El arte es una forma de expresi√≥n humana.\"\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 2Ô∏è: Capturar las activaciones para cada prompt\n",
    "\n",
    "```python\n",
    "resultados_prompts = {}\n",
    "\n",
    "for idx, prompt in enumerate(prompts):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Interceptar con hook\n",
    "    handle = model.model.layers[n].mlp.register_forward_hook(guardar_salida_mlp(f\"prompt_{idx}\"))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _ = model(**inputs)\n",
    "    \n",
    "    resultados_prompts[f'prompt_{idx}'] = activaciones_mlp[f'prompt_{idx}']\n",
    "    handle.remove()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 3Ô∏è: Visualizar comparando prompts\n",
    "\n",
    "Podemos graficar histogramas superpuestos:\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for idx, prompt in enumerate(prompts):\n",
    "    activaciones = resultados_prompts[f'prompt_{idx}'].flatten().numpy()\n",
    "    plt.hist(activaciones, bins=100, alpha=0.3, density=True, label=f'Prompt {idx+1}')\n",
    "\n",
    "plt.title(f\"Comparaci√≥n de distribuciones de activaciones en la capa {n}\")\n",
    "plt.xlabel(\"Valor de activaci√≥n\")\n",
    "plt.ylabel(\"Frecuencia relativa\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Interpretaci√≥n\n",
    "- Si las distribuciones son similares, la capa MLP est√° respondiendo de forma general.\n",
    "- Diferencias notables entre prompts indican especializaci√≥n: ciertas dimensiones se activan m√°s seg√∫n el tipo de entrada.\n",
    "- Esto respalda la idea de que la MLP refina informaci√≥n contextual.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones y recomendaciones\n",
    "\n",
    "### Conclusiones principales:\n",
    "1. **La MLP en cada bloque Transformer** act√∫a como un refinador de representaciones, expandiendo y comprimiendo la informaci√≥n despu√©s de la atenci√≥n.\n",
    "2. **Las activaciones de la MLP** son vectores en espacios de alta dimensi√≥n que muestran estructuras internas, evidenciadas por patrones y agrupaciones cuando se proyectan a 2D.\n",
    "3. **La variaci√≥n en las activaciones entre diferentes prompts** revela que la red ajusta sus representaciones dependiendo del contenido sem√°ntico de la entrada.\n",
    "4. Las herramientas de reducci√≥n de dimensionalidad como **PCA** y **t-SNE** permiten visualizar y entender mejor la estructura interna de las activaciones.\n",
    "\n",
    "---\n",
    "\n",
    "###  Recomendaciones para futuros an√°lisis:\n",
    "- Probar con m√°s prompts y dominios tem√°ticos (ciencia, arte, pol√≠tica) para observar especializaci√≥n.\n",
    "- Realizar an√°lisis estad√≠sticos sobre las activaciones para medir dispersi√≥n, concentraci√≥n y valores extremos.\n",
    "- Comparar activaciones entre capas tempranas y capas tard√≠as para ver c√≥mo evoluciona la informaci√≥n.\n",
    "- Investigar qu√© dimensiones de la MLP dominan las salidas y c√≥mo est√°n correlacionadas con tokens clave.\n",
    "\n",
    "---\n",
    "\n",
    "###  Resumen para anexar al reporte:\n",
    "- Se explic√≥ formalmente qu√© es la MLP y su funci√≥n dentro del modelo Llama 3.2.\n",
    "- Se mostr√≥ c√≥mo interceptar la salida de la MLP utilizando hooks.\n",
    "- Se analizaron las activaciones obtenidas mediante visualizaci√≥n b√°sica y avanzada.\n",
    "- Se compararon las respuestas del modelo ante diferentes prompts.\n",
    "- Se plantearon recomendaciones para un an√°lisis m√°s profundo en proyectos de interpretabilidad.\n",
    "\n",
    "Este an√°lisis forma una base s√≥lida para demostrar comprensi√≥n te√≥rica, capacidad pr√°ctica y an√°lisis cr√≠tico en el proyecto de interpretabilidad mecanicista de transformers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
