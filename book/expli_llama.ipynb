{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretabilidad Mecanicista de Transformers\n",
    "\n",
    "## Introducción\n",
    "\n",
    "Este documento detalla el proceso para descargar e implementar el modelo **Llama 3.2 1B en fp16** desde **Hugging Face**, extrayendo la *n*-ésima salida de la capa MLP (*Multi-Layer Perceptron*). También se documentará cada paso siguiendo una metodología.\n",
    "\n",
    "## ¿Qué es Llama 3.2 1B?\n",
    "\n",
    "Llama 3.2 es un modelo de lenguaje basado en la arquitectura **Transformer**, desarrollado por Meta. Su tamaño (1B de parámetros) lo hace eficiente para tareas de procesamiento del lenguaje natural.\n",
    "\n",
    "## ¿Qué es fp16 y por qué es importante?\n",
    "\n",
    "**fp16 (Floating Point 16 bits)** es un formato de precisión reducida que permite acelerar el entrenamiento y la inferencia del modelo, reduciendo el uso de memoria sin perder mucha precisión.\n",
    "\n",
    "## ¿Qué es Hugging Face?\n",
    "\n",
    "**Hugging Face** es una plataforma líder en el desarrollo de modelos de inteligencia artificial, especialmente en el campo del procesamiento del lenguaje natural (NLP). Proporciona una gran variedad de modelos preentrenados, herramientas para el entrenamiento y despliegue de modelos, y una comunidad activa de investigadores y desarrolladores.\n",
    "\n",
    "### ¿Para qué se usa Hugging Face?\n",
    "\n",
    "- **Repositorio de modelos:** Permite descargar y compartir modelos preentrenados.\n",
    "- **Transformers Library:** Proporciona una API para usar modelos de NLP fácilmente.\n",
    "- **Hugging Face Hub:** Un espacio para colaborar y almacenar modelos y datasets.\n",
    "- **Inference API:** Para probar modelos sin necesidad de descargarlos localmente.\n",
    "\n",
    "### Conceptos clave que usaremos en Hugging Face\n",
    "\n",
    "1. **Token de autenticación:** Necesario para acceder a modelos privados o restringidos.\n",
    "2. **Modelos preentrenados:** Conjuntos de pesos y configuraciones listos para usar.\n",
    "3. **Tokenizer:** Convierte texto en tensores numéricos que el modelo puede procesar.\n",
    "4. **Pipeline:** Interfaz sencilla para ejecutar modelos en tareas específicas.\n",
    "5. **AutoModel y AutoTokenizer:** Clases que nos permiten cargar modelos y tokenizadores sin necesidad de conocer su arquitectura exacta.\n",
    "\n",
    "---\n",
    "\n",
    "## Acceso al modelo en Hugging Face\n",
    "\n",
    "Para acceder a *Llama 3.2*, hay dos métodos principales:\n",
    "\n",
    "### Método 1: Solicitud de acceso manual \n",
    "\n",
    "1. Ir a [Hugging Face](https://huggingface.co/).\n",
    "2. Usar la barra de búsqueda para encontrar **Llama 3.2 1B**.\n",
    "3. Llenar el formulario de solicitud de acceso.\n",
    "4. Una vez aprobado, podrás acceder a los archivos del modelo en la pestaña **Files and versions**.\n",
    "\n",
    "Este método es útil si el modelo tiene restricciones de acceso y no requiere autenticación en código.\n",
    "\n",
    "### Método 2: Autenticación con un token de acceso\n",
    "\n",
    "Si necesitas automatizar el proceso o descargar modelos privados, puedes generar un token de acceso:\n",
    "\n",
    "1. Ve a **Settings > Access Tokens** en Hugging Face.\n",
    "2. Genera un nuevo *token* con permisos de \"read\".\n",
    "3. Usa el siguiente código para autenticarte:\n",
    "\n",
    "```python\n",
    "from huggingface_hub import login\n",
    "login(\"TU_TOKEN_AQUI\")  # Reemplaza con tu token\n",
    "```\n",
    "\n",
    "Este método es más formal y útil si trabajas en servidores o con varios modelos en diferentes proyectos.\n",
    "\n",
    "---\n",
    "\n",
    "## Configuración del entorno\n",
    "\n",
    "Instalaremos las dependencias necesarias en nuestro entorno de Python:\n",
    "\n",
    "```bash\n",
    "pip install torch torchvision torchaudio transformers huggingface_hub\n",
    "```\n",
    "\n",
    "Verificamos la instalación ejecutando:\n",
    "\n",
    "```bash\n",
    "python3 -c \"import torch; print(torch.__version__)\"\n",
    "```\n",
    "\n",
    "Si el comando muestra un número de versión (`2.x.x`), significa que **PyTorch está correctamente instalado**.\n",
    "\n",
    "Opcionalmente, podemos crear un entorno virtual (quizá esrte paso lo debas hacer primero para poder instalar paquetes, ya deoendera de tu gestor de paquetes que tengas en uso):\n",
    "\n",
    "```bash\n",
    "python -m venv llama_env\n",
    "source llama_env/bin/activate  # En macOS/Linux\n",
    "llama_env\\Scripts\\activate  # En Windows\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Descarga del modelo desde Hugging Face\n",
    "\n",
    "Cargaremos el modelo y el *tokenizer* desde Hugging Face:\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"  # Nombre exacto del modelo\n",
    "\n",
    "# Cargar el tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    print(\"Tokenizer cargado exitosamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar el tokenizer: {e}\")\n",
    "\n",
    "# Cargar el modelo completo\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    print(\"Modelo cargado exitosamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar el modelo: {e}\")\n",
    "```\n",
    "\n",
    "Si el modelo se descarga correctamente, estará listo para su análisis y uso en inferencias. En caso de errores, verifica los permisos de acceso en Hugging Face o la correcta instalación de las dependencias.\n",
    "\n",
    "---\n",
    "\n",
    "## Estructura del modelo Llama 3.2 1B\n",
    "\n",
    "Llama 3.2 1B sigue la arquitectura **Transformer**, que está compuesta por múltiples bloques de procesamiento de información. Sus principales componentes son:\n",
    "\n",
    "###  1. **Embeddings**\n",
    "Los embeddings convierten palabras o tokens en vectores numéricos de alta dimensión. Estos vectores son la entrada del modelo y representan el significado de las palabras en un espacio matemático.\n",
    "\n",
    "###  2. **Múltiples capas de atención (Self-Attention)**\n",
    "Cada capa de atención analiza las relaciones entre todas las palabras de la oración para determinar cuáles son más relevantes para la predicción.\n",
    "\n",
    "###  3. **Capa MLP (Multi-Layer Perceptron)**\n",
    "Después de cada capa de atención, los datos pasan por una **MLP (Red Neuronal de Múltiples Capas)**. Esta capa:\n",
    "   - Refina la información extraída por la atención.\n",
    "   - Introduce no linealidad al modelo.\n",
    "   - Generaliza mejor las representaciones del lenguaje.\n",
    "\n",
    "Cada MLP en el Transformer tiene dos capas completamente conectadas con una función de activación no lineal intermedia.\n",
    "\n",
    "###  4. **Capa de salida**\n",
    "Finalmente, la capa de salida del modelo convierte la representación final en una probabilidad de predicción sobre el siguiente token en la secuencia.\n",
    "\n",
    "###  5. **Estructura en profundidad**\n",
    "Llama 3.2 1B tiene varias capas Transformer apiladas, donde cada capa tiene una subcapa de **self-attention** y una subcapa MLP. Las activaciones intermedias dentro de la MLP son esenciales para analizar la información que el modelo está aprendiendo en cada paso.\n",
    "\n",
    "---\n",
    "\n",
    "## Guardar y cargar el modelo localmente\n",
    "\n",
    "Para evitar descargar el modelo cada vez que lo necesitemos, podemos almacenarlo localmente:\n",
    "\n",
    "```python\n",
    "# Guardar el modelo localmente\n",
    "model.save_pretrained(\"./llama3_model\")\n",
    "tokenizer.save_pretrained(\"./llama3_model\")\n",
    "\n",
    "# Cargarlo más tarde sin conexión\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./llama3_model\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./llama3_model\")\n",
    "```\n",
    "\n",
    "Esto optimiza el tiempo y evita depender de internet en cada ejecución.\n",
    "\n",
    "---\n",
    "\n",
    "## Generar texto con Llama 3.2\n",
    "\n",
    "Una vez que el modelo está cargado, podemos probarlo generando texto:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Definir el modelo\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# Cargar el tokenizer y el modelo\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Verificar si MPS está disponible y mover el modelo a GPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Tokenizar la entrada y mover a GPU\n",
    "prompt = \"Explica la importancia de la interpretabilidad en transformers.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generar texto con MPS activado\n",
    "print(\"Generando texto...\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_length=100,  # Reducido para mejorar velocidad\n",
    "        temperature=0.7,  # Controla la aleatoriedad\n",
    "        top_p=0.9  # Controla la diversidad de respuestas\n",
    "    )\n",
    "print(\"Generación completada.\")\n",
    "\n",
    "# Decodificar la salida del modelo\n",
    "texto_generado = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Texto generado:\", texto_generado)\n",
    "\n",
    "```\n",
    "\n",
    "Este código permite ver cómo Llama 3.2 responde a una consulta en lenguaje natural.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorando la arquitectura interna de Llama 3.2 1B\n",
    "\n",
    "Antes de extraer salidas intermedias, es fundamental comprender cómo está estructurado el modelo.\n",
    "\n",
    "## 1️ Ver la configuración general del modelo\n",
    "\n",
    "La configuración del modelo contiene información clave como:\n",
    "- Número de capas (`num_hidden_layers`)\n",
    "- Dimensión de los embeddings (`hidden_size`)\n",
    "- Número de cabezas de atención (`num_attention_heads`)\n",
    "- Tamaño de la capa intermedia MLP (`intermediate_size`)\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Ver configuración general del modelo\n",
    "print(model.config)\n",
    "```\n",
    "\n",
    "### Explicación detallada de la configuración del modelo:\n",
    "\n",
    "- **_name_or_path**: indica el nombre exacto del modelo que estamos utilizando.\n",
    "- **architectures**: muestra la clase principal usada para la arquitectura, en este caso `LlamaForCausalLM`.\n",
    "- **attention_bias**: especifica si hay un sesgo aplicado en las matrices de atención (aquí es `false`).\n",
    "- **attention_dropout**: nivel de abandono (dropout) aplicado en la atención (0.0 significa sin dropout).\n",
    "- **bos_token_id / eos_token_id**: identificadores especiales de inicio y fin de secuencia.\n",
    "- **head_dim**: tamaño de la dimensión de cada cabeza de atención.\n",
    "- **hidden_act**: función de activación en la MLP (aquí `silu`).\n",
    "- **hidden_size**: tamaño de la representación oculta, es decir, la dimensión del embedding (2048).\n",
    "- **initializer_range**: rango usado para inicializar los pesos.\n",
    "- **intermediate_size**: tamaño de la capa intermedia de la MLP (8192).\n",
    "- **max_position_embeddings**: el máximo número de posiciones que puede procesar el modelo (131072).\n",
    "- **mlp_bias**: indica si la MLP usa sesgos (falso en este caso).\n",
    "- **model_type**: tipo de modelo (`llama`).\n",
    "- **num_attention_heads**: número de cabezas de atención (32).\n",
    "- **num_hidden_layers**: número de capas Transformer (16).\n",
    "- **num_key_value_heads**: número de cabezas para valores y llaves (8).\n",
    "- **pretraining_tp**: indica partición para entrenamiento (1 significa sin particiones).\n",
    "- **rms_norm_eps**: valor epsilon para estabilidad numérica en normalización.\n",
    "- **rope_scaling**: contiene parámetros relacionados con el mecanismo de rotación posicional (RoPE).\n",
    "- **rope_theta**: parámetro adicional para la frecuencia rotacional (500000.0).\n",
    "- **tie_word_embeddings**: indica si las embeddings de entrada y salida están ligadas.\n",
    "- **torch_dtype**: tipo de datos usado (`float32`).\n",
    "- **transformers_version**: versión de la librería Transformers usada (4.49.0).\n",
    "- **use_cache**: indica si se utiliza cache para acelerar inferencias.\n",
    "- **vocab_size**: tamaño del vocabulario (128256 tokens).\n",
    "\n",
    "---\n",
    "\n",
    "## 2️ Ver cuántos bloques Transformer tiene el modelo\n",
    "\n",
    "El modelo tiene una lista de capas accesible con `model.model.layers`. Cada elemento es un bloque Transformer completo.\n",
    "\n",
    "```python\n",
    "# Visualizar la lista de bloques Transformer\n",
    "print(model.model.layers)\n",
    "```\n",
    "\n",
    "### Explicación línea por línea del resultado:\n",
    "\n",
    "```plaintext\n",
    "ModuleList(\n",
    "  (0-15): 16 x LlamaDecoderLayer(\n",
    "```\n",
    "- `ModuleList`: indica que es una lista de módulos (capas).\n",
    "- `(0-15): 16 x LlamaDecoderLayer`: el modelo tiene 16 capas numeradas de la 0 a la 15, cada una es un `LlamaDecoderLayer`.\n",
    "\n",
    "```plaintext\n",
    "    (self_attn): LlamaAttention(\n",
    "```\n",
    "- Cada capa contiene un módulo de autoatención (`self_attn`).\n",
    "\n",
    "```plaintext\n",
    "      (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
    "```\n",
    "- `q_proj`: capa lineal que proyecta la consulta (query) desde 2048 a 2048 dimensiones.\n",
    "\n",
    "```plaintext\n",
    "      (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
    "```\n",
    "- `k_proj`: proyección de la clave (key) de 2048 dimensiones a 512.\n",
    "\n",
    "```plaintext\n",
    "      (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
    "```\n",
    "- `v_proj`: proyección del valor (value) de 2048 a 512 dimensiones.\n",
    "\n",
    "```plaintext\n",
    "      (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
    "```\n",
    "- `o_proj`: proyecta la salida de vuelta a 2048 dimensiones.\n",
    "\n",
    "```plaintext\n",
    "    )\n",
    "```\n",
    "- Fin del módulo de atención.\n",
    "\n",
    "```plaintext\n",
    "    (mlp): LlamaMLP(\n",
    "```\n",
    "- Comienza la descripción del módulo MLP (multi-layer perceptron).\n",
    "\n",
    "```plaintext\n",
    "      (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
    "```\n",
    "- `gate_proj`: primera capa lineal que expande la dimensión de 2048 a 8192.\n",
    "\n",
    "```plaintext\n",
    "      (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
    "```\n",
    "- `up_proj`: otra proyección de expansión.\n",
    "\n",
    "```plaintext\n",
    "      (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
    "```\n",
    "- `down_proj`: reduce nuevamente de 8192 dimensiones a 2048.\n",
    "\n",
    "```plaintext\n",
    "      (act_fn): SiLU()\n",
    "```\n",
    "- `act_fn`: la función de activación usada es SiLU (Sigmoid Linear Unit).\n",
    "\n",
    "```plaintext\n",
    "    )\n",
    "```\n",
    "- Fin del módulo MLP.\n",
    "\n",
    "```plaintext\n",
    "    (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
    "```\n",
    "- `input_layernorm`: normalización de entrada.\n",
    "\n",
    "```plaintext\n",
    "    (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
    "```\n",
    "- `post_attention_layernorm`: normalización después del bloque de atención.\n",
    "\n",
    "```plaintext\n",
    "  )\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 3️ Analizar un bloque Transformer específico\n",
    "\n",
    "Para entender mejor la estructura interna, accedemos al primer bloque (índice 0):\n",
    "\n",
    "```python\n",
    "# Acceder al primer bloque Transformer\n",
    "primer_bloque = model.model.layers[0]\n",
    "\n",
    "# Ver la estructura interna del primer bloque\n",
    "print(primer_bloque)\n",
    "```\n",
    "Al imprimir el contenido de un bloque Transformer, observamos la estructura de un **LlamaDecoderLayer**, que es la unidad básica repetida en el modelo.\n",
    "\n",
    "# Estructura del `LlamaDecoderLayer`\n",
    "\n",
    "```python\n",
    "LlamaDecoderLayer(\n",
    "  (self_attn): LlamaAttention(\n",
    "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
    "    (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
    "    (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
    "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
    "  )\n",
    "  (mlp): LlamaMLP(\n",
    "    (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
    "    (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
    "    (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
    "    (act_fn): SiLU()\n",
    "  )\n",
    "  (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
    "  (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
    ")\n",
    "```\n",
    "\n",
    "# Desglose de cada componente:\n",
    "\n",
    "#  **(self_attn): LlamaAttention**\n",
    "Este bloque es el mecanismo de atención, compuesto por:\n",
    "- **q_proj**: Proyección lineal de las queries.\n",
    "- **k_proj**: Proyección lineal de las keys (notar que reduce de 2048 a 512 dimensiones).\n",
    "- **v_proj**: Proyección lineal de los values (también reduce de 2048 a 512 dimensiones).\n",
    "- **o_proj**: Proyección lineal para combinar el resultado de la atención (vuelve a 2048).\n",
    "\n",
    "###  **(mlp): LlamaMLP**\n",
    "Es la red neuronal de múltiples capas, compuesta por:\n",
    "- **gate_proj**: Proyección lineal que lleva de 2048 a 8192 dimensiones.\n",
    "- **up_proj**: Otra proyección lineal de 2048 a 8192.\n",
    "- **down_proj**: Reduce de 8192 de vuelta a 2048 dimensiones.\n",
    "- **act_fn**: La función de activación no lineal **SiLU()**.\n",
    "\n",
    "###  **Normalizaciones**\n",
    "- **input_layernorm**: Normalización RMS antes de la atención, estabiliza la entrada.\n",
    "- **post_attention_layernorm**: Otra normalización RMS aplicada después del bloque de atención.\n",
    "\n",
    "## Observaciones importantes:\n",
    "- Las proyecciones lineales con `bias=False` indican que no hay término constante agregado.\n",
    "- La reducción de dimensionalidad en las proyecciones de keys y values ayuda a ahorrar memoria y computación.\n",
    "- La MLP amplifica la representación (multiplicando por 4 el tamaño del vector) y luego la comprime, lo cual ayuda a capturar relaciones complejas.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 4️ La MLP dentro de un bloque Transformer\n",
    "\n",
    "Cada bloque Transformer contiene un submódulo `mlp` que es un perceptrón multicapa con dos capas lineales y una activación no lineal.\n",
    "\n",
    "```python\n",
    "# Visualizar la estructura de la MLP dentro del primer bloque\n",
    "print(primer_bloque.mlp)\n",
    "```\n",
    "\n",
    "Al imprimir el contenido de la MLP dentro de un bloque Transformer, vemos la estructura siguiente:\n",
    "\n",
    "```python\n",
    "LlamaMLP(\n",
    "  (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
    "  (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
    "  (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
    "  (act_fn): SiLU()\n",
    ")\n",
    "```\n",
    "\n",
    "# Descripción de cada componente\n",
    "\n",
    "  **gate_proj**\n",
    "- Es una capa lineal que transforma un vector de dimensión 2048 a 8192.\n",
    "- El nombre *gate* indica que se utiliza junto con una función de activación para controlar qué información pasa y qué se bloquea.\n",
    "- No tiene sesgo (`bias=False`), lo que significa que solo es una multiplicación lineal.\n",
    "\n",
    "  **up_proj**\n",
    "- También proyecta de 2048 a 8192 dimensiones.\n",
    "- Funciona en conjunto con `gate_proj` para ampliar la representación interna.\n",
    "\n",
    " **act_fn: SiLU()**\n",
    "- La función de activación es **SiLU** (*Sigmoid Linear Unit*), que introduce no linealidad en la transformación.\n",
    "- Esta función es suave y se ha demostrado eficaz en modelos grandes.\n",
    "\n",
    " **down_proj**\n",
    "- Una vez ampliada y transformada la representación, esta capa la reduce de nuevo de 8192 a 2048 dimensiones.\n",
    "- La compresión permite que la información relevante pase, eliminando redundancia y permitiendo un procesamiento eficiente.\n",
    "\n",
    " ¿Por qué estas proyecciones?  \n",
    "La MLP dentro de un Transformer actúa como una red que:\n",
    "1. Expande la representación (de 2048 a 8192).\n",
    "2. Aplica una activación no lineal.\n",
    "3. Comprime nuevamente a 2048 dimensiones.\n",
    "\n",
    "Este proceso permite que la red capture relaciones más complejas y refinadas que no podrían obtenerse solo mediante capas de atención.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interceptando la salida de la MLP en la capa n-ésima\n",
    "\n",
    "Para entender qué información está procesando el modelo, vamos a interceptar la salida de la MLP en una capa específica.\n",
    "\n",
    "### ¿Cómo hacerlo?\n",
    "\n",
    "Utilizaremos *hooks* de PyTorch.  \n",
    "Un *hook* es una función que se ejecuta automáticamente cada vez que pasa información por una capa específica.  \n",
    "Así podremos capturar la salida intermedia sin modificar el modelo.\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 1️: Definir una función hook\n",
    "\n",
    "Un hook es una función que se ejecuta cada vez que la capa procesa información.\n",
    "Esto nos permitirá interceptar y guardar la salida intermedia de la MLP.\n",
    "\n",
    "```python\n",
    "# Diccionario para guardar las activaciones\n",
    "activaciones_mlp = {}\n",
    "\n",
    "# Función hook para almacenar la salida\n",
    "def guardar_salida_mlp(layer_num):\n",
    "    def hook(module, input, output):\n",
    "        activaciones_mlp[f'capa_{layer_num}'] = output.detach().cpu()\n",
    "    return hook\n",
    "```\n",
    "**Explicación:**\n",
    "- `activaciones_mlp` es un diccionario donde almacenamos la salida de la MLP.\n",
    "- `guardar_salida_mlp(layer_num)` devuelve una función interna (hook) que captura el `output` de la MLP cuando se ejecuta el `forward`.\n",
    "- `output.detach().cpu()` desconecta el tensor del grafo de cómputo y lo pasa a la CPU para ahorrar memoria.\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 2️: Registrar el hook en la capa deseada\n",
    "\n",
    "Elegimos el número de capa `n` en la cual queremos interceptar la salida y registramos el hook.\n",
    "\n",
    "```python\n",
    "n = 5  # este numero lo cambiamos a gusto para interceptar la capa que se desee\n",
    "handle = model.model.layers[n].mlp.register_forward_hook(guardar_salida_mlp(n))\n",
    "```\n",
    "**Explicación:**\n",
    "- Se elige la capa `n` donde quieres capturar la salida.\n",
    "- `register_forward_hook()` vincula la función hook a la capa MLP de ese bloque.\n",
    "---\n",
    "\n",
    "### Paso 3️: Ejecutar una inferencia para activar el hook\n",
    "\n",
    "Realizamos una inferencia normal; el hook capturará la salida automáticamente.\n",
    "# si no tienes definido donde correras el modelo, definirlo ahora con torch.device(\"mps\" if torch.backends.mps.is_available() else 2cpu\")\n",
    "```python\n",
    "prompt = \"La interpretabilidad en transformers es fundamental.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = model(**inputs)\n",
    "```\n",
    "**Explicación:**\n",
    "- Definimos el dispositivo para que el modelo y las entradas trabajen en la misma plataforma.\n",
    "- El modelo ejecuta un `forward` sobre el `prompt`, y gracias al hook, se almacena automáticamente la salida de la MLP de la capa interceptada.\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 4️: Visualizar la activación interceptada\n",
    "\n",
    "Revisamos qué se ha capturado y el tamaño de la salida.\n",
    "\n",
    "```python\n",
    "print(f\"Salida interceptada en la capa {n}:\")\n",
    "print(activaciones_mlp[f'capa_{n}'])\n",
    "print(f\"Forma de la salida: {activaciones_mlp[f'capa_{n}'].shape}\")\n",
    "```\n",
    "**Explicación de la salida:**\n",
    "- La salida es un tensor tridimensional de forma `(1, secuencia, dimensiones)`.\n",
    "- En este caso: `torch.Size([1, 11, 2048])` significa:\n",
    "  - 1: solo un ejemplo procesado.\n",
    "  - 11: tokens que forman la secuencia.\n",
    "  - 2048: dimensión de la representación interna que produce la MLP.\n",
    "- Cada fila representa las activaciones (características) para un token en la capa interceptada.\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 5: Eliminar el hook\n",
    "\n",
    "Eliminamos el hook para liberar recursos y evitar que siga interceptando salidas.\n",
    "\n",
    "```python\n",
    "handle.remove()\n",
    "**Explicación:**\n",
    "- El hook consume recursos y permanece activo mientras no se elimine.\n",
    "- Siempre es buena práctica eliminarlo después de haber capturado la salida deseada.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificando que la extracción de la salida MLP fue exitosa\n",
    "\n",
    "Después de interceptar la salida de la MLP y ejecutar una inferencia, es importante comprobar que la captura fue realizada correctamente.\n",
    "\n",
    "#### 1️. Revisar las claves del diccionario\n",
    "El diccionario `activaciones_mlp` debe contener una clave con el nombre `capa_n`.\n",
    "\n",
    "```python\n",
    "print(activaciones_mlp.keys())\n",
    "```\n",
    "Salida esperada:\n",
    "```python\n",
    "dict_keys(['capa_5'])  # Si n=5\n",
    "```\n",
    "\n",
    "#### 2️. Comprobar el tamaño de la activación capturada\n",
    "\n",
    "```python\n",
    "print(activaciones_mlp[f'capa_{n}'].shape)\n",
    "```\n",
    "La forma típica será:\n",
    "- `(batch_size, sequence_length, hidden_size)`\n",
    "- Por ejemplo: `(1, 15, 2048)` si el batch size es 1, el prompt tiene 15 tokens y el `hidden_size` es 2048.\n",
    "\n",
    "#### 3️. Visualizar valores ejemplo\n",
    "\n",
    "```python\n",
    "print(activaciones_mlp[f'capa_{n}'][0, 0, :10])  # Primer token, primeras 10 activaciones\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicación matemática y análisis de la salida de la MLP\n",
    "\n",
    "Dentro de cada bloque Transformer, la MLP es un mapeo no lineal que transforma la representación obtenida después de la atención.\n",
    "\n",
    "### Formula general de la MLP\n",
    "\n",
    "Si la entrada a la MLP es un vector $x \\in \\mathbb{R}^d$, la MLP realiza la siguiente operación:\n",
    "\n",
    "$$\n",
    "\\text{MLP}(x) = W_2(\\sigma(W_1 x + b_1)) + b_2\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $W_1 \\in \\mathbb{R}^{d_{inter} \\times d}$ es la matriz de pesos de la primera capa lineal (expansión).\n",
    "- $b_1$ es el vector de sesgo (en Llama 3.2 puede ser nulo).\n",
    "- $\\sigma$ es la función de activación no lineal (SiLU).\n",
    "- $W_2 \\in \\mathbb{R}^{d \\times d_{inter}}$ es la matriz de compresión.\n",
    "- $b_2$ es el vector de sesgo final.\n",
    "- $d$ es la dimensión de entrada/salida (ejemplo: 2048).\n",
    "- $d_{inter}$ es la dimensión intermedia expandida (ejemplo: 8192).\n",
    "\n",
    "### Desglose paso a paso:\n",
    "\n",
    "####  Expansión lineal:\n",
    "$$\n",
    "z = W_1 x + b_1\n",
    "$$\n",
    "Se expande la representación de $d$ a $d_{inter}$.\n",
    "\n",
    "####  Activación SiLU:\n",
    "$$\n",
    "z_{act} = \\text{SiLU}(z) = z \\cdot \\text{sigmoid}(z)\n",
    "$$\n",
    "La activación SiLU introduce no linealidad suave, favoreciendo la estabilidad en modelos grandes.\n",
    "\n",
    "####  Compresión lineal:\n",
    "$$\n",
    "y = W_2 z_{act} + b_2\n",
    "$$\n",
    "Se reduce de nuevo la representación a la dimensión original $d$.\n",
    "\n",
    "### Correspondencia con el código del modelo\n",
    "En el bloque `LlamaMLP`, estas operaciones corresponden a:\n",
    "- `gate_proj` y `up_proj`: expansión.\n",
    "- `act_fn = SiLU()`: activación.\n",
    "- `down_proj`: compresión.\n",
    "\n",
    "---\n",
    "\n",
    "## Análisis de las activaciones capturadas\n",
    "\n",
    "Una vez que interceptamos la salida de la MLP, obtenemos un tensor de forma:\n",
    "$$\n",
    "(\\text{batch size}, \\text{sequence length}, d)\n",
    "$$\n",
    "\n",
    "Interpretación:\n",
    "- La dimensión $d$ es la dimensión de activación (ejemplo: 2048).\n",
    "- Cada posición de la secuencia tiene un vector de activaciones, representando la respuesta del modelo para ese token en esa capa.\n",
    "\n",
    "## Visualización básica de las activaciones\n",
    "\n",
    "### Mostrar el tamaño de las activaciones\n",
    "```python\n",
    "# Ver forma de las activaciones\n",
    "print(activaciones_mlp[f'capa_{n}'].shape)\n",
    "```\n",
    "\n",
    "### Graficar histograma de valores\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "activaciones = activaciones_mlp[f'capa_{n}'].flatten().numpy()\n",
    "\n",
    "plt.hist(activaciones, bins=100, density=True)\n",
    "plt.title(f\"Distribución de activaciones de la capa {n}\")\n",
    "plt.xlabel(\"Valor de activación\")\n",
    "plt.ylabel(\"Frecuencia relativa\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Calcular media y varianza de las activaciones\n",
    "```python\n",
    "print(\"Media de las activaciones:\", activaciones.mean())\n",
    "print(\"Varianza de las activaciones:\", activaciones.var())\n",
    "```\n",
    "\n",
    "## Interpretación de la distribución\n",
    "- Una media cercana a 0 y varianza controlada indican una activación normalizada y estable.\n",
    "- Valores extremos en la cola del histograma representan \"picos\" de activación.\n",
    "- Estos picos suelen corresponder a dimensiones que el modelo usa para tomar decisiones clave.\n",
    "\n",
    "En la siguiente sección, se aplicará reducción de dimensionalidad (PCA o t-SNE) para visualizar estas representaciones en un espacio 2D o 3D y analizar agrupamientos semánticos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización avanzada de las activaciones de la MLP\n",
    "\n",
    "### Reducción de dimensionalidad con PCA\n",
    "\n",
    "Las activaciones que hemos capturado son vectores en un espacio de alta dimensión ($d=2048$). Para poder interpretarlas visualmente, usaremos **Análisis de Componentes Principales (PCA)**, que permite proyectar estos datos a un espacio de menor dimensión (usualmente 2D o 3D), preservando la mayor cantidad posible de varianza.\n",
    "\n",
    "---\n",
    "\n",
    "###  Explicación matemática de PCA (breve)\n",
    "\n",
    "Dado un conjunto de datos $X$ con $m$ muestras y $d$ dimensiones, PCA:\n",
    "1. Centra los datos restando la media.\n",
    "2. Calcula la matriz de covarianza $C = X^T X / m$.\n",
    "3. Obtiene los vectores propios y valores propios de $C$.\n",
    "4. Ordena esos vectores propios y selecciona los primeros $k$ para proyectar los datos en un subespacio de dimensión $k$.\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 1️: Preparar los datos para PCA\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Extraemos el tensor de activaciones (ejemplo: capa n)\n",
    "activaciones_tensor = activaciones_mlp[f'capa_{n}']  # Shape: (batch_size, seq_length, hidden_dim)\n",
    "\n",
    "# Convertimos el tensor en un arreglo 2D: (batch_size * seq_length, hidden_dim)\n",
    "activaciones_2d = activaciones_tensor.reshape(-1, activaciones_tensor.shape[-1]).numpy()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 2️: Aplicar PCA\n",
    "\n",
    "```python\n",
    "# Aplicamos PCA para reducir a 2 dimensiones\n",
    "pca = PCA(n_components=2)\n",
    "activaciones_pca = pca.fit_transform(activaciones_2d)\n",
    "\n",
    "# Información sobre varianza explicada\n",
    "print(\"Varianza explicada por cada componente:\", pca.explained_variance_ratio_)\n",
    "```\n",
    "\n",
    "**Salida esperada:**\n",
    "- Un array con dos valores que indican qué porcentaje de la varianza original es explicada por cada componente.\n",
    "- Ejemplo:\n",
    "```text\n",
    "Varianza explicada por cada componente: [0.35 0.20]\n",
    "```\n",
    "Esto significaría que el primer componente retiene el 35% de la varianza y el segundo el 20%.\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 3️: Graficar las activaciones proyectadas\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(activaciones_pca[:, 0], activaciones_pca[:, 1], alpha=0.3, s=10)\n",
    "plt.title(f\"Proyección PCA de las activaciones de la capa {n}\")\n",
    "plt.xlabel(\"Componente principal 1\")\n",
    "plt.ylabel(\"Componente principal 2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Salida esperada:**\n",
    "- Un gráfico de dispersión (scatter plot) donde cada punto representa una posición de token proyectada a 2D.\n",
    "- Pueden aparecer nubes densas o grupos de puntos, lo que indica que las activaciones tienen patrones estructurados.\n",
    "\n",
    "---\n",
    "\n",
    "En la siguiente sección explicaremos cómo interpretar estas agrupaciones y qué significan en el contexto de la interpretabilidad mecanicista de transformers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretación de las agrupaciones observadas en PCA\n",
    "\n",
    "Una vez que hemos proyectado las activaciones en 2D, podemos analizar visualmente los patrones:\n",
    "\n",
    "### ¿Qué representan los grupos o nubes de puntos?\n",
    "- Cada punto en el gráfico corresponde a un vector de activación para un token específico dentro del lote de entrada.\n",
    "- Grupos densos indican que hay tokens o posiciones que comparten representaciones internas similares.\n",
    "- Puntos alejados o dispersos pueden representar outliers o tokens con roles especiales (por ejemplo, delimitadores o símbolos que la red trata de manera particular).\n",
    "\n",
    "---\n",
    "\n",
    "### Ejemplos de interpretación:\n",
    "- Si observamos una nube central muy densa con algunas ramas o salidas laterales, esto indica que la mayoría de las activaciones están cerca de un espacio común, pero ciertos tokens activan dimensiones específicas.\n",
    "- Si aparecen subgrupos claramente diferenciados, puede ser evidencia de clústers semánticos: grupos de tokens con funciones similares.\n",
    "\n",
    "---\n",
    "\n",
    "## Visualización avanzada con t-SNE \n",
    "\n",
    "El algoritmo **t-SNE** (t-distributed Stochastic Neighbor Embedding) es una técnica de reducción de dimensionalidad no lineal, ideal para visualizar agrupaciones y relaciones locales.\n",
    "\n",
    "### Paso 1️: Aplicar t-SNE\n",
    "\n",
    "```python\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Reducimos primero con PCA a 50 dimensiones para mayor eficiencia\n",
    "pca_50 = PCA(n_components=50)\n",
    "activaciones_pca50 = pca_50.fit_transform(activaciones_2d)\n",
    "\n",
    "# Ahora aplicamos t-SNE\n",
    "print(\"Ejecutando t-SNE... (puede tardar unos segundos)\")\n",
    "tsne = TSNE(n_components=2, perplexity=30, learning_rate=200, n_iter=1000, verbose=1)\n",
    "activaciones_tsne = tsne.fit_transform(activaciones_pca50)\n",
    "```\n",
    "\n",
    "### Paso 2️: Graficar la proyección t-SNE\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(activaciones_tsne[:, 0], activaciones_tsne[:, 1], alpha=0.4, s=10, cmap=\"viridis\")\n",
    "plt.title(f\"Visualización t-SNE de las activaciones de la capa {n}\")\n",
    "plt.xlabel(\"Dimensión 1\")\n",
    "plt.ylabel(\"Dimensión 2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Interpretación de t-SNE\n",
    "- t-SNE permite observar relaciones locales: puntos cercanos en el gráfico representan activaciones similares.\n",
    "- Clusters bien definidos son evidencia de estructuras internas aprendidas por la red.\n",
    "- Si observamos brazos o ramificaciones, pueden indicar caminos de transformación de información desde representaciones generales a específicas.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparando activaciones de la MLP entre diferentes prompts\n",
    "\n",
    "Para entender aún más la interpretabilidad mecanicista, es útil comparar cómo cambian las activaciones de la MLP al variar el texto de entrada.\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 1️: Definir diferentes prompts\n",
    "\n",
    "```python\n",
    "prompts = [\n",
    "    \"La inteligencia artificial está transformando la educación.\",\n",
    "    \"La biología estudia los seres vivos y su entorno.\",\n",
    "    \"Las matemáticas son el lenguaje del universo.\",\n",
    "    \"El arte es una forma de expresión humana.\"\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 2️: Capturar las activaciones para cada prompt\n",
    "\n",
    "```python\n",
    "resultados_prompts = {}\n",
    "\n",
    "for idx, prompt in enumerate(prompts):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Interceptar con hook\n",
    "    handle = model.model.layers[n].mlp.register_forward_hook(guardar_salida_mlp(f\"prompt_{idx}\"))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _ = model(**inputs)\n",
    "    \n",
    "    resultados_prompts[f'prompt_{idx}'] = activaciones_mlp[f'prompt_{idx}']\n",
    "    handle.remove()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 3️: Visualizar comparando prompts\n",
    "\n",
    "Podemos graficar histogramas superpuestos:\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for idx, prompt in enumerate(prompts):\n",
    "    activaciones = resultados_prompts[f'prompt_{idx}'].flatten().numpy()\n",
    "    plt.hist(activaciones, bins=100, alpha=0.3, density=True, label=f'Prompt {idx+1}')\n",
    "\n",
    "plt.title(f\"Comparación de distribuciones de activaciones en la capa {n}\")\n",
    "plt.xlabel(\"Valor de activación\")\n",
    "plt.ylabel(\"Frecuencia relativa\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Interpretación\n",
    "- Si las distribuciones son similares, la capa MLP está respondiendo de forma general.\n",
    "- Diferencias notables entre prompts indican especialización: ciertas dimensiones se activan más según el tipo de entrada.\n",
    "- Esto respalda la idea de que la MLP refina información contextual.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones y recomendaciones\n",
    "\n",
    "### Conclusiones principales:\n",
    "1. **La MLP en cada bloque Transformer** actúa como un refinador de representaciones, expandiendo y comprimiendo la información después de la atención.\n",
    "2. **Las activaciones de la MLP** son vectores en espacios de alta dimensión que muestran estructuras internas, evidenciadas por patrones y agrupaciones cuando se proyectan a 2D.\n",
    "3. **La variación en las activaciones entre diferentes prompts** revela que la red ajusta sus representaciones dependiendo del contenido semántico de la entrada.\n",
    "4. Las herramientas de reducción de dimensionalidad como **PCA** y **t-SNE** permiten visualizar y entender mejor la estructura interna de las activaciones.\n",
    "\n",
    "---\n",
    "\n",
    "###  Recomendaciones para futuros análisis:\n",
    "- Probar con más prompts y dominios temáticos (ciencia, arte, política) para observar especialización.\n",
    "- Realizar análisis estadísticos sobre las activaciones para medir dispersión, concentración y valores extremos.\n",
    "- Comparar activaciones entre capas tempranas y capas tardías para ver cómo evoluciona la información.\n",
    "- Investigar qué dimensiones de la MLP dominan las salidas y cómo están correlacionadas con tokens clave.\n",
    "\n",
    "---\n",
    "\n",
    "###  Resumen para anexar al reporte:\n",
    "- Se explicó formalmente qué es la MLP y su función dentro del modelo Llama 3.2.\n",
    "- Se mostró cómo interceptar la salida de la MLP utilizando hooks.\n",
    "- Se analizaron las activaciones obtenidas mediante visualización básica y avanzada.\n",
    "- Se compararon las respuestas del modelo ante diferentes prompts.\n",
    "- Se plantearon recomendaciones para un análisis más profundo en proyectos de interpretabilidad.\n",
    "\n",
    "Este análisis forma una base sólida para demostrar comprensión teórica, capacidad práctica y análisis crítico en el proyecto de interpretabilidad mecanicista de transformers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
