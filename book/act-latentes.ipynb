{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretación de latentes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import jaxtyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joulesd/mech-interp/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint cached at -> /home/joulesd/.cache/huggingface/hub/models--mech-interp-uam--llama3.2-1b-sae/snapshots/891afeb81f0a19d6a791b6b8a3110bc3d87c3f5f/sae.pth\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "ckpt_path = hf_hub_download(\n",
    "    repo_id=\"mech-interp-uam/llama3.2-1b-sae\",\n",
    "    filename = \"sae.pth\",\n",
    ")\n",
    "print(f\"Checkpoint cached at -> {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sae(\n",
       "  (enc): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "  (dec): Linear(in_features=16384, out_features=2048, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "if next(iter(state_dict)).startswith(\"module.\"):\n",
    "    state_dict = {k.replace(\"module.\", \"\", 1): v\n",
    "                  for k, v in state_dict.items()}\n",
    "\n",
    "missing, unexpected = sae.load_state_dict(state_dict, strict=False)\n",
    "if missing or unexpected:\n",
    "    print(\"Key mismatch:\", missing, unexpected)\n",
    "\n",
    "sae.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae import Sae\n",
    "sae = Sae(\n",
    "    d_in = 2048, \n",
    "    d_sae=2048*8, \n",
    "    use_pre_enc_bias=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recon shape: torch.Size([])\n",
      "Latent L0  : 8294.0\n"
     ]
    }
   ],
   "source": [
    "in_dim = sae.enc.in_features\n",
    "dummy = torch.randn(1, in_dim, device = next(sae.parameters()).device)\n",
    "with torch.no_grad():\n",
    "    recon_dict= sae(dummy)\n",
    "print(\"Recon shape:\", recon_dict[\"reconstruction\"].shape)\n",
    "print(\"Latent L0  :\", recon_dict[\"l0\"].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = sae(dummy)\n",
    "print(type(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cargar datos con id's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=API_KEY) if API_KEY else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\n",
    "def simulate_activating_examples_gpt(api_key, n=10) -> list[tuple[int, list[str]]]:\n",
    "   \n",
    "   # Simula ejemplos de activación máxima en frases relacionadas con comunicación.\n",
    "    #Regresa el mismo formato que fetch_max_activating_examples().\n",
    "    \n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    system_prompt = (\n",
    "    \"You are a fluent, context-aware asssistan that wirtes natural, varied, and meaningful sentences\"\n",
    "    \"related to the concept of commnication. Yout output will be used  to study neuron activations in a language model\"\n",
    "    \"Each sentence must contain exactly one significant token related to communication, and that token must be wrapped with double angle brackets like this: <<talk>>\"\n",
    "    \"Avoid generic language or filler. Ensure the token is conceptually central to the sentence's meaning\"\n",
    "    )\n",
    "    user_prompt = (\n",
    "        f\"Generate {n} short sentences (10 to 15 words each) about communication. \"\n",
    "        \"In each sentence, mark exactly one key word related to communication ussing double angle like this: <<talk>>\" \n",
    "        \"Return only the sentences, one per line, with no bullet poinsts, numbering, or explanations\"\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        max_tokens=300,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    raw_sentences = response.choices[0].message.content.strip().split(\"\\n\")\n",
    "    raw_sentences = [s.strip().lstrip(\"0123456789. \") for s in raw_sentences if s.strip()]\n",
    "\n",
    "    parsed = []\n",
    "    for i, sentence in enumerate(raw_sentences):\n",
    "        tokens = sentence.split()\n",
    "        for j, tok in enumerate(tokens):\n",
    "            if tok.startswith(\"<<\") and tok.endswith(\">>\"):\n",
    "                tokens[j] = f\"<<{tok.strip('<>')}>>\"\n",
    "                break\n",
    "        parsed.append((i, tokens))\n",
    "\n",
    "    return parsed\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def create_prompt_gpt4o_from_simulated(examples: list[tuple[int, list[str]]], use_chain_of_thought=True) -> dict[str, str]:\n",
    "    \n",
    "    #Construye un prompt de GPT-4o a partir de ejemplos simulados con tokens activadores marcados.\n",
    "    \n",
    "    formatted = \"\\n\".join(f\"{i+1}. {' '.join(tokens)}\" for i, tokens in examples)\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are analyzing a latent neuron in a transformer-based language model. \"\n",
    "        \"Each sentence below contains one token that strongly activates this latent neuron, highlighted using << >>. \"\n",
    "        \"Your task is to interpret what concept, theme, or category this neuron responds to. \"\n",
    "        \"Be precise but not overly narrow. Use fewer than 20 words. \"\n",
    "        \"Avoid punctuation, lists, formatting, or generic labels like 'words' or 'nouns'. \"\n",
    "        \"Focus on shared semantic meaning across the highlighted tokens.\"\n",
    "    )\n",
    "\n",
    "    assistant_prompt = (\n",
    "        \"Start by mentally grouping the highlighted tokens into a conceptual category. \"\n",
    "        \"Then, write a final interpretation in fewer than 20 words. \"\n",
    "        \"This neuron activates on\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"system\": system_prompt,\n",
    "        \"user\": f\"The activating examples are:\\n\\n{formatted}\",\n",
    "        \"assistant\": assistant_prompt,\n",
    "    }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt4o_explanation_from_prompt(prompts: dict, n_completions=3, max_tokens=100) -> list[str]:\n",
    "    \"\"\"\n",
    "    Llama a la API de GPT-4o con un prompt ya generado y devuelve las interpretaciones.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompts[\"system\"]},\n",
    "            {\"role\": \"user\", \"content\": prompts[\"user\"]},\n",
    "            {\"role\": \"assistant\", \"content\": prompts[\"assistant\"]},\n",
    "        ],\n",
    "        n=n_completions,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return [choice.message.content.strip() for choice in response.choices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY no está configurada.\n"
     ]
    }
   ],
   "source": [
    "if API_KEY:\n",
    "    #simulated_examples = simulate_activating_examples_gpt(api_key=API_KEY, n=10)\n",
    "    #prompt = create_prompt_gpt4o_from_simulated(simulated_examples, use_chain_of_thought=True)\n",
    "    explanations = get_gpt4o_explanation_from_prompt(prompt, n_completions=3)\n",
    "\n",
    "    for i, exp in enumerate(explanations):\n",
    "        print(f\"[Explicación {i+1}]: {exp}\")\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY no está configurada.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
