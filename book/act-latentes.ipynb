{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretación de latentes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joulesd/mech-interp/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import Iterator, Tuple, Dict, List\n",
    "\n",
    "import torch, numpy as np\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ID   = \"mech-interp-uam/llama-mlp8-outputs\"  \n",
    "repo_id  = \"mech-interp-uam/llama3.2-1b-sae\" \n",
    "CACHE_DIR    = Path(\"/home/joulesd/hf_cache\")\n",
    "batch_size   = 1024\n",
    "top_k       = 20\n",
    "act_thr      = 0.0\n",
    "device       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype        = torch.float16  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 668501/668501 [00:52<00:00, 12803.12 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\n",
    "    dataset_ID,\n",
    "    split=\"train\",                    \n",
    "    cache_dir=CACHE_DIR,\n",
    "    streaming=False                   \n",
    ").with_format(\"torch\", columns=[\"activations\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sae'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msae\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sae                                    \u001b[38;5;66;03m# o la ruta correcta\u001b[39;00m\n\u001b[32m      3\u001b[39m state_path = hf_hub_download(repo_id, \u001b[33m\"\u001b[39m\u001b[33msae.pth\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m state_dict = torch.load(state_path, map_location=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sae'"
     ]
    }
   ],
   "source": [
    "from sae import Sae                                    # o la ruta correcta\n",
    "\n",
    "state_path = hf_hub_download(repo_id, \"sae.pth\")\n",
    "state_dict = torch.load(state_path, map_location=\"cpu\")\n",
    "sae = Sae(d_in=2048, d_sae=2048*8, use_pre_enc_bias=True) \\\n",
    "        .to(device, dtype).eval()\n",
    "_ = sae.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_stats_and_topk(\n",
    "    dataset,\n",
    "    sae_model: Sae,\n",
    "    k: int = top_k,\n",
    "    batch_size: int = batch_size,\n",
    "    act_thr: float = act_thr,\n",
    ") -> Tuple[np.ndarray,\n",
    "           Dict[int, List[Tuple[float, int]]]]:\n",
    "    d_sae   = sae_model.W_enc.shape[0]        \n",
    "    counts  = np.zeros(d_sae, dtype=np.int64)\n",
    "    heaps   = {i: [] for i in range(d_sae)} \n",
    "\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    example_offset = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            acts = batch[\"activations\"].to(device, dtype) \n",
    "            lat  = sae_model(acts)[\"latent\"].cpu().numpy()     \n",
    "            activated = lat > act_thr\n",
    "            counts += activated.sum(axis=0)\n",
    "\n",
    "            for row_idx, vec in enumerate(lat):\n",
    "                global_idx = example_offset + row_idx\n",
    "                for j, val in enumerate(vec):\n",
    "\n",
    "                    if len(heaps[j]) < k:\n",
    "                        heapq.heappush(heaps[j], (val, global_idx))\n",
    "                    elif val > heaps[j][0][0]:\n",
    "                        heapq.heapreplace(heaps[j], (val, global_idx))\n",
    "\n",
    "            example_offset += lat.shape[0]\n",
    "\n",
    "    topk = {j: sorted(h, reverse=True) for j, h in heaps.items()}\n",
    "    return counts, topk\n",
    "\n",
    "import heapq\n",
    "counts, topk_by_neuron = collect_stats_and_topk(ds, sae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_topk_batches(\n",
    "    dataset,\n",
    "    topk_dict: Dict[int, List[Tuple[float, int]]],\n",
    "    batch_size: int = batch_size,\n",
    ") -> Iterator[Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]:\n",
    "    samples = []\n",
    "    for j, lst in topk_dict.items():\n",
    "        for rank, (_, ex_idx) in enumerate(lst):\n",
    "            samples.append((ex_idx, j, rank))\n",
    "    samples.sort()\n",
    "\n",
    "\n",
    "    unique_idx = sorted({ex for ex, _, _ in samples})\n",
    "    id2acts = {i: dataset[i][\"activations\"] for i in unique_idx}\n",
    "\n",
    "    acts_batch, lat_batch, meta_batch = [], [], []\n",
    "    for ex_idx, neuron_id, rank in samples:\n",
    "        vec = id2acts[ex_idx].unsqueeze(0).to(device, dtype)   \n",
    "        with torch.no_grad():\n",
    "            lat = sae(vec)[\"latent\"]\n",
    "\n",
    "        acts_batch.append(vec.squeeze(0).cpu())\n",
    "        lat_batch .append(lat.squeeze(0).cpu())\n",
    "        meta_batch.append(torch.tensor([neuron_id, rank]))\n",
    "\n",
    "        if len(acts_batch) == batch_size:\n",
    "            yield (torch.stack(acts_batch),\n",
    "                   torch.stack(lat_batch),\n",
    "                   torch.stack(meta_batch))\n",
    "            acts_batch, lat_batch, meta_batch = [], [], []\n",
    "\n",
    "    if acts_batch:\n",
    "        yield (torch.stack(acts_batch),\n",
    "               torch.stack(lat_batch),\n",
    "               torch.stack(meta_batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, openai\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "def explain_neuron(neuron_id: int, topk_lat: torch.Tensor) -> str:\n",
    "    prompt = f\"\"\"\n",
    "    Eres un asistente que interpreta neuronas latentes de un SAE.\n",
    "    La neurona #{neuron_id} tiene estas {len(topk_lat)} activaciones top-k:\n",
    "\n",
    "    {topk_lat.tolist()}\n",
    "\n",
    "    Explica en español qué concepto lingüístico o semántico capta\n",
    "    esta neurona y da dos ejemplos de frases donde esperas que se active.\n",
    "    \"\"\"\n",
    "    resp = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\":\"user\",\"content\": prompt}],\n",
    "        temperature=0.3,\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for acts, lat, meta in iter_topk_batches(ds, topk_by_neuron, 64):\n",
    "    for i in range(lat.shape[0]):\n",
    "        nid   = int(meta[i,0])\n",
    "        rank  = int(meta[i,1])\n",
    "        lvec  = lat[i]\n",
    "        if rank == 0:        \n",
    "            explanation = explain_neuron(nid, lat[meta[:,0]==nid])\n",
    "            print(f\"Neurona {nid}:\")\n",
    "            print(explanation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dead = np.where(counts == 0)[0]\n",
    "print(f\"Neuronas completamente inactivas: {len(dead)} / {len(counts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_by_neuron = {j: lst for j, lst in topk_by_neuron.items() if counts[j] > 0}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
