{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducción\n",
    "\n",
    "* Definición de Transformers y su relevancia en el aprendizaje profundo.\n",
    "\n",
    "    Los transformadores son un tipo de arquitectura de red neuronal que transforma o cambia una secuencia de entrada en una secuencia de salida. Para ello, aprenden el contexto y rastrea las relaciones entre los componentes de la secuencia de entrada: \"¿De qué color es el cielo?\". El modelo transformador usa una representación matemática interna que identifica la relevancia y la relación entre las palabras color, cielo y azul. Usa esa información para generar el resultado: \"El cielo es azul\".\n",
    "    Actualmente, las organizaciones usan modelos de transformadores para todo tipo de conversiones de secuencias, desde el reconocimiento de voz hasta la traducción automática y el análisis de secuencia de proteínas.\n",
    "\n",
    "    Los modelos tempranos de aprendizaje profundo que se centraron ampliamente en las tareas de procesamiento de lenguaje natural (NLP) tenían como fin lograr que las computadoras comprendan al lenguaje humano natural. Adivinaron la palabrasiguiente en una secuencia basada en la palabra anterior.\n",
    "    Los primeros modelos de machine learning (ML) aplicaban una tecnología similar a una escala más amplia. Trazaban la frecuencia de relación entre diferentes pares de palabras o grupos de palabras en su conjunto de datos de entrenamiento e intentaban adivinar la siguiente palabra. Sin embargo, la tecnología primitiva no podía retener el contexto más allá de una determinada longitud de entrada. Por ejemplo, uno de los primeros modelos de ML no podía generar un párrafo significativo porque no podía retener el contexto entre la primera y la última oración de un párrafo.\n",
    "\n",
    "    Con los modelos de transformadores, puede utilizar técnicas como el aprendizaje por transferencia y la generación aumentada de recuperación (RAG). Estas técnicas permiten la personalización de los modelos existentes para aplicaciones específicas de la organización del sector. Los modelos pueden entrenarse previamente en conjuntos de datos grandes y, a continuación, ajustarse con precisión en conjuntos de datos más pequeños y específicos para tareas específicas. Este enfoque ha democratizado el uso de modelos sofisticados y ha eliminado las limitaciones de recursos en el entrenamiento de modelos grandes desde cero. Los modelos pueden funcionar bien en varios dominios y tareas para diversos casos de uso.\n",
    "\n",
    "\n",
    "\n",
    "* Breve descripción de redes neuronales previas (MLP, RNN, LSTM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Redes neuronales Feedforward (MLP)\n",
    "\n",
    "* Definición de MLP  y su estructura: capas de entrada, ocultas y salida.\n",
    "    Son la quinta-esencia de los modelos de aprendizaje profundo, el objetivo de de una red feedforward es aproximar alguna función $f^*$. Por ejemplo, para clasificar $y = f^*(x)$ mapea una entrada x a la categoría y. Una red feedforward define el mapeo $y = f(x,\\theta)$ y aprende el valor del parámetro $\\theta$ que resulta en la mejor aproximación de la función. \n",
    "\n",
    "    Estos modelos son llamados feedforward porque la información fluye a través de la función evaluada en x, a través de cálculos intermedios se suele definir $f$  y finalmente la salida y. Aquí no hay conexiones feedback en las que la red está extiende hacia atrás en sí misma. Cuando las redes feedforward se extienden para incluir conexiones feedback, son llamadas Redes Recurrentes.\n",
    "\n",
    "* Matemáticas del Feedforward: Cálculo de la salida de una red neuronal.\n",
    "    Dado un modelo con $L$ capas ocultas, cada capa realiza la siguiente operación:\n",
    "    $$ z^{(l)} = W^{(l)}a^{(l-1)} + b^{(l)}$$\n",
    "    $$ a^{(l)} = \\sigma(z^{(l)})$$\n",
    "    * l es el indice de la capa\n",
    "    * $x = a^{(0)}$ es la entrada inicial\n",
    "    * $z^{(l)}$ es la activación antes de aplicar la función de activación.\n",
    "    * $W{(l)}$ es la matriz de pesos de la capa l\n",
    "    * $b^{(l)}$ es el vector de sesgos (bias)\n",
    "    * $\\sigma$ es la función de activación\n",
    "    * $a^{(l)}$ es la salida de la capa de la activación\n",
    "\n",
    "    Para ampliar los modelos lineales para representar funciones no lineales de x, podemos aplicar el modelo lineal no a x en sí mismo, sino a una entrada transformada $\\phi(x)$, donde $\\phi$ es una transformación no lineal. La estrategia del aprendizaje profundo consiste en aprender $\\phi$. En este enfoque, tenemos el modelo $y = f(x; \\theta, w) = \\phi(x; \\theta)^Tw$. Ahora tenemos los parámetros $\\theta$ que utilizamos para aprender $\\phi$ a partir de una clase amplia de funciones, y parámetros $w$ que asignan de $\\phi(x)$ a la salida deseada. En este enfoque, se parametriza la representación como $\\phi(x;\\theta)$ y se utiliza el algoritmo de optimización para definir el $\\theta$ que corresponde a una buena representación.\n",
    "\n",
    "\n",
    "\n",
    "* Función de activación: ReLU,  Sigmoid, Tanh\n",
    "    La función ReLU \n",
    "    $$f_{relu} : \\Re \\rightarrow [0, \\infty]$$\n",
    "    $$z \\rightarrow max[0,z] $$\n",
    "    es llamada la función de parte positiva o función rampa, es la función identidad para los argumentos positivos y la función constante para el valor cero y los negativos. Un claro beneficio de l afunción ReLU es que tanto la función en sí, como sus derivadas sosn fáciles de aplicar con bajo costo computacional.\n",
    "\n",
    "    La función sigmoide\n",
    "    $$f_{log}: \\Re \\rightarrow (0,1)$$\n",
    "    $$z \\rightarrow \\frac{1}{1+e^{-z}}$$\n",
    "    es una función acotada y diferenciable que no es decreciente y tiene exactamente un punto de inflexión es decir, es una curva suave. Como las funciones sigmoides \"aplastan\" los valores reales en un intervalo, a veces se denominan funciones de aplastamiento. La activación sigmoidea tiene una larga tradición en la teoría y la practica de las redes neuronales. Una motivación ha sido la interpretación de funciones sigmoides como aproximaciones matemáticamente manejables de las funciones escalonadas.\n",
    "\n",
    "    La función\n",
    "    $$f_{tanh}: \\Re \\rightarrow (-1,1)$$\n",
    "    $$z \\rightarrow tanh[z]:= \\frac{e^z-e^{-z}}{e^z+e^{-z}}$$\n",
    "    es llamada la función tangente hipérbolica, es infinitamente diferenciable. Más aún, las series de Taylor de tanh y arctan coinciden hasta el cuarto orden, Así podemos pensar en tanh como una versión desplazada y escalada de la función logistica o como una aproximación de arctan. En concreto, tanh combina dos características populares de la función logistica y de arctan: la derivada de tanh es una función sencilla de la función original y está centrada en cero. \n",
    "\n",
    "* Backpropagation en MLP: \n",
    "    Algoritmo de retropropagación para el ajuste de pesos.\n",
    "    La retropropagación consiste en comprender cómo el cambio de los pesos y los sesgos de una red modifica la función de coste. En última instancia, esto significa calcular las derivadas parciales $\\frac{\\partial C}{\\partial {w_{jk}}^l}$ y $\\frac{\\partial C}{\\partial {b_j}^l}$, pero para calcularlas, primero se introduce una cantidad intermedia $\\delta_j^l$, que es el error en el nodo j-ésimo.\n",
    "    Hay cuatro ecuaciones principales que interactúan para calcular cómo cambian los pesos de una red neuronal y minimizar el error:\n",
    "    1. El error en la salida $\\delta^L = \\nabla_aC \\odot \\sigma'(z^L)$, donde $\\odot$ denota el producto de Hadamard (elemento a elemento).\n",
    "    2. El error en la capa l en términos del error en la capa l+1: $\\delta^l = ((W^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^l)$.\n",
    "    3. La tasa de cambio de la función de coste con respecto a los sesgos: $\\frac{\\partial C}{\\partial b_j^l} = \\delta_j^l$.\n",
    "    4. La tasa de cambio de la función de coste con respecto a los pesos: $\\frac{\\partial C}{\\partial w_{jk}^l} = a_k^{l-1} \\delta_j^l$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Redes recurrentes (RNN, LSTM)\n",
    "\n",
    "* RNN: Estructura y uso en secuencias.\n",
    "\n",
    "* Backpropagation Through Time (BPTT): Extensión de retropropagación para redes recurrentes.\n",
    "\n",
    "* LSTM y GRU: Solución al problema anterior mediante puertas.\n",
    "\n",
    "* Feedforward en RNN: Cálculo del estado oculto.\n",
    "\n",
    "* Backpropagation en LSTM: Algoritmo para actualizar pesos a través de las puertas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transformers y Autoantención \n",
    "\n",
    "\n",
    "* **Atención**  \n",
    "  La atención captura las relaciones entre todos los elementos de la secuencia. Luego, la capa feedforward refina estas representaciones. Ambas acciones se repiten en varias capas apiladas para mejorar la abstracción de la información.\n",
    "\n",
    "  La función de atención puede ser descrita como una operación entre tres secuencias de vectores: **Query (Q)**, **Keys (K)**, y **Values (V)**\n",
    "\n",
    "  La salida se calcula como una suma ponderad de los values, donde el peso asignado a cada valor se obtiene mediante una función de compatibilidad basada en el producto punto entre la query y la key:\n",
    "\n",
    "  Un primera definición podría ser\n",
    "  $$\n",
    "  r_i = \\sum_j^T (q_i \\cdot k_j) v_j\n",
    "  $$\n",
    "  donde podríamos pensar que el resultado va a incorporar información en el elemento $i$ usando los otros elementos de la secuencia, un solo vector $q_i$ dicta cual información\n",
    "  se va a incorporar de los vectores valor al resultado $r_i$.\n",
    "\n",
    "  Notemos que si todos esos vectores punto estuvieran limitados a ser 0 o ser 1, podemos definir un mapeo con la expreción de arriba, que tome $q_i$ y nos devuelva $v_i$, justo como en una base de datos; uno provee una consulta (\"querry\" en inglés), la cual tiene un resultado correspondiente $v_i$. Pero, notemos la formulación de arriba es más general, uno podría pensar en el como \"una base de datos diferenciable.\n",
    "\n",
    "  Sin embargo, hay dos problemas\n",
    "  - Si consideramos todos los componentes de los vectores querry y key como independientes, con varianza 1 y con esperanza 0, la varianza de cada producto punto entre un vector querry y un vector key sería $d_k$, la dimensión de tales vectores, lo cual va al contrario de la prevalente practica de mantener varianza constante mientras estos se van aplicando capas y capas del modelo.\n",
    "  - Si vemos esa definición de attención como un mapeo lineal que toma un vector con todos los componentes de los todos los vectores querry, key, y value, y saca un solo vector con todos los componentes de todos los vectores resultado, nos daremos cuenta que estamos ante una función lineal.\n",
    "\n",
    "  El primer problema lo resolvemos dividiendo cada uno de esos productos punto entre $\\sqrt{d_k}$. El segundo al aplicar $\\text{softmax}$ a esos coefficientes de la suma ponderada\n",
    "\n",
    "  $$\n",
    "  \\alpha_{i,j} = \\text{softmax}(q_i \\cdot k_1, ..., q_i \\cdot k_T) \\\\\n",
    "  r_i = \\sum_j^T \\alpha_{i,j} v_j\n",
    "  $$\n",
    "\n",
    "  Esto nos lleva a la formulación en términos de matrices\n",
    "\n",
    "  $$\n",
    "  \\text{Atención}(Q,K,V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "  $$\n",
    "\n",
    "  Es decir, cada vector resultado es la suma ponderada (de hecho, la combinación convexa), de los vectores value.\n",
    "\n",
    "\n",
    "* **Autoatencción**\n",
    "La autoatención hace que conforme los elementos de la secuencia $h_i$ vayan avanzando a lo largo del transformer, estos puedan intercambiar información unos con otros.\n",
    "\n",
    "Esto se logra mediante tres matrices de parámetos, digamos $W_Q$, $W_K$, $W_V$, que se aplican a la secuencia $h_1, ..., h_T$ para obtener secuencias $q_1, ..., q_T$, $k_1, ..., k_T$, y $v_1, ... v_T$, y sobre estas haciendo la operación de atención.\n",
    "\n",
    "Uno usaría autoantención si pretende codificar una imágen (o más generalmente, cualquier colección de vectores, los cuales pueden tener o no una noción de posición), a una secuencia de vectores $z_i$ con información reelevantes para, por ejemplo, para un clasificador de imágenes. De hecho, esto es lo que se usa para obtener una representación vectorial de imágenes que luego puede ser usada para darle la capasidad de ver imágenes (condicionar la salida con respecto a una imágen extra en la entrada) a grandes modelos de lenguage o modelos generadores de imágenes.\n",
    "\n",
    "* **Autoatencción Enmascarada**\n",
    "Si deseamos que nuestro modelo aprenda a generar un vector de salida $o_i$ condicionado a la secuencia $e_1, ..., e_i$ de vectores de entrada, es eliminando toda forma de que los vectores $e_i, ... e_T$ puedan influenciar la salida $o_i$. Esto, en la capa de attención significa que la combinación convexa y el $\\text{softmax}$ sea solo usando $v_j$ y $k_j$ con $j \\le i$\n",
    "\n",
    "* **Atención cruzada**\n",
    "Supongamos queremos usar información de una secuencia en otra, primero podríamos codificarla en una secuencia $z_i$ y luego realizar una operación de attención donde los vectores querry vengan de la secuencia que condiciona, y los vectores value y key vengan de la secuencia que se está condicionando. Podemos pensar en esto como cada vector $z_i$ combinando los elementos de la secuencia original $v_i$, usando informacción que proviene de la secuencia original, $q_i$. Notemos que aquí también podemos limitar nuestra combinación convexa, para los mismos fines mencionados anteriormente.\n",
    "\n",
    "En resumen, para los tres casos de atención usamos tres matrices para obtener los vectores querry, key, y value, pero el origen de los vectores a los cuales les aplicamos las matrices depende en el caso de la atención cruzada y la combinación convexa tiene menos términos en el caso de la atención enmascarada.\n",
    "\n",
    "\n",
    "Si queremos un transformer que pueda hacer traducción de inglés a español, podríamos usar una red neuronal con attención para codificar la oración en inglés a una secuencia de vectores $z_1, ..., z_S$, y otra con attención enmascarada, entrenada para que dada una entrada $e_1, ..., e_T$, producir una secuencia $o_1, ... o_T$, donde cada $o_i$ es una distribución condicionada en $e_1, ..., e_i$ y en la codificación $z_1, ..., z_S$.\n",
    "\n",
    "Ahora si podemos definir qué es un transformer\n",
    "\n",
    "* Arquitectura Transformer: Componentes principales (atención, capas feedforward).\n",
    "    En el paper *Attention is all you need*, tenemos una descripción del Transformer que es la que se detalla a continuación, sin embargo, los parámetros que se proponen suelen variar según las necesidades del modelo. Actualmente, la arquitectura Transformer ha sido adaptada y optimizada para diversas aplicaciones, demostrando su flexibilidad y eficacia en múltiples campos de la inteligencia artificial.\n",
    "    \n",
    "    El codificador mapea una secuencia de entrada de representaciones de símbolos $ (x_1,..., x_n) $ a una secuencia de representaciones continuas $z = (z_1,..., z_n) $. Dado $ z $, el decodificador genera una secuencia de símbolos de salida $ (y_1,...,y_m) $, de un elemento cada vez. En cada paso, el modelo es autoregresivo, consumiendo los símbolos generados previamente como entrada adicional al generar el siguiente paso. El Transformer sigue esta arquitectura general utilizando capas apiladas de autoatención y completamente conectadas punto a punto, tanto para el codificador como para el decodificador. En otras palabras, el encoder transforma la secuencia de entrada en una representación más rica y útil (los vectores $ z$), que contienen información semántica importante para que el decoder genere la salida correcta.\n",
    "\n",
    "    * **Encoder:**  \n",
    "        El codificador está compuesto por una pila de $ N = 6 $ capas idénticas. Cada capa tiene dos subcapas.\n",
    "\n",
    "        La primera capa es un mecanismo de autoatención con múltiples cabezas, la segunda es una red simple totalmente conectada en función de la posición. Se emplea una conexión residual (las cuales son enlaces adicionales que conectan algunas capas de una red neuronal con otras capas que no son directamente adyacentes), alrededor de cada una de las dos subcapas, seguida de una normalización de capas. Es decir, la salida de cada subcapa es  \n",
    "        $$\n",
    "        \\text{LayerNorm}\\left( x + \\text{sublayer}(x)\\right)\n",
    "        $$\n",
    "        donde $ \\text{sublayer}(x) $ es una función implementada por la propia subcapa. Para facilitar estas conexiones residuales, todas las subcapas del modelo, así como las capas embedding, producen salidas de dimensión  $d_{\\text{model}} = 512 $.\n",
    "\n",
    "    * **Decoder:**  \n",
    "        El decoder usa la representación generada por el encoder para generar una salida paso a paso. Lo hace siendo autoregresivo, es decir, en cada paso genera un token y lo usa como entrada en el siguiente paso. El decoder tiene dos mecanismos de atención: \n",
    "        - **Autoatención**, que relaciona palabras ya generadas entre sí.  \n",
    "        - **Atención cruzada** (*encoder-decoder attention*), que se enfoca en partes relevantes de $z $  \n",
    "\n",
    "        Luego, una capa feedforward ayuda a procesar la información antes de predecir el siguiente token.\n",
    "Para la capa de atención necesitamos transformadores de los datos de entrada:\n",
    "    * **Query (Q)**: es la consulta, es la representación de la palabra actual que está tratando de encontrar información relevante de la secuencia, se usa para comparar con las claves de todas las palabras de la secuencia.\n",
    "    * **Key (K)**: Es la representación de cada palabra en la secuencia, se compara con la consulta para determinar la relevancia de cada palabra con respecto a la palabra actual.\n",
    "    * **Value (V)**: Es la información contenida en cada palabra de la secuencia, se usa para generar la salida ponderada basada en la atención calculada.\n",
    "\n",
    "* **Aplicación de Feedforward en Transformers**  \n",
    "  Además de las subcapas de atención, cada una de las capas del codificador y del decodificador contiene una red feedforward totalmente conectada, que se aplica a cada posición de forma idéntica e independiente.  \n",
    "\n",
    "  Esta red consiste en dos transformaciones lineales con una activación ReLU intermedia:\n",
    "\n",
    "  $$\n",
    "  \\text{FFN}(x) = \\max(0, x W_1 + b_1) W_2 + b_2\n",
    "  $$\n",
    "\n",
    "  Aunque las transformaciones lineales son las mismas en distintas posiciones, utilizan parámetros diferentes de una capa a otra. Otra forma de describir esto es como dos convoluciones con tamaño de núcleo 1.  \n",
    "\n",
    "  La dimensionalidad de la entrada y la salida es $ d_{\\text{model}} = 512 $, y la capa interna tiene una dimensionalidad de $ d_{\\text{ff}} = 2048 $.\n",
    "\n",
    "\n",
    "\n",
    "* Backpropagation en Transformers: Cómo se aplica retropropagación en la capa de autoatención y en el feedforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparación entre Redes Neuronales y Transformers\n",
    "\n",
    "* Comparación entre MLP, RNN y Transformers\n",
    "\n",
    "* Ventajas de Transformers en tareas con NLP y visión\n",
    "\n",
    "* Evolución en el uso de Feedforward: De MLP a modelos complejos como los Transformers.\n",
    " * Impacto de Backpropagation en el entrenamiendo de modelos complejos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Mezcla de Arquitecturas (Transformers + Otras redes)\n",
    "\n",
    "* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
