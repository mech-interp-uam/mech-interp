{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducción\n",
    "\n",
    "* Definición de Transformers y su relevancia en el aprendizaje profundo.\n",
    "\n",
    "Los transformadores son un tipo de arquitectura de red neuronal que transforma o cambia una secuencia de entrada en una secuencia de salida. Para ello, aprenden el contexto y rastrea las relaciones entre los componentes de la secuencia de entrada: \"¿De qué color es el cielo?\". El modelo transformador usa una representación matemática interna que identifica la relevancia y la relación entre las palabras color, cielo y azul. Usa esa información para generar el resultado: \"El cielo es azul\".\n",
    "Actualmente, las organizaciones usan modelos de transformadores para todo tipo de conversiones de secuencias, desde el reconocimiento de voz hasta la traducción automática y el análisis de secuencia de proteínas.\n",
    "\n",
    "Los modelos tempranos de aprendizaje profundo que se centraron ampliamente en las tareas de procesamiento de lenguaje natural (NLP) tenían como fin lograr que las computadoras comprendan al lenguaje humano natural. Adivinaron la palabrasiguiente en una secuencia basada en la palabra anterior.\n",
    "Los primeros modelos de machine learning (ML) aplicaban una tecnología similar a una escala más amplia. Trazaban la frecuencia de relación entre diferentes pares de palabras o grupos de palabras en su conjunto de datos de entrenamiento e intentaban adivinar la siguiente palabra. Sin embargo, la tecnología primitiva no podía retener el contexto más allá de una determinada longitud de entrada. Por ejemplo, uno de los primeros modelos de ML no podía generar un párrafo significativo porque no podía retener el contexto entre la primera y la última oración de un párrafo.\n",
    "\n",
    "Con los modelos de transformadores, puede utilizar técnicas como el aprendizaje por transferencia y la generación aumentada de recuperación (RAG). Estas técnicas permiten la personalización de los modelos existentes para aplicaciones específicas de la organización del sector. Los modelos pueden entrenarse previamente en conjuntos de datos grandes y, a continuación, ajustarse con precisión en conjuntos de datos más pequeños y específicos para tareas específicas. Este enfoque ha democratizado el uso de modelos sofisticados y ha eliminado las limitaciones de recursos en el entrenamiento de modelos grandes desde cero. Los modelos pueden funcionar bien en varios dominios y tareas para diversos casos de uso.\n",
    "\n",
    "\n",
    "\n",
    "* Breve descripción de redes neuronales previas (MLP, RNN, LSTM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Redes neuronales Feedforward (MLP)\n",
    "\n",
    "* Definición de MLP  y su estructura: capas de entrada, ocultas y salida.\n",
    "\n",
    "* Matemáticas del Feedforward: Cálculo de la salida de una red neuronal.\n",
    "\n",
    "* Función de activación: ReLU,  Sigmoid, Tanh\n",
    "\n",
    "* Backpropagation en MLP: ALgoritmo de retropropagación para el ajuste de pesos.\n",
    "\n",
    "* Fórmula de retropropagación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Redes recurrentes (RNN, LSTM)\n",
    "\n",
    "* RNN: Estructura y uso en secuencias.\n",
    "\n",
    "* Backpropagation Through Time (BPTT): Extensión de retropropagación para redes recurrentes.\n",
    "\n",
    "* LSTM y GRU: Solución al problema anterior mediante puertas.\n",
    "\n",
    "* Feedforward en RNN: Cálculo del estado oculto.\n",
    "\n",
    "* Backpropagation en LSTM: Algoritmo para actualizar pesos a través de las puertas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transformers y Autoantención \n",
    "\n",
    "* Arquitectura Transformer: Componentes principales (atención, capas feedforward).\n",
    "En transformers el codificador mapea una secuencia de entrada de representaciones de símbolos $ (x_1,..., x_n) $ a una secuencia de representaciones continuas $ z = (z_1,..., z_n) $. Dado $z$, el decodificador genera una secuencia de símbolos de salida $ (y_1,...,y_m)$ de un elemento cada vez. En cada paso, el modelo es autoregresivo, consumiendo los símbolos generados previamente como entrada adicional al generar el siguiente paso. El Transformer sigue esta arquitectura general ultilizando capas apiladas de autoatención y completamente conectadas punto a punto, tanto para el codificacor y el decodificador.\n",
    "    * Encoder: El codificador está compuesto por una pila de N = 6 capas idénticas. Cada capa tiene subcapas.\n",
    "    La primera capa es una mecanismo de autoatención con múltiples cabezas, la segunda es una red simple totalmente conectada en función de la posición. Se emplea una conexión residual (las cuales son enlaces adicionales que conectan algunas capas de una red neuronal con otras capas que no son directamente adyacentes), alrededor de cada una de las dos subcapas, seguida de una normalización de capas. Es decir la salida de cada subcapa es \n",
    "    $$ LayerNorm(x + sublayer(x))$$ \n",
    "    donde  $ sublayer(x) $ es una función implementada por la propia subcapa. Para facilitar estas conexiones residuales, todas las subcapas del modelo así como las capas embedding producen salidas de dimensión $d_{model} = 512$.\n",
    "\n",
    "* Autoatención.\n",
    "La función de Atención puede ser descrita como un mapeo entre de la $Query(Q)$ y el conjunto de los pares $key-value(K,V)$ a la salida, donde query, keys, values, y las salidas son todos vectores.\n",
    "La salida se calcula como una suma ponderada de los values, donde el peso asignado a cada valor se calcula mediante una función de compatibilidad que es el producto punto entre la query y la key.\n",
    "$$Compatibilidad = QK^T$$\n",
    "Luego se normaliza usando softmax para convertirlo en una distribución de probabilidades\n",
    "$$\\alpha_{i,j} = softmax(\\frac{QK^T}{\\sqrt(d_k)}) $$ donde d_k es la dimensión de las keys y querys y se usan para estabilizar los gradientes.\n",
    "Como resultado final obtenemos la siguiente expresión:\n",
    "$$ Atención(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt(d_k)})V\n",
    "Es decir, cada value $v_i$ se multiplica por su peso $\\alpha_{i,j}$ y  luego se suman todos los valores ponderados.\n",
    "* Aplicación de Feedforward en Transformers\n",
    "Además de las subcapas de atención, cada una de las capas del codificador y decodificador contiene una red feedforward  totalmente conectada que se aplica a cada posición por separado y de forma idéntica completamente conectada, que se aplica a cada posición por separado y de forma idéntica. Este consiste en dos transformaciones lineales con una activación ReLU intermedia.\n",
    "$$ FFN(x) = max (0, xW_1 + b_1)W_2 + b_2 $$\n",
    "Aunque las transformaciones lineales son las mismas en las distintas posiciones, utilizan parámetros diferentes de una capa a otra. Otra forma de describir esto es como dos convoluciones con tamaño de núcleo 1. La dimensionalidad de la entrada y la salida es $d_{model} =512$,  y la capa interna tiene una dimensionalidad $d_{ff} = 248$\n",
    "\n",
    "\n",
    "* Backpropagation en Transformers: Cómo se aplica retropropagación en la capa de autoatención y en el feedforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparación entre Redes Neuronales y Transformers\n",
    "\n",
    "* Comparación entre MLP, RNN y Transformers\n",
    "\n",
    "* Ventajas de Transformers en tareas con NLP y visión\n",
    "\n",
    "* Evolución en el uso de Feedforward: De MLP a modelos complejos como los Transformers.\n",
    " * Impacto de Backpropagation en el entrenamiendo de modelos complejos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Mezcla de Arquitecturas (Transformers + Otras redes)\n",
    "\n",
    "* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
