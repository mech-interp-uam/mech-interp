{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducción\n",
    "\n",
    "* Definición de Transformers y su relevancia en el aprendizaje profundo.\n",
    "\n",
    "    Los transformadores son un tipo de arquitectura de red neuronal que transforma o cambia una secuencia de entrada en una secuencia de salida. Para ello, aprenden el contexto y rastrea las relaciones entre los componentes de la secuencia de entrada: \"¿De qué color es el cielo?\". El modelo transformador usa una representación matemática interna que identifica la relevancia y la relación entre las palabras color, cielo y azul. Usa esa información para generar el resultado: \"El cielo es azul\".\n",
    "    Actualmente, las organizaciones usan modelos de transformadores para todo tipo de conversiones de secuencias, desde el reconocimiento de voz hasta la traducción automática y el análisis de secuencia de proteínas.\n",
    "\n",
    "    Los modelos tempranos de aprendizaje profundo que se centraron ampliamente en las tareas de procesamiento de lenguaje natural (NLP) tenían como fin lograr que las computadoras comprendan al lenguaje humano natural. Adivinaron la palabrasiguiente en una secuencia basada en la palabra anterior.\n",
    "    Los primeros modelos de machine learning (ML) aplicaban una tecnología similar a una escala más amplia. Trazaban la frecuencia de relación entre diferentes pares de palabras o grupos de palabras en su conjunto de datos de entrenamiento e intentaban adivinar la siguiente palabra. Sin embargo, la tecnología primitiva no podía retener el contexto más allá de una determinada longitud de entrada. Por ejemplo, uno de los primeros modelos de ML no podía generar un párrafo significativo porque no podía retener el contexto entre la primera y la última oración de un párrafo.\n",
    "\n",
    "    Con los modelos de transformadores, puede utilizar técnicas como el aprendizaje por transferencia y la generación aumentada de recuperación (RAG). Estas técnicas permiten la personalización de los modelos existentes para aplicaciones específicas de la organización del sector. Los modelos pueden entrenarse previamente en conjuntos de datos grandes y, a continuación, ajustarse con precisión en conjuntos de datos más pequeños y específicos para tareas específicas. Este enfoque ha democratizado el uso de modelos sofisticados y ha eliminado las limitaciones de recursos en el entrenamiento de modelos grandes desde cero. Los modelos pueden funcionar bien en varios dominios y tareas para diversos casos de uso.\n",
    "\n",
    "\n",
    "\n",
    "* Breve descripción de redes neuronales previas (MLP, RNN, LSTM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Redes neuronales Feedforward (MLP)\n",
    "\n",
    "* Definición de MLP  y su estructura: capas de entrada, ocultas y salida.\n",
    "    Son la quinta-esencia de los modelos de aprendizaje profundo, el objetivo de de una red feedforward es aproximar alguna función $f^*$. Por ejemplo, para clasificar $y = f^*(x)$ mapea una entrada x a la categoría y. Una red feedforward define el mapeo $y = f(x,\\theta)$ y aprende el valor del parámetro $\\theta$ que resulta en la mejor aproximación de la función. \n",
    "\n",
    "    Estos modelos son llamados feedforward porque la información fluye a través de la función evaluada en x, a través de cálculos intermedios se suele definir $f$  y finalmente la salida y. Aquí no hay conexiones feedback en las que la red está extiende hacia atrás en sí misma. Cuando las redes feedforward se extienden para incluir conexiones feedback, son llamadas Redes Recurrentes.\n",
    "\n",
    "* Matemáticas del Feedforward: Cálculo de la salida de una red neuronal.\n",
    "    Dado un modelo con $L$ capas ocultas, cada capa realiza la siguiente operación:\n",
    "    $$ z^{(l)} = W^{(l)}a^{(l-1)} + b^{(l)}$$\n",
    "    $$ a^{(l)} = \\sigma(z^{(l)})$$\n",
    "    * l es el indice de la capa\n",
    "    * $x = a^{(0)}$ es la entrada inicial\n",
    "    * $z^{(l)}$ es la activación antes de aplicar la función de activación.\n",
    "    * $W{(l)}$ es la matriz de pesos de la capa l\n",
    "    * $b^{(l)}$ es el vector de sesgos (bias)\n",
    "    * $\\sigma$ es la función de activación\n",
    "    * $a^{(l)}$ es la salida de la capa de la activación\n",
    "\n",
    "    Para ampliar los modelos lineales para representar funciones no lineales de x, podemos aplicar el modelo lineal no a x en sí mismo, sino a una entrada transformada $\\phi(x)$, donde $\\phi$ es una transformación no lineal. La estrategia del aprendizaje profundo consiste en aprender $\\phi$. En este enfoque, tenemos el modelo $y = f(x; \\theta, w) = \\phi(x; \\theta)^Tw$. Ahora tenemos los parámetros $\\theta$ que utilizamos para aprender $\\phi$ a partir de una clase amplia de funciones, y parámetros $w$ que asignan de $\\phi(x)$ a la salida deseada. En este enfoque, se parametriza la representación como $\\phi(x;\\theta)$ y se utiliza el algoritmo de optimización para definir el $\\theta$ que corresponde a una buena representación.\n",
    "\n",
    "\n",
    "\n",
    "* Función de activación: ReLU,  Sigmoid, Tanh\n",
    "    La función ReLU \n",
    "    $$f_{relu} : \\Re \\rightarrow [0, \\infty]$$\n",
    "    $$z \\rightarrow max[0,z] $$\n",
    "    es llamada la función de parte positiva o función rampa, es la función identidad para los argumentos positivos y la función constante para el valor cero y los negativos. Un claro beneficio de l afunción ReLU es que tanto la función en sí, como sus derivadas sosn fáciles de aplicar con bajo costo computacional.\n",
    "\n",
    "    La función sigmoide\n",
    "    $$f_{log}: \\Re \\rightarrow (0,1)$$\n",
    "    $$z \\rightarrow \\frac{1}{1+e^{-z}}$$\n",
    "    es una función acotada y diferenciable que no es decreciente y tiene exactamente un punto de inflexión es decir, es una curva suave. Como las funciones sigmoides \"aplastan\" los valores reales en un intervalo, a veces se denominan funciones de aplastamiento. La activación sigmoidea tiene una larga tradición en la teoría y la practica de las redes neuronales. Una motivación ha sido la interpretación de funciones sigmoides como aproximaciones matemáticamente manejables de las funciones escalonadas.\n",
    "\n",
    "    La función\n",
    "    $$f_{tanh}: \\Re \\rightarrow (-1,1)$$\n",
    "    $$z \\rightarrow tanh[z]:= \\frac{e^z-e^{-z}}{e^z+e^{-z}}$$\n",
    "    es llamada la función tangente hipérbolica, es infinitamente diferenciable. Más aún, las series de Taylor de tanh y arctan coinciden hasta el cuarto orden, Así podemos pensar en tanh como una versión desplazada y escalada de la función logistica o como una aproximación de arctan. En concreto, tanh combina dos características populares de la función logistica y de arctan: la derivada de tanh es una función sencilla de la función original y está centrada en cero. \n",
    "\n",
    "* Backpropagation en MLP: \n",
    "    Algoritmo de retropropagación para el ajuste de pesos.\n",
    "    La retropropagación consiste en comprender cómo el cambio de los pesos y los sesgos de una red modifica la función de coste. En última instancia, esto significa calcular las derivadas parciales $\\frac{\\partial C}{\\partial {w_{jk}}^l}$ y $\\frac{\\partial C}{\\partial {b_j}^l}$, pero para calcularlas, primero se introduce una cantidad intermedia $\\delta_j^l$, que es el error en el nodo j-ésimo.\n",
    "    Hay cuatro ecuaciones principales que interactúan para calcular cómo cambian los pesos de una red neuronal y minimizar el error:\n",
    "    1. El error en la salida $\\delta^L = \\nabla_aC \\odot \\sigma'(z^L)$, donde $\\odot$ denota el producto de Hadamard (elemento a elemento).\n",
    "    2. El error en la capa l en términos del error en la capa l+1: $\\delta^l = ((W^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^l)$.\n",
    "    3. La tasa de cambio de la función de coste con respecto a los sesgos: $\\frac{\\partial C}{\\partial b_j^l} = \\delta_j^l$.\n",
    "    4. La tasa de cambio de la función de coste con respecto a los pesos: $\\frac{\\partial C}{\\partial w_{jk}^l} = a_k^{l-1} \\delta_j^l$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Redes recurrentes (RNN, LSTM)\n",
    "\n",
    "* RNN: Estructura y uso en secuencias.\n",
    "\n",
    "* Backpropagation Through Time (BPTT): Extensión de retropropagación para redes recurrentes.\n",
    "\n",
    "* LSTM y GRU: Solución al problema anterior mediante puertas.\n",
    "\n",
    "* Feedforward en RNN: Cálculo del estado oculto.\n",
    "\n",
    "* Backpropagation en LSTM: Algoritmo para actualizar pesos a través de las puertas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transformers y Autoantención \n",
    "\n",
    "* Arquitectura Transformer: Componentes principales (atención, capas feedforward).\n",
    "    En el paper *Attention is all you need*, tenemos una descripción del Transformer que es la que se detalla a continuación, sin embargo, los parámetros que se proponen pueden variar según las necesidades del modelo. Actualmente, la arquitectura Transformer ha sido adaptada y optimizada para diversas aplicaciones, demostrando su flexibilidad y eficacia en múltiples campos de la inteligencia artificial.\n",
    "    \n",
    "    El codificador mapea una secuencia de entrada de representaciones de símbolos $ (x_1,..., x_n) $ a una secuencia de representaciones continuas $z = (z_1,..., z_n) $. Dado $ z $, el decodificador genera una secuencia de símbolos de salida $ (y_1,...,y_m) $, de un elemento cada vez. En cada paso, el modelo es autoregresivo, consumiendo los símbolos generados previamente como entrada adicional al generar el siguiente paso. El Transformer sigue esta arquitectura general utilizando capas apiladas de autoatención y completamente conectadas punto a punto, tanto para el codificador como para el decodificador. En otras palabras, el encoder transforma la secuencia de entrada en una representación más rica y útil (los vectores $ z$), que contienen información semántica importante para que el decoder genere la salida correcta.\n",
    "\n",
    "    * **Encoder:**  \n",
    "        El codificador está compuesto por una pila de $ N = 6 $ capas idénticas. Cada capa tiene dos subcapas.\n",
    "\n",
    "        La primera capa es un mecanismo de autoatención con múltiples cabezas, la segunda es una red simple totalmente conectada en función de la posición. Se emplea una conexión residual (las cuales son enlaces adicionales que conectan algunas capas de una red neuronal con otras capas que no son directamente adyacentes), alrededor de cada una de las dos subcapas, seguida de una normalización de capas. Es decir, la salida de cada subcapa es  \n",
    "        $$\n",
    "        \\text{LayerNorm}\\left( x + \\text{sublayer}(x)\\right)\n",
    "        $$\n",
    "        donde $ \\text{sublayer}(x) $ es una función implementada por la propia subcapa. Para facilitar estas conexiones residuales, todas las subcapas del modelo, así como las capas embedding, producen salidas de dimensión  $d_{\\text{model}} = 512 $.\n",
    "\n",
    "    * **Decoder:**  \n",
    "        El decoder usa la representación generada por el encoder para generar una salida paso a paso. Lo hace siendo autoregresivo, es decir, en cada paso genera un token y lo usa como entrada en el siguiente paso. El decoder tiene dos mecanismos de atención: \n",
    "        - **Autoatención**, que relaciona palabras ya generadas entre sí.  \n",
    "        - **Atención cruzada** (*encoder-decoder attention*), que se enfoca en partes relevantes de $z $  \n",
    "\n",
    "        Luego, una capa feedforward ayuda a procesar la información antes de predecir el siguiente token.\n",
    "Para la capa de atención necesitamos transformadores de los datos de entrada:\n",
    "    * **Query (Q)**: es la consulta, es la representación de la palabra actual que está tratando de encontrar información relevante de la secuencia, se usa para comparar con las claves de todas las palabras de la secuencia.\n",
    "    * **Key (K)**: Es la representación de cada palabra en la secuencia, se compara con la consulta para determinar la relevancia de cada palabra con respecto a la palabra actual.\n",
    "    * **Value (V)**: Es la información contenida en cada palabra de la secuencia, se usa para generar la salida ponderada basada en la atención calculada.\n",
    "\n",
    "* **Autoatención**  \n",
    "  La autoatención captura las relaciones entre todos los elementos de la secuencia. Luego, la capa feedforward refina estas representaciones. Ambas acciones se repiten en varias capas apiladas para mejorar la abstracción de la información.  \n",
    "\n",
    "  La función de atención puede ser descrita como un mapeo entre la **Query (Q)** y el conjunto de pares **Key-Value (K, V)** hacia la salida, donde las queries, keys, values y las salidas son todos vectores.  \n",
    "\n",
    "  La salida se calcula como una suma ponderada de los values, donde el peso asignado a cada valor se obtiene mediante una función de compatibilidad basada en el producto punto entre la query y la key:  \n",
    "\n",
    "  $$\n",
    "  \\text{Compatibilidad} = QK^T\n",
    "  $$\n",
    "\n",
    "  Luego, se normaliza usando softmax para convertirlo en una distribución de probabilidad:\n",
    "\n",
    "  $$\n",
    "  \\alpha_{i,j} = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right)\n",
    "  $$\n",
    "\n",
    "  donde $d_k$ es la dimensión de las keys y queries, y se usa para estabilizar los gradientes.  \n",
    "\n",
    "  Como resultado final, obtenemos la siguiente expresión para la atención:  \n",
    "\n",
    "  $$\n",
    "  \\text{Atención}(Q,K,V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "  $$\n",
    "\n",
    "  Es decir, cada value $ v_i $ se multiplica por su peso $ \\alpha_{i,j} $ y luego se suman todos los valores ponderados.  \n",
    "\n",
    "* **Aplicación de Feedforward en Transformers**  \n",
    "  Además de las subcapas de atención, cada una de las capas del codificador y del decodificador contiene una red feedforward totalmente conectada, que se aplica a cada posición de forma idéntica e independiente.  \n",
    "\n",
    "  Esta red consiste en dos transformaciones lineales con una activación ReLU intermedia:\n",
    "\n",
    "  $$\n",
    "  \\text{FFN}(x) = \\max(0, x W_1 + b_1) W_2 + b_2\n",
    "  $$\n",
    "\n",
    "  Aunque las transformaciones lineales son las mismas en distintas posiciones, utilizan parámetros diferentes de una capa a otra. Otra forma de describir esto es como dos convoluciones con tamaño de núcleo 1.  \n",
    "\n",
    "  La dimensionalidad de la entrada y la salida es $ d_{\\text{model}} = 512 $, y la capa interna tiene una dimensionalidad de $ d_{\\text{ff}} = 2048 $.\n",
    "\n",
    "\n",
    "\n",
    "* Backpropagation en Transformers: Cómo se aplica retropropagación en la capa de autoatención y en el feedforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparación entre Redes Neuronales y Transformers\n",
    "\n",
    "* Comparación entre MLP, RNN y Transformers\n",
    "\n",
    "* Ventajas de Transformers en tareas con NLP y visión\n",
    "\n",
    "* Evolución en el uso de Feedforward: De MLP a modelos complejos como los Transformers.\n",
    " * Impacto de Backpropagation en el entrenamiendo de modelos complejos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Mezcla de Arquitecturas (Transformers + Otras redes)\n",
    "\n",
    "* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
