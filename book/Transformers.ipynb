{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducción\n",
    "\n",
    "* Definición de Transformers y su relevancia en el aprendizaje profundo.\n",
    "\n",
    "Los transformadores son un tipo de arquitectura de red neuronal que transforma o cambia una secuencia de entrada en una secuencia de salida. Para ello, aprenden el contexto y rastrea las relaciones entre los componentes de la secuencia de entrada: \"¿De qué color es el cielo?\". El modelo transformador usa una representación matemática interna que identifica la relevancia y la relación entre las palabras color, cielo y azul. Usa esa información para generar el resultado: \"El cielo es azul\".\n",
    "Actualmente, las organizaciones usan modelos de transformadores para todo tipo de conversiones de secuencias, desde el reconocimiento de voz hasta la traducción automática y el análisis de secuencia de proteínas.\n",
    "\n",
    "Los modelos tempranos de aprendizaje profundo que se centraron ampliamente en las tareas de procesamiento de lenguaje natural (NLP) tenían como fin lograr que las computadoras comprendan al lenguaje humano natural. Adivinaron la palabrasiguiente en una secuencia basada en la palabra anterior.\n",
    "Los primeros modelos de machine learning (ML) aplicaban una tecnología similar a una escala más amplia. Trazaban la frecuencia de relación entre diferentes pares de palabras o grupos de palabras en su conjunto de datos de entrenamiento e intentaban adivinar la siguiente palabra. Sin embargo, la tecnología primitiva no podía retener el contexto más allá de una determinada longitud de entrada. Por ejemplo, uno de los primeros modelos de ML no podía generar un párrafo significativo porque no podía retener el contexto entre la primera y la última oración de un párrafo.\n",
    "\n",
    "Con los modelos de transformadores, puede utilizar técnicas como el aprendizaje por transferencia y la generación aumentada de recuperación (RAG). Estas técnicas permiten la personalización de los modelos existentes para aplicaciones específicas de la organización del sector. Los modelos pueden entrenarse previamente en conjuntos de datos grandes y, a continuación, ajustarse con precisión en conjuntos de datos más pequeños y específicos para tareas específicas. Este enfoque ha democratizado el uso de modelos sofisticados y ha eliminado las limitaciones de recursos en el entrenamiento de modelos grandes desde cero. Los modelos pueden funcionar bien en varios dominios y tareas para diversos casos de uso.\n",
    "\n",
    "\n",
    "\n",
    "* Breve descripción de redes neuronales previas (MLP, RNN, LSTM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Redes neuronales Feedforward (MLP)\n",
    "\n",
    "* Definición de MLP  y su estructura: capas de entrada, ocultas y salida.\n",
    "\n",
    "* Matemáticas del Feedforward: Cálculo de la salida de una red neuronal.\n",
    "\n",
    "* Función de activación: ReLU,  Sigmoid, Tanh\n",
    "\n",
    "* Backpropagation en MLP: ALgoritmo de retropropagación para el ajuste de pesos.\n",
    "\n",
    "* Fórmula de retropropagación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Redes recurrentes (RNN, LSTM)\n",
    "\n",
    "* RNN: Estructura y uso en secuencias.\n",
    "\n",
    "* Backpropagation Through Time (BPTT): Extensión de retropropagación para redes recurrentes.\n",
    "\n",
    "* LSTM y GRU: Solución al problema anterior mediante puertas.\n",
    "\n",
    "* Feedforward en RNN: Cálculo del estado oculto.\n",
    "\n",
    "* Backpropagation en LSTM: Algoritmo para actualizar pesos a través de las puertas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers y Autoantención \n",
    "\n",
    "* Arquitectura Transformer: Componentes principales (atención, capas feedforward).\n",
    "\n",
    "Un modelo transformador es una red neuronal que aprende en contexto de los datos secuenciales y genera nuevos datos a partir de él, se consideran la evolución \n",
    "* Autoatención\n",
    "\n",
    "* Aplicación de Feedforward en Transformers\n",
    "\n",
    "* Backpropagation en Transformers: Cómo se aplica retropropagación en la capa de autoatención y en el feedforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparación entre Redes Neuronales y Transformers\n",
    "\n",
    "* Comparación entre MLP, RNN y Transformers\n",
    "\n",
    "* Ventajas de Transformers en tareas con NLP y visión\n",
    "\n",
    "* Evolución en el uso de Feedforward: De MLP a modelos complejos como los Transformers.\n",
    " * Impacto de Backpropagation en el entrenamiendo de modelos complejos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mezcla de Arquitecturas (Transformers + Otras redes)\n",
    "\n",
    "* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
