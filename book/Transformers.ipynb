{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducción\n",
    "\n",
    "* Definición de Transformers y su relevancia en el aprendizaje profundo.\n",
    "\n",
    "    Los transformers son un tipo de arquitectura de red neuronal que transforma \n",
    "    o cambia una secuencia de entrada en una secuencia de salida. Para ello, \n",
    "    aprenden el contexto y rastrea las relaciones entre los componentes de la \n",
    "    secuencia de entrada: \"¿De qué color es el cielo?\". El modelo transformer \n",
    "    usa una representación matemática interna que identifica la relevancia y la \n",
    "    relación entre las palabras color, cielo y azul. Usa esa información para \n",
    "    generar el resultado: \"El cielo es azul\".\n",
    "    Actualmente, las organizaciones usan modelos de transformers para todo tipo \n",
    "    de conversiones de secuencias, desde el reconocimiento de voz hasta la \n",
    "    traducción automática y el análisis de secuencia de proteínas.\n",
    "\n",
    "    Los modelos tempranos de aprendizaje profundo que se centraron ampliamente \n",
    "    en las tareas de procesamiento de lenguaje natural (NLP) tenían como fin \n",
    "    lograr que las computadoras comprendan al lenguaje humano natural. Adivinaron\n",
    "    la palabrasiguiente en una secuencia basada en la palabra anterior.\n",
    "    Los primeros modelos de machine learning (ML) aplicaban una tecnología \n",
    "    similar a una escala más amplia. Trazaban la frecuencia de relación entre \n",
    "    diferentes pares de palabras o grupos de palabras en su conjunto de datos de \n",
    "    entrenamiento e intentaban adivinar la siguiente palabra. Sin embargo, la tecnología \n",
    "    primitiva no podía retener el contexto más allá de una determinada longitud \n",
    "    de entrada. Por ejemplo, uno de los primeros modelos de ML no podía generar \n",
    "    un párrafo significativo porque no podía retener el contexto entre la primera \n",
    "    y la última oración de un párrafo.\n",
    "\n",
    "    Con los modelos de transformers, puede utilizar técnicas como el aprendizaje \n",
    "    por transferencia y la generación aumentada de recuperación (RAG). Estas \n",
    "    técnicas permiten la personalización de los modelos existentes para \n",
    "    aplicaciones específicas de la organización del sector. Los modelos pueden \n",
    "    entrenarse previamente en conjuntos de datos grandes y, a continuación, \n",
    "    ajustarse con precisión en conjuntos de datos más pequeños y específicos para\n",
    "    tareas específicas. Este enfoque ha democratizado el uso de modelos \n",
    "    sofisticados y ha eliminado las limitaciones de recursos en el entrenamiento \n",
    "    de modelos grandes desde cero. Los modelos pueden funcionar bien en varios \n",
    "    dominios y tareas para diversos casos de uso.\n",
    "\n",
    "\n",
    "\n",
    "* Breve descripción de redes neuronales previas (MLP, RNN, LSTM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Redes neuronales Feedforward (MLP)\n",
    "\n",
    "## Definición de MLP y su estructura: capas de entrada, ocultas y salida\n",
    "\n",
    "Son la quinta-esencia de los modelos de aprendizaje profundo, el objetivo de una\n",
    "red feedforward es aproximar alguna función $f^*$. Por ejemplo, para clasificar \n",
    "$y = f^*(x)$ mapea una entrada $x$ a la categoría $y$. Una red feedforward \n",
    "define el mapeo $y = f(x|\\theta)$ y aprende el valor del parámetro $\\theta$ que \n",
    "resulta en la mejor aproximación de la función. \n",
    "\n",
    "Estos modelos son llamados feedforward porque la información fluye a través de \n",
    "la función evaluada en $x$, a través de cálculos intermedios se suele definir \n",
    "$f$ y finalmente la salida $y$. Aquí no hay conexiones feedback en las que la \n",
    "red se extiende hacia atrás en sí misma. Cuando las redes feedforward se \n",
    "extienden para incluir conexiones feedback, son llamadas Redes Recurrentes.\n",
    "\n",
    "## Matemáticas del Feedforward: Cálculo de la salida de una red neuronal\n",
    "\n",
    "Dado un modelo con $L$ capas ocultas, cada capa realiza la siguiente operación:\n",
    "\n",
    "$$\n",
    "z^{(l)} = W^{(l)}a^{(l-1)} + b^{(l)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "a^{(l)} = \\sigma(z^{(l)})\n",
    "$$\n",
    "\n",
    "* $l$ es el índice de la capa  \n",
    "* $x = a^{(0)}$ es la entrada inicial  \n",
    "* $z^{(l)}$ es la activación antes de aplicar la función de activación  \n",
    "* $W^{(l)}$ es la matriz de pesos de la capa $l$  \n",
    "* $b^{(l)}$ es el vector de sesgos (bias)  \n",
    "* $\\sigma$ es la función de activación  \n",
    "* $a^{(l)}$ es la salida de la capa de la activación  \n",
    "\n",
    "Para ampliar los modelos lineales para representar funciones no lineales de $x$,\n",
    "podemos aplicar el modelo lineal no a $x$ en sí mismo, sino a una entrada \n",
    "transformada $\\phi(x)$, donde $\\phi$ es una transformación no lineal. La \n",
    "estrategia del aprendizaje profundo consiste en aprender $\\phi$. En este enfoque, \n",
    "tenemos el modelo $y = f(x; \\theta, w) = \\phi(x; \\theta)^Tw$. Ahora tenemos los \n",
    "parámetros $\\theta$ que utilizamos para aprender $\\phi$ a partir de una clase \n",
    "amplia de funciones, y parámetros $w$ que asignan de $\\phi(x)$ a la salida \n",
    "deseada. En este enfoque, se parametriza la representación como $\\phi(x;\\theta)$\n",
    "y se utiliza el algoritmo de optimización para definir el $\\theta$ que \n",
    "corresponde a una buena representación.\n",
    "\n",
    "## Función de activación: ReLU, Sigmoid, Tanh\n",
    "\n",
    "**La función ReLU**  \n",
    "$$\n",
    "f_{\\text{relu}} : \\mathbb{R} \\rightarrow [0, \\infty)\n",
    "$$\n",
    "\n",
    "$$\n",
    "z \\rightarrow \\max[0,z]\n",
    "$$\n",
    "\n",
    "Es llamada la función de parte positiva o función rampa, es la función identidad\n",
    " para los argumentos positivos y la función constante para el valor cero y los \n",
    " negativos. Un claro beneficio de la función ReLU es que tanto la función en sí,\n",
    " como sus derivadas son fáciles de aplicar con bajo costo computacional.\n",
    "\n",
    "**La función sigmoide**  \n",
    "$$\n",
    "f_{\\text{log}}: \\mathbb{R} \\rightarrow (0,1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "z \\rightarrow \\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "\n",
    "Es una función acotada y diferenciable que no es decreciente y tiene exactamente \n",
    "un punto de inflexión, es decir, es una curva suave. Como las funciones \n",
    "sigmoides \"aplastan\" los valores reales en un intervalo, a veces se denominan \n",
    "funciones de aplastamiento. La activación sigmoidea tiene una larga tradición en\n",
    "la teoría y la práctica de las redes neuronales. Una motivación ha sido la \n",
    "interpretación de funciones sigmoides como aproximaciones matemáticamente \n",
    "manejables de las funciones escalonadas.\n",
    "\n",
    "**La función**  \n",
    "$$\n",
    "f_{\\text{tanh}} : \\mathbb{R} \\rightarrow (-1,1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "z \\rightarrow \\tanh[z] := \\frac{e^z - e^{-z}}{e^z + e^{-z}}\n",
    "$$\n",
    "\n",
    "Es llamada la función tangente hiperbólica, es infinitamente diferenciable. Más \n",
    "aún, las series de Taylor de $\\tanh$ y $\\arctan$ coinciden hasta el cuarto orden. \n",
    "Así podemos pensar en $\\tanh$ como una versión desplazada y escalada de la \n",
    "función logística o como una aproximación de $\\arctan$. En concreto, $\\tanh$ \n",
    "combina dos características populares de la función logística y de $\\arctan$: \n",
    "la derivada de $\\tanh$ es una función sencilla de la función original y está \n",
    "centrada en cero. \n",
    "\n",
    "## Backpropagation en MLP: Algoritmo de retropropagación para el ajuste de pesos\n",
    "\n",
    "La retropropagación consiste en comprender cómo el cambio de los pesos y los \n",
    "sesgos de una red modifica la función de costo. En última instancia, esto \n",
    "significa calcular las derivadas parciales $\\frac{\\partial C}{\\partial w_{jk}^l}$ \n",
    "y $\\frac{\\partial C}{\\partial b_j^l}$, pero para calcularlas, primero se \n",
    "introduce una cantidad intermedia $\\delta_j^l$, que es el error en el nodo \n",
    "$j$-ésimo.  \n",
    "\n",
    "Hay cuatro ecuaciones principales que interactúan para calcular cómo cambian los\n",
    "pesos de una red neuronal y minimizar el error:  \n",
    "\n",
    "1. **El error en la salida**  \n",
    "$$\n",
    "\\delta^L = \\nabla_a C \\odot \\sigma'(z^L)\n",
    "$$  \n",
    "Donde $\\odot$ denota el producto de Hadamard (elemento a elemento).  \n",
    "\n",
    "2. **El error en la capa $l$ en términos del error en la capa $l+1$**  \n",
    "$$\n",
    "\\delta^l = \\left((W^{l+1})^T \\delta^{l+1}\\right) \\odot \\sigma'(z^l)\n",
    "$$  \n",
    "\n",
    "3. **La tasa de cambio de la función de costo con respecto a los sesgos**  \n",
    "$$\n",
    "\\frac{\\partial C}{\\partial b_j^l} = \\delta_j^l\n",
    "$$  \n",
    "\n",
    "4. **La tasa de cambio de la función de costo con respecto a los pesos**  \n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w_{jk}^l} = a_k^{l-1} \\delta_j^l\n",
    "$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Redes recurrentes (RNN, LSTM)\n",
    "\n",
    "* RNN: Estructura y uso en secuencias.\n",
    "Las RNN están diseñadas para procesar secuencias temporales o secuenciales (como\n",
    " texto, series de tiempo, etc.). A diferencia del perceptrón multicapa, las RNN\n",
    "  mantienen un *estado oculto* que actúa como memoria de lo que se ha procesado\n",
    "  anteriormente. En cada paso temporal $t$, la RNN recibe una entrada $x^{(t)} y \n",
    "  se actualiza su estado oculto $h^{(t)} usando:\n",
    "  $$\n",
    "  h^{(t)} =  \\sigma (W_h h^{(t-1)} + W_x x^{(t)} + b),\n",
    "$$\n",
    "donde $\\sigma es la función de activación con $(Tanh)$, $W_h$ y $W_x$ son\n",
    "matrices de pesos y $b$ es el sesgo. La salida $y^{(t)}$, se calcula como:\n",
    "$$\n",
    "y^{(t)} = \\text{softmax}(W_y h^{(t)} + b_y)\n",
    "$$\n",
    "\n",
    "* Backpropagation Through Time (BPTT): Extensión de retropropagación para redes \n",
    "recurrentes.\n",
    "\n",
    "El BPTT es una adaptación de la retropropagación para redes recurrentes. Dado que las\n",
    "RNN procesan secuencias paso a paso, los gradientes debes propagarse hacia atrás en el \n",
    "tiempo. Para una secuencia de longitud $\\tau$, la función de pérdida $L$ depende\n",
    "de todas las salidas:\n",
    "$$\n",
    "\\text{L} = \\sum_{t=1}^\\tau L^{(t)}\n",
    "$$\n",
    "Los gradientes se calculan recursivamente desde $\\text{t}=\\tau$ hasta $\\text{t} = 1$. \n",
    "Por ejemplo, el gradiente respecto a $W_h$ es:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_{h}} = \\sum_{t=1}^{T}\n",
    "$$\n",
    "Sin embargo, las RNN estándar sufren del problema de desvanecimiento/explosión de\n",
    "gradientes, lo que limita su capacidad de aprender sus dependencias a largo plazo.\n",
    "\n",
    "* LSTM y GRU: Solución al problema anterior mediante puertas.\n",
    "LSTM (Long Short-Term Memory)\\\\\n",
    "Las LSTM introducen *celdas de memoria* y *puertas* para controlar el flujo de\n",
    "información. \n",
    "1. Puerta de olvido($f_t$): Decide qué información descartar. \n",
    "2. Puerta de entrada ($i_t$): Actualiza la celda con nueva información. \n",
    "3. Puerta de salida ($o_t$): Determina qué parte de la celda se usa para la salida. \n",
    "\n",
    "Las ecuaciones para una celda LSTM:\n",
    "$$\n",
    "\\begin{align*}\n",
    "f_t &= \\sigma\\left(W_f \\begin{bmatrix} \\mathbf{h}^{(t-1)} \\\\ \\mathbf{x}^{(t)} \\end{bmatrix} + \\mathbf{b}_f\\right), \\\\\n",
    "\\dot{i}_t &= \\sigma\\left(W_i \\begin{bmatrix} \\mathbf{h}^{(t-1)} \\\\ \\mathbf{x}^{(t)} \\end{bmatrix} + \\mathbf{b}_i\\right), \\\\\n",
    "\\tilde{C}_t &= \\tanh\\left(W_C \\begin{bmatrix} \\mathbf{h}^{(t-1)} \\\\ \\mathbf{x}^{(t)} \\end{bmatrix} + \\mathbf{b}_C\\right), \\\\\n",
    "C_t &= f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t, \\\\\n",
    "o_t &= \\sigma\\left(W_o \\begin{bmatrix} \\mathbf{h}^{(t-1)} \\\\ \\mathbf{x}^{(t)} \\end{bmatrix} + \\mathbf{b}_o\\right), \\\\\n",
    "\\mathbf{h}^{(t)} &= o_t \\odot \\tanh(C_t),\n",
    "\\end{align*}\n",
    "$$\n",
    "donde $\\odot$ denota la multiplicación elemento a elemento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transformers y Autoantención \n",
    "\n",
    "\n",
    "* **Atención**  \n",
    "  La atención captura las relaciones entre todos los elementos de la secuencia. \n",
    "  Luego, la capa feedforward refina estas representaciones. Ambas acciones se \n",
    "  repiten en varias capas apiladas para mejorar la abstracción de la información.\n",
    "\n",
    "  La función de atención puede ser descrita como una operación entre tres \n",
    "  secuencias de vectores: **Query (Q)**, **Keys (K)**, y **Values (V)**\n",
    "\n",
    "  La salida se calcula como una suma ponderad de los values, donde el peso \n",
    "  asignado a cada valor se obtiene mediante una función de compatibilidad \n",
    "  basada en el producto punto entre la query y la key:\n",
    "\n",
    "  Un primera definición podría ser\n",
    "  $$\n",
    "  r_i = \\sum_j^T (q_i \\cdot k_j) v_j\n",
    "  $$\n",
    "  donde podríamos pensar que el resultado va a incorporar información en el \n",
    "  elemento $i$ usando los otros elementos de la secuencia, un solo vector $q_i$ \n",
    "  dicta cual información se va a incorporar de los vectores valor al resultado \n",
    "  $r_i$.\n",
    "\n",
    "  Notemos que si todos esos vectores punto estuvieran limitados a ser 0 o ser 1, \n",
    "  podemos definir un mapeo con la expreción de arriba, que tome $q_i$ y nos \n",
    "  devuelva $v_i$, justo como en una base de datos; uno provee una consulta \n",
    "  (\"query\" en inglés), la cual tiene un resultado correspondiente $v_i$. Pero, \n",
    "  notemos la formulación de arriba es más general, uno podría pensar en el como \n",
    "  \"una base de datos diferenciable\".\n",
    "\n",
    "  Sin embargo, hay dos problemas\n",
    "  - Si consideramos todos los componentes de los vectores querry y key como \n",
    "  independientes, con varianza 1 y con esperanza 0, la varianza de cada producto \n",
    "  punto entre un vector querry y un vector key sería $d_k$, la dimensión de \n",
    "  tales vectores, lo cual va al contrario de la prevalente practica de mantener \n",
    "  varianza constante mientras estos se van aplicando capas y capas del modelo.\n",
    "  - Si vemos esa definición de attención como un mapeo lineal que toma un vector \n",
    "  con todos los componentes de los todos los vectores querry, key, y value, y \n",
    "  saca un solo vector con todos los componentes de todos los vectores resultado, \n",
    "  nos daremos cuenta que estamos ante una función lineal.\n",
    "\n",
    "  El primer problema lo resolvemos dividiendo cada uno de esos productos punto \n",
    "  entre $\\sqrt{d_k}$. El segundo al aplicar $\\text{softmax}$ a esos coefficientes \n",
    "  de la suma ponderada\n",
    "\n",
    "  $$\n",
    "  \\alpha_{i,j} = \\text{softmax}(q_i \\cdot k_1, ..., q_i \\cdot k_T) \\\\\n",
    "  r_i = \\sum_j^T \\alpha_{i,j} v_j\n",
    "  $$\n",
    "\n",
    "  Esto nos lleva a la formulación en términos de matrices\n",
    "\n",
    "  $$\n",
    "  \\text{Atención}(Q,K,V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "  $$\n",
    "\n",
    "  Es decir, cada vector resultado es la suma ponderada (de hecho, la combinación \n",
    "  convexa), de los vectores value.\n",
    "\n",
    "\n",
    "* **Autoatencción**\n",
    "La autoatención hace que conforme los elementos de la secuencia $h_i$ vayan \n",
    "avanzando a lo largo del transformer, estos puedan intercambiar información unos con otros.\n",
    "\n",
    "Esto se logra mediante tres matrices de parámetos, digamos $W_Q$, $W_K$, $W_V$, \n",
    "que se aplican a la secuencia $h_1, ..., h_T$ para obtener secuencias \n",
    "$q_1, ..., q_T$, $k_1, ..., k_T$, y $v_1, ... v_T$, y sobre estas haciendo la \n",
    "operación de atención.\n",
    "\n",
    "Uno usaría autoantención si pretende codificar una imágen (o más generalmente, \n",
    "cualquier colección de vectores, los cuales pueden tener o no una noción de \n",
    "posición), a una secuencia de vectores $z_i$ con información reelevantes para, \n",
    "por ejemplo, para un clasificador de imágenes. De hecho, esto es lo que se usa \n",
    "para obtener una representación vectorial de imágenes que luego puede ser usada \n",
    "para darle la capasidad de ver imágenes (condicionar la salida con respecto a \n",
    "una imágen extra en la entrada) a grandes modelos de lenguage o modelos \n",
    "generadores de imágenes.\n",
    "\n",
    "* **Autoatencción Enmascarada**\n",
    "Si deseamos que nuestro modelo aprenda a generar un vector de salida $o_i$ \n",
    "condicionado a la secuencia $e_1, ..., e_i$ de vectores de entrada, es eliminando \n",
    "toda forma de que los vectores $e_i, ... e_T$ puedan influenciar la salida $o_i$. \n",
    "Esto, en la capa de attención significa que la combinación convexa y el \n",
    "$\\text{softmax}$ sea solo usando $v_j$ y $k_j$ con $j \\le i$\n",
    "\n",
    "* **Atención cruzada**\n",
    "Supongamos queremos usar información de una secuencia en otra, primero podríamos \n",
    "codificarla en una secuencia $z_i$ y luego realizar una operación de attención \n",
    "donde los vectores querry vengan de la secuencia que condiciona, y los vectores \n",
    "value y key vengan de la secuencia que se está condicionando. Podemos pensar en \n",
    "esto como cada vector $z_i$ combinando los elementos de la secuencia original\n",
    "$v_i$, usando informacción que proviene de la secuencia original, $q_i$. Notemos\n",
    "que aquí también podemos limitar nuestra combinación convexa, para los mismos \n",
    "fines mencionados anteriormente.\n",
    "\n",
    "En resumen, para los tres casos de atención usamos tres matrices para obtener los\n",
    "vectores querry, key, y value, pero el origen de los vectores a los cuales les \n",
    "aplicamos las matrices depende en el caso de la atención cruzada y la combinación \n",
    "convexa tiene menos términos en el caso de la atención enmascarada.\n",
    "\n",
    "\n",
    "Si queremos un transformer que pueda hacer traducción de inglés a español, \n",
    "podríamos usar una red neuronal con attención para codificar la oración en inglés\n",
    "a una secuencia de vectores $z_1, ..., z_S$, y otra con attención enmascarada, \n",
    "entrenada para que dada una entrada $e_1, ..., e_T$, producir una secuencia \n",
    "$o_1, ... o_T$, donde cada $o_i$ es una distribución condicionada en \n",
    "$e_1, ..., e_i$ y en la codificación $z_1, ..., z_S$.\n",
    "\n",
    "Ahora si podemos definir qué es un transformer\n",
    "\n",
    "* Arquitectura Transformer: Componentes principales (atención, capas feedforward).\n",
    "    En el paper *Attention is all you need*, tenemos una descripción del \n",
    "    Transformer que es la que se detalla a continuación, sin embargo, los \n",
    "    parámetros que se proponen suelen variar según las necesidades del modelo. \n",
    "    Actualmente, la arquitectura Transformer ha sido adaptada y optimizada para \n",
    "    diversas aplicaciones, demostrando su flexibilidad y eficacia en múltiples \n",
    "    campos de la inteligencia artificial.\n",
    "    \n",
    "    El codificador mapea una secuencia de entrada de representaciones de símbolos \n",
    "    $ (x_1,..., x_n) $ a una secuencia de representaciones continuas \n",
    "    $z = (z_1,..., z_n) $. Dado $ z $, el decodificador genera una secuencia de \n",
    "    símbolos de salida $ (y_1,...,y_m) $, de un elemento cada vez. En cada paso, \n",
    "    el modelo es autoregresivo, consumiendo los símbolos generados previamente \n",
    "    como entrada adicional al generar el siguiente paso. El Transformer sigue \n",
    "    esta arquitectura general utilizando capas apiladas de autoatención y \n",
    "    completamente conectadas punto a punto, tanto para el codificador como para \n",
    "    el decodificador. En otras palabras, el encoder transforma la secuencia de \n",
    "    entrada en una representación más rica y útil (los vectores $ z$), que \n",
    "    contienen información semántica importante para que el decoder genere la \n",
    "    salida correcta.\n",
    "\n",
    "    * **Encoder:**  \n",
    "        El codificador está compuesto por una pila de $ N = 6 $ capas idénticas. \n",
    "        Cada capa tiene dos subcapas.\n",
    "\n",
    "        La primera capa es un mecanismo de autoatención con múltiples cabezas, \n",
    "        la segunda es una red simple totalmente conectada en función de la \n",
    "        posición. Se emplea una conexión residual (las cuales son enlaces \n",
    "        adicionales que conectan algunas capas de una red neuronal con otras \n",
    "        capas que no son directamente adyacentes), alrededor de cada una de las \n",
    "        dos subcapas, seguida de una normalización de capas. Es decir, la salida \n",
    "        de cada subcapa es  \n",
    "        $$\n",
    "        \\text{LayerNorm}\\left( x + \\text{sublayer}(x)\\right)\n",
    "        $$\n",
    "        donde $ \\text{sublayer}(x) $ es una función implementada por la propia \n",
    "        subcapa. Para facilitar estas conexiones residuales, todas las subcapas \n",
    "        del modelo, así como las capas embedding, producen salidas de dimensión  \n",
    "        $d_{\\text{model}} = 512 $.\n",
    "\n",
    "    * **Decoder:**  \n",
    "        El decoder usa la representación generada por el encoder para generar \n",
    "        una salida paso a paso. Lo hace siendo autoregresivo, es decir, en cada \n",
    "        paso genera un token y lo usa como entrada en el siguiente paso. El \n",
    "        decoder tiene dos mecanismos de atención: \n",
    "        - **Autoatención**, que relaciona palabras ya generadas entre sí.  \n",
    "        - **Atención cruzada** (*encoder-decoder attention*), que se enfoca en \n",
    "        partes relevantes de $z $  \n",
    "\n",
    "        Luego, una capa feedforward ayuda a procesar la información antes de \n",
    "        predecir el siguiente token.\n",
    "Para la capa de atención necesitamos transformaciones de los datos de entrada:\n",
    "    * **Query (Q)**: es la consulta, es la representación de la palabra actual \n",
    "    que está tratando de encontrar información relevante de la secuencia, se usa \n",
    "    para comparar con las claves de todas las palabras de la secuencia.\n",
    "    * **Key (K)**: Es la representación de cada palabra en la secuencia, se \n",
    "    compara con la consulta para determinar la relevancia de cada palabra con \n",
    "    respecto a la palabra actual.\n",
    "    * **Value (V)**: Es la información contenida en cada palabra de la secuencia, \n",
    "    se usa para generar la salida ponderada basada en la atención calculada.\n",
    "\n",
    "* **Aplicación de Feedforward en Transformers**  \n",
    "  Además de las subcapas de atención, cada una de las capas del codificador y del \n",
    "  decodificador contiene una red feedforward totalmente conectada, que se aplica \n",
    "  a cada posición de forma idéntica e independiente.  \n",
    "\n",
    "  Esta red consiste en dos transformaciones lineales con una activación ReLU \n",
    "  intermedia:\n",
    "\n",
    "  $$\n",
    "  \\text{FFN}(x) = \\max(0, x W_1 + b_1) W_2 + b_2\n",
    "  $$\n",
    "\n",
    "  Aunque las transformaciones lineales son las mismas en distintas posiciones, \n",
    "  utilizan parámetros diferentes de una capa a otra. Otra forma de describir esto \n",
    "  es como dos convoluciones con tamaño de núcleo 1.  \n",
    "\n",
    "  La dimensionalidad de la entrada y la salida es $ d_{\\text{model}} = 512 $, y \n",
    "  la capa interna tiene una dimensionalidad de $ d_{\\text{ff}} = 2048 $.\n",
    "\n",
    "\n",
    "\n",
    "* Backpropagation en Transformers: Cómo se aplica retropropagación en la capa de \n",
    "autoatención y en el feedforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparación entre Redes Neuronales previas y Transformers\n",
    "\n",
    "* Manejo de secuencias y dependiencias.\n",
    "    - Multilayer Perceptrón. \n",
    "    Tiene la limitación de no procesar secuencias. Tratan cada entrada como \n",
    "    independiente. (ej. clasificación de imagenes)\n",
    "    **Ecuación típica** : $y = \\sigma(W x + b), sin contexto temporal.\n",
    "    \n",
    "    - Redes recurrentes (RNN/LSTM):\n",
    "    Vemos la ventaja de que capturan dependencias locales en secuencias mediante\n",
    "    estados ocultos ($h^{(t)}$), sin embargo a largo plazo sufren de desvanecimiento\n",
    "    de gradientes.\n",
    "\n",
    "    - Transformers:\n",
    "    Su mayor aporte es que capturan dependencias globales mediante la **autoatención**\n",
    "    independientemente de la distancia entre tokens.\n",
    "    **Mecanismo clave**: $$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^\\top}{\\sqrt{d_k}} \\right)V$$\n",
    "\n",
    "\n",
    "\n",
    "* Paralelización y eficiencia computacional.\n",
    "    - RNN/LSTM:\n",
    "\n",
    "    Procesamiento secuencial: Cada paso temporal tt depende de t−1t−1, \n",
    "    imposibilitando paralelización total.\n",
    "\n",
    "    Coste temporal: $O(\\tau)$ para una secuencia de longitud $\\tau$.\n",
    "\n",
    "    - Transformers:\n",
    "\n",
    "    Procesamiento paralelo: Todas las posiciones de la secuencia se procesan \n",
    "    simultáneamente gracias a la autoatención.\n",
    "\n",
    "    Coste computacional: $O(n^2)$ para secuencias de longitud $n$ (cuello de \n",
    "    botella en textos largos).\n",
    "\n",
    "    Ejemplo: Entrenar un Transformer en GPUs es significativamente más rápido \n",
    "    que una RNN para tareas como traducción automática.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Mezcla de Arquitecturas (Transformers + Otras redes)\n",
    "\n",
    "Los Transformers pueden potenciarse al combinarse con otras arquitecturas como \n",
    "**CNNs**, **RNNs (LSTM/GRU)** o **Autoencoders**. Estas combinaciones permiten \n",
    "aprovechar fortalezas complementarias: relaciones locales, dinámicas temporales \n",
    "o aprendizaje no supervisado.\n",
    "\n",
    "---\n",
    "\n",
    "##  1. Transformers + CNN (Redes Convolucionales)\n",
    "\n",
    "Las CNNs son efectivas para capturar patrones locales, especialmente en imágenes. \n",
    "Al usarlas como paso previo a los Transformers, se puede mejorar la eficiencia y \n",
    "la robustez a nivel espacial.\n",
    "\n",
    "###  Ejemplo: *Convolutional Vision Transformers (CvT)*\n",
    "\n",
    "#### Representación matemática:\n",
    "\n",
    "1. **Extracción local con convolución:**\n",
    "\n",
    "\\[\n",
    "F_{\\text{CNN}}(X) = \\text{Conv}_{k \\times k}(X) = Z \\in \\mathbb{R}^{H' \\times W' \\times d}\n",
    "\\]\n",
    "\n",
    "2. **Aplanado a tokens:**\n",
    "\n",
    "\\[\n",
    "Z_{\\text{seq}} \\in \\mathbb{R}^{n \\times d}, \\quad n = H' \\cdot W'\n",
    "\\]\n",
    "\n",
    "3. **Aplicación de atención:**\n",
    "\n",
    "\\[\n",
    "\\text{Att}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^\\top}{\\sqrt{d_k}} \\right) V\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Transformers + RNN (LSTM/GRU)\n",
    "\n",
    "Las redes recurrentes capturan dependencias temporales fuertes, lo cual puede \n",
    "complementar la capacidad global del Transformer, especialmente en tareas \n",
    "secuenciales.\n",
    "\n",
    "### Ejemplo: *LSTM seguido de Transformer*\n",
    "\n",
    "#### Representación matemática:\n",
    "\n",
    "1. **Secuencia de entrada:**\n",
    "\n",
    "$$\n",
    "X = (x_1, x_2, \\dots, x_T)\n",
    "$$\n",
    "\n",
    "2. **Codificación con LSTM:**\n",
    "\n",
    "$$\n",
    "h_t = \\text{LSTM}(x_t, h_{t-1}), \\quad H = [h_1, \\dots, h_T]\n",
    "$$\n",
    "\n",
    "3. **Transformación con atención:**\n",
    "\n",
    "$$\n",
    "Q = HW_Q, \\quad K = HW_K, \\quad V = HW_V\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Att}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^\\top}{\\sqrt{d_k}} \\right) V\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Transformers + Autoencoders\n",
    "\n",
    "Los autoencoders permiten el preentrenamiento sin supervisión, útil para aprender \n",
    "representaciones robustas. En modelos como **MAE** (Masked Autoencoders), se \n",
    "combinan con Transformers para tareas de reconstrucción.\n",
    "\n",
    "###  Ejemplo: *Masked Autoencoder (MAE)*\n",
    "\n",
    "#### Representación matemática:\n",
    "\n",
    "1. **Entrada con máscaras:**\n",
    "\n",
    "$$\n",
    "X \\in \\mathbb{R}^{n \\times d}\n",
    "$$\n",
    "\n",
    "2. **Encoder:**\n",
    "\n",
    "$$\n",
    "Z = \\text{Encoder}(X_{\\text{visibles}})\n",
    "$$\n",
    "\n",
    "3. **Decoder:**\n",
    "\n",
    "$$\n",
    "\\hat{X} = \\text{Decoder}(Z + X_{\\text{máscara}})\n",
    "$$\n",
    "\n",
    "4. **Función de pérdida (reconstrucción):**\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\| X - \\hat{X} \\|_2^2\n",
    "$$\n",
    "\n",
    "Algo que hay que notar:\n",
    "Podemos ver todas estas combinaciones como funciones anidadas:\n",
    "\n",
    "$$\n",
    "f(x) = f_{\\text{att}}(f_{\\text{pre}}(x)) \\quad \\text{o} \\quad f(x) = f_{\\text{post}}(f_{\\text{att}}(x))\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $ f_{\\text{att}} $: Bloque Transformer (autoatención + feedforward),\n",
    "- $ f_{\\text{pre}} $: CNN, RNN o encoder,\n",
    "- $ f_{\\text{post}} $: Decoder o módulo adicional.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
