{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Danos una introducción a los fenómenos que la interpretabilidad mecanicista trata de explicar, en concreto cosas como\n",
    "\n",
    "    Las neuronas monosemánticas como\n",
    "        * La neurona de sentimientos que mencionaron Hinton\n",
    "        * La neurona de Donal Trump que Chris Olah mencionó en el podcast de Lex es común en muchas inteligencias artificiales de la época\n",
    "\n",
    "    Los resultados de word2vec\n",
    "\n",
    "   Las definiciones de:\n",
    "\n",
    "    Características\n",
    "\n",
    "    Monosemanticidad\n",
    "\n",
    "    Polisemanticidad\n",
    "\n",
    "    La teoría de la representación lineal\n",
    "\n",
    "    Superposición\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La interpretabilidad trata de explicar la relación causa-efecto de un algoritmo \n",
    "y saber como variará la salida ante cambios en la entrada o en los parámetros.\n",
    "Con los modelos tradicionales de deepl learning como las RNNs, LSTMs o CNNs es\n",
    "difícil predecir la salida del modelo ante cambios en la entrada y también \n",
    "explicar con detalle el funcionamiento interno, más allá de la función \n",
    "proporcionada por cada una de las capas, y las semetrías del modelo. \n",
    "\n",
    "En los mecanismos diferenciables de atención, la atención permite al modelo \n",
    "seq2seq (utilizado particularmente en modelos de traducción automática y \n",
    "procesamiento de lenguaje natural) centrarse en unos elementos de la secuencia \n",
    "de entrada más que en otros.\n",
    "Los coeficientes de atención relacionan cada elemento de la entrada codificada \n",
    "por en encoder con la salida en el instante t como se ve a continuación:\n",
    "$$\n",
    "c(t) = \\sum_{i=1}^T a_{ti} h(i)\n",
    "$$\n",
    "- donde:  \n",
    "    - $c(t)$ es el contexto en el instante t, que se calcula como una combinación ponderada de los estados ocultos del encoder.  \n",
    "    - $h(i)$ representa el estado oculto del encoder en la posición i. Estos estados contienen la información de la secuencia de entrada.  \n",
    "    - $\\alpha_{ti}$ es el coeficiente de atención o peso de atención que indica cuánta importancia tiene el estado $h(i)$ en el tiempo $t$. Estos valores se calculan con una función de atención y suman 1 en conjunto (son probabilidades).  \n",
    "    - $T$ es la longitud de la secuencia de entrada.  \n",
    "\n",
    "Estos coeficientes permiten dotar de interpretabilidas al modelo y poder averiguar la relación entre los elementos d ela entrada y los de la salida. Sin \n",
    "embargo la atención identifica parcialmente la importancia de cada elemento de la entrada del modelo."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
