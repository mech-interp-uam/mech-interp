{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Formulaci√≥n Matem√°tica de Autoencoders**\n",
    "\n",
    "El estudio de los autoencoders, en general, es importante para poder interpretar lo que aprenden los modelos Transformer. Esta es una de las herramientas m√°s \n",
    "utilizadas actualmente para la interpretaci√≥n de dichos modelos. En esta secci√≥n se analizar√° la estructura matem√°tica de los autoencoders simples y, posteriormente,\n",
    " de los autoencoders sparsos, los cuales incorporan peque√±as penalizaciones en la funci√≥n de p√©rdida. Estas penalizaciones les otorgan propiedades particulares que \n",
    " pueden ser √∫tiles para nuestros fines.\n",
    "\n",
    "\n",
    "## **Autoencoder Simple**\n",
    "\n",
    "\n",
    "### **Definici√≥n del Autoencoder**\n",
    "Un **autoencoder** es una funci√≥n compuesta $ h: \\mathcal{X} \\to \\mathcal{X} $ definida por la composici√≥n de dos funciones diferenciables:\n",
    "\n",
    "$$\n",
    "h(\\mathbf{x}) = g_{\\phi}\\bigl(f_{\\theta}(\\mathbf{x})\\bigr)\n",
    "$$\n",
    "\n",
    "donde:\n",
    "\n",
    "- $f_{\\theta}: \\mathcal{X} \\to \\mathbb{R}^m$ es la **funci√≥n de codificaci√≥n (encoder)**.\n",
    "- $g_{\\phi}: \\mathbb{R}^m \\to \\mathcal{X}$ es la **funci√≥n de decodificaci√≥n (decoder)**.\n",
    "\n",
    "El objetivo del autoencoder es encontrar los par√°metros $\\theta$ y $\\phi$ tales que $h(\\mathbf{x}) \\approx \\mathbf{x}$, minimizando una funci√≥n de p√©rdida adecuada.\n",
    "\n",
    "\n",
    "### **Codificador (Encoder)**\n",
    "El encoder transforma la entrada $\\mathbf{x} \\in \\mathbb{R}^n$ en una representaci√≥n latente $\\mathbf{z} \\in \\mathbb{R}^m$, con $m < n$ en el caso de reducci√≥n de dimensionalidad:\n",
    "\n",
    "$$\n",
    "\\mathbf{z} = f_{\\theta}(\\mathbf{x}) = \\sigma\\bigl(W_e \\,\\mathbf{x} + \\mathbf{b}_e\\bigr)\n",
    "$$\n",
    "\n",
    "donde:\n",
    "\n",
    "- $W_e \\in \\mathbb{R}^{m \\times n}$ es la **matriz de pesos del encoder**.\n",
    "- $\\mathbf{b}_e \\in \\mathbb{R}^{m}$ es el **vector de sesgo**.\n",
    "- $\\sigma: \\mathbb{R} \\to \\mathbb{R}$ es una funci√≥n de activaci√≥n (**ReLU**, **Sigmoid**, **Tanh**).\n",
    "- $\\mathbf{z} \\in \\mathbb{R}^{m}$ es la representaci√≥n latente.\n",
    "\n",
    "\n",
    "### **Decodificador (Decoder)**\n",
    "El decoder reconstruye la entrada original a partir de $\\mathbf{z}$:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{x}} = g_{\\phi}(\\mathbf{z}) = \\sigma'\\bigl(W_d \\,\\mathbf{z} + \\mathbf{b}_d\\bigr)\n",
    "$$\n",
    "\n",
    "donde:\n",
    "\n",
    "- $W_d \\in \\mathbb{R}^{n \\times m}$ es la **matriz de pesos del decoder**.\n",
    "- $\\mathbf{b}_d \\in \\mathbb{R}^{n}$ es el **vector de sesgo**.\n",
    "- $\\sigma': \\mathbb{R} \\to \\mathbb{R}$ es una funci√≥n de activaci√≥n (puede diferir de $\\sigma$).\n",
    "- $\\hat{\\mathbf{x}} \\in \\mathbb{R}^n$ es la **reconstrucci√≥n de la entrada**.\n",
    "\n",
    "\n",
    "### **Funci√≥n de P√©rdida**\n",
    "\n",
    "Es importante considerar que la funci√≥n de p√©rdida puede variar. En principio, el error cuadr√°tico medio (Mean Squared Error, MSE) es una de las m√°s utilizadas en autoencoders simples; sin embargo, dependiendo de la tarea a realizar, en algunos casos conviene m√°s utilizar una u otra.\n",
    "\n",
    "Para un conjunto de datos \n",
    "\n",
    "$$\n",
    "\\mathcal{D} = \\{\\mathbf{x}_i\\}_{i=1}^{N},\n",
    "$$\n",
    "\n",
    "el entrenamiento del autoencoder minimiza la diferencia entre la entrada $\\mathbf{x}_i$ y la reconstrucci√≥n $\\hat{\\mathbf{x}}_i$. Usamos el **Error Cuadr√°tico Medio (MSE)** promediado:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{MSE} = \\frac{1}{N} \\sum_{i=1}^{N}\n",
    "\\left(\n",
    "    \\frac{1}{n} \\sum_{j=1}^{n} \n",
    "    \\bigl(x_{i,j} - \\hat{x}_{i,j}\\bigr)^2\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "donde:\n",
    "\n",
    "- $N$ es el n√∫mero total de muestras.\n",
    "- $d$ es la dimensi√≥n de cada muestra $\\mathbf{x}_i$.\n",
    "- $x_{i,j}$ y $\\hat{x}_{i,j}$ representan la $j$-√©sima componente de la muestra $\\mathbf{x}_i$ y de su reconstrucci√≥n, respectivamente.\n",
    "\n",
    "<br>\n",
    "<details>\n",
    "<summary>Nota: Sobre algunas otras funciones de p√©rdida. </summary>\n",
    "\n",
    "El **Error cuadr√°tico medio** es ideal para datos continuos, como im√°genes con valores reales. Es f√°cil de usar y da resultados estables, pero puede generar salidas borrosas porque penaliza fuertemente los errores grandes.\n",
    "\n",
    "**Binary Crossentropy** se usa cuando los datos est√°n entre 0 y 1, como im√°genes normalizadas. Funciona bien con activaciones como sigmoide, y modela la probabilidad de cada p√≠xel o bit.\n",
    "\n",
    "La Binary Crossentropy se expresa como:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{BCE} = \n",
    "- \\frac{1}{N \\times d} \\sum_{i=1}^{N} \\sum_{j=1}^{d}\n",
    "\\Bigl[\n",
    "    x_{i,j} \\, \\log\\bigl(\\hat{x}_{i,j}\\bigr) \n",
    "    +\n",
    "    \\bigl(1 - x_{i,j}\\bigr) \\, \\log\\bigl(1 - \\hat{x}_{i,j}\\bigr)\n",
    "\\Bigr],\n",
    "$$\n",
    "\n",
    "donde $ d $ es la dimensi√≥n de cada muestra, $ x_{i,j} \\in \\{0,1\\} $ y $ \\hat{x}_{i,j} $ es la probabilidad estimada por el modelo para dicha componente. Esta funci√≥n de p√©rdida castiga fuertemente aquellas predicciones en las que $ \\hat{x}_{i,j} $ difiere de $ x_{i,j} $ con alto grado de confianza (debido al uso del logaritmo).\n",
    "\n",
    "\n",
    "**Categorical Crossentropy** es m√°s adecuada cuando la salida son categor√≠as, como texto o etiquetas. \n",
    "Para cuando cada muestra $ \\mathbf{x}_i $ pertenece a una de $ K $ categor√≠as y se representa en formato *one-hot* (solo una de sus $ K $ posiciones es 1, mientras que el resto son 0), tenemos la **Categorical Crossentropy**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{CCE} =\n",
    "- \\frac{1}{N}\n",
    "\\sum_{i=1}^{N}\n",
    "\\sum_{k=1}^{K}\n",
    "x_{i,k} \\,\\log\\bigl(\\hat{x}_{i,k}\\bigr),\n",
    "$$\n",
    "\n",
    "donde $ x_{i,k} $ es 1 si la clase $ k $ es la correcta para la muestra $ \\mathbf{x}_i $, y $ \\hat{x}_{i,k} $ es la probabilidad que el modelo asigna a la clase $ k $. El objetivo en este caso es alinear la distribuci√≥n pronosticada con la verdadera, penalizando fuertemente cuando la probabilidad de la clase correcta resulta ser baja.\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "### **Optimizaci√≥n**\n",
    "El objetivo es encontrar los par√°metros $\\theta$ y $\\phi$ que minimicen la funci√≥n de p√©rdida:\n",
    "\n",
    "$$\n",
    "\\theta^*, \\phi^* = \\arg \\min_{\\theta, \\phi} \\,\\mathcal{L}_{MSE}.\n",
    "$$\n",
    "\n",
    "La optimizaci√≥n se resuelve mediante **descenso de gradiente**, por ejemplo usando una tasa de aprendizaje $\\eta$:\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta\\,\\nabla_{\\theta} \\,\\mathcal{L}_{MSE},\n",
    "\\quad\n",
    "\\phi \\leftarrow \\phi - \\eta\\,\\nabla_{\\phi} \\,\\mathcal{L}_{MSE}.\n",
    "$$\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>Nota: Sobre la optimizacion</summary>\n",
    "\n",
    "Hay diferentes formas de optimizacion. \n",
    "</details>\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sparse Autoencoder**\n",
    "\n",
    "Los sparse autoencoders son √∫tiles para interpretar modelos Transformer porque permiten identificar representaciones latentes significativas y m√°s f√°cilmente interpretables. Al forzar que solo una peque√±a parte del espacio latente se active para cada ejemplo, el modelo tiende a representar conceptos m√°s espec√≠ficos y dispersos. Esto facilita analizar qu√© tipo de informaci√≥n se activa internamente en respuesta a una entrada, ayudando a entender mejor c√≥mo el Transformer organiza y procesa los datos.\n",
    "\n",
    "Los sparse autoencoders respetan la estructura del \"autoencoder simple\" y simplemente se a√±ade un t√©rmino de penalizaci√≥n que fomenta activaciones promedio bajas en la capa latente, lo que provoca la dispersi√≥n. \n",
    "Adem√°s, es importante considerar que el espacio latente puede ser distinto al de los autoencoders simples. El espacio latente puede llegar a ser mayor que el tama√±o de la entrada, pero esto depender√° del tipo de datos con los que se est√© trabajando. Si los datos tienen una estructura com√∫n o baja complejidad, entonces requerir√°n un espacio latente m√°s peque√±o que la entrada. Por otro lado, si los datos son m√°s complejos o variados, requerir√°n un espacio latente m√°s grande.\n",
    "\n",
    " **Nota: anotar el caso de la dimensionalidad para el ejemplo a abordar.** \n",
    "\n",
    "<details>\n",
    "<summary> Nota: Informaci√≥n extra sobre las funci√≥n de ponalizaci√≥n </summary>\n",
    "\n",
    "Las funciones de penalizaci√≥n que inducen dispersi√≥n tienen en com√∫n que su objetivo es restringir el soporte efectivo del vector de representaci√≥n latente, es decir, promover que la mayor√≠a de sus componentes sean iguales o cercanos a cero. Para lograr esto, se introduce un t√©rmino adicional en la funci√≥n de p√©rdida que aumenta su valor cuando el n√∫mero o magnitud de componentes diferentes de cero en la representaci√≥n latente crece.\n",
    "\n",
    "La condici√≥n de diferenciabilidad que suelen tener las funciones de penalizaci√≥n utilizadas en aprendizaje autom√°tico no es una necesidad te√≥rica, sino una conveniencia computacional. En teor√≠a, se pueden utilizar funciones no diferenciables como la norma L‚ÇÄ exacta para inducir dispersi√≥n, ya que definen perfectamente bien el problema de optimizaci√≥n. Sin embargo, en la pr√°ctica, los m√©todos est√°ndar como el descenso de gradiente requieren calcular derivadas, por lo que se favorecen penalizaciones suaves y diferenciables que permitan aplicar este tipo de algoritmos de forma eficiente.Por lo tanto, es totalmente v√°lido prescindir de esta restricci√≥n si se est√° dispuesto a trabajar con enfoques diferentes, aunque estos podr√≠an no ser √≥ptimos desde el punto de vista computacional.\n",
    "\n",
    "</details>\n",
    "\n",
    "### Funciones de Penalizaci√≥n\n",
    "\n",
    "\n",
    "\n",
    "La funcion de penalizacione m√°s comunes es la **divergencia KL (Kullback-Leibler)**, que compara la activaci√≥n promedio de cada neurona con un valor objetivo peque√±o. Si llamamos \\(\\hat{\\rho}_j\\) a la activaci√≥n promedio de la neurona \\(j\\), calculada como\n",
    "\n",
    "**Activaci√≥n promedio de la neurona $j$:**  \n",
    "\n",
    "   Sea $\\mathbf{z}_i = f_{\\theta}(\\mathbf{x}_i)$ la salida del encoder para la muestra $i$. La **activaci√≥n promedio** de la neurona $j$ es:\n",
    "\n",
    "   $$\n",
    "   \\hat{\\rho}_j = \\frac{1}{N}\\sum_{i=1}^N z_{i,j}.\n",
    "   $$\n",
    "\n",
    "**Penalizaci√≥n (Divergencia KL):**  \n",
    "   Se define $\\rho$ como la activaci√≥n deseada (por ejemplo, $\\rho=0.05$). La desviaci√≥n de $\\hat{\\rho}_j$ respecto a $\\rho$ se mide con la **Divergencia KL**:\n",
    "\n",
    "   $$\n",
    "   \\mathrm{KL}\\bigl(\\rho \\,\\|\\, \\hat{\\rho}_j\\bigr)\n",
    "   =\n",
    "   \\rho \\,\\log \\frac{\\rho}{\\hat{\\rho}_j}\n",
    "   \\;+\\;\n",
    "   (1-\\rho)\\,\\log \\frac{1-\\rho}{\\,1-\\hat{\\rho}_j}.\n",
    "   $$\n",
    "\n",
    "**Funci√≥n de costo total (con dispersi√≥n):**  \n",
    "   El t√©rmino de esparsidad se agrega al MSE multiplicado por un factor $\\beta$:\n",
    "\n",
    "   $$\n",
    "   \\mathcal{L}_{Sparse}\n",
    "   =\n",
    "   \\mathcal{L}_{MSE}\n",
    "   \\;+\\;\n",
    "   \\beta \\sum_{j=1}^{m} \n",
    "   \\mathrm{KL}\\bigl(\\rho \\,\\|\\, \\hat{\\rho}_j\\bigr).\n",
    "   $$\n",
    "\n",
    "Este t√©rmino adicional obliga a que la **activaci√≥n promedio** de cada neurona $\\hat{\\rho}_j$ se acerque a $\\rho$, convirtiendo as√≠ un autoencoder normal en un **autoencoder *sparse***.\n",
    "\n",
    "\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary> Nota: Sobre la divergencia KL </summary>\n",
    "\n",
    "\n",
    "### Definici√≥n General de la Divergencia KL\n",
    "\n",
    "La divergencia KL entre dos distribuciones de probabilidad $P(x)$ y $Q(x)$ se define como:\n",
    "\n",
    "$$\n",
    "D_{KL}(P \\,\\|\\, Q) \\;=\\; \\sum_{x} P(x)\\,\\log\\!\\Bigl(\\tfrac{P(x)}{Q(x)}\\Bigr).\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $P(x)$ es la distribuci√≥n de referencia.\n",
    "- $Q(x)$ es la distribuci√≥n que usamos para aproximar a $P(x)$.\n",
    "- La divergencia KL mide cu√°nta informaci√≥n se pierde cuando usamos $Q(x)$ en lugar de $P(x)$.\n",
    "\n",
    "El principal objetivo de la divergencia de Kullback-Leibler (KL) es medir cu√°nta informaci√≥n se pierde cuando usamos una distribuci√≥n de probabilidad ùëÑ para aproximar otra distribuci√≥n P. En otras palabras, mide la diferencia entre dos distribuciones de probabilidad y nos dice cu√°nto nos alejamos de la distribuci√≥n \"verdadera\" al usar una aproximaci√≥n.\n",
    "\n",
    "\n",
    "-- Imagen del articulo pendiente:  Solomon Kullback y Richard A. Leibler en su art√≠culo de 1951: \"On Information and Sufficiency\".\n",
    "\n",
    "\n",
    "### Divergencia KL con Bernoulli\n",
    "\n",
    "Cuando $P$ y $Q$ son distribuciones de Bernoulli con par√°metros $\\rho$ y $\\hat{\\rho}_j$, respectivamente, la variable aleatoria $X$ solo puede tomar los valores $0$ o $1$. Entonces:\n",
    "\n",
    "- Para $X = 1$:\n",
    "  \n",
    "  $$\n",
    "  P(1) = \\rho, \\quad Q(1) = \\hat{\\rho}_j.\n",
    "  $$\n",
    "  \n",
    "- Para $X = 0$:\n",
    "  \n",
    "  $$\n",
    "  P(0) = 1 - \\rho, \\quad Q(0) = 1 - \\hat{\\rho}_j.\n",
    "  $$\n",
    "\n",
    "Aplicamos la definici√≥n de la divergencia KL:\n",
    "\n",
    "$$\n",
    "D_{KL}(P \\,\\|\\, Q) \n",
    "= \\sum_{x \\in \\{0,1\\}} P(x) \\log \\frac{P(x)}{Q(x)}.\n",
    "$$\n",
    "\n",
    "entonces,\n",
    "\n",
    "$$\n",
    "D_{KL}(\\text{Bern}(\\rho) \\,\\|\\, \\text{Bern}(\\hat{\\rho}_j))\n",
    "=\n",
    "\\rho \\,\\log\\!\\Bigl(\\tfrac{\\rho}{\\hat{\\rho}_j}\\Bigr)\n",
    "\\;+\\;\n",
    "(1-\\rho)\\,\\log\\!\\Bigl(\\tfrac{1-\\rho}{1-\\hat{\\rho}_j}\\Bigr).\n",
    "$$\n",
    "\n",
    "Esto nos da la divergencia KL espec√≠fica para dos distribuciones Bernoulli.\n",
    "\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "Y otra funcion de penalizacion en la que nos centraremos por su uso practico en la palicacion de este proyecto es la  regulacion **$L_0$**\n",
    "\n",
    "\n",
    "La **penalizaci√≥n \\(L_0\\)** (a veces denominada ‚Äúnorma \\(L_0\\)‚Äù) se define como el n√∫mero de elementos distintos de cero en un vector.  \n",
    "Si \\(z_i \\in \\mathbb{R}^H\\) es el vector de activaciones de la capa oculta para la muestra \\(i\\), entonces la ‚Äúnorma‚Äù \\(L_0\\) de \\(z_i\\) se expresa como:\n",
    "\n",
    "$$\n",
    "\\| z_i \\|_0 \\;=\\; \\bigl|\\{\\,j : z_{i,j} \\neq 0\\,\\}\\bigr|.\n",
    "$$\n",
    "\n",
    "En otras palabras, \\(\\| z_i \\|_0\\) es simplemente la cantidad de neuronas que est√°n encendidas (activas) en la muestra \\(i\\).  \n",
    "Para todo el conjunto de datos, con \\(M\\) muestras, la penalizaci√≥n \\(L_0\\) se puede escribir de forma compacta como:\n",
    "\n",
    "$$\n",
    "L_0 \\;=\\; \\sum_{i=1}^{M} \\| z_i \\|_0,\n",
    "$$\n",
    "\n",
    "o, de forma m√°s expl√≠cita:\n",
    "\n",
    "$$\n",
    "L_0 \\;=\\; \\sum_{i=1}^{M} \\sum_{j=1}^{H} \\mathbf{1}(z_{i,j} \\neq 0),\n",
    "$$\n",
    "\n",
    "donde \\(\\mathbf{1}(\\cdot)\\) es la funci√≥n indicadora, que vale 1 si la condici√≥n se cumple y 0 en caso contrario, \\(z_{i,j}\\) es la activaci√≥n de la neurona \\(j\\) en la muestra \\(i\\), \\(M\\) es el n√∫mero de muestras de entrenamiento y \\(H\\) es el n√∫mero de neuronas en la capa oculta.\n",
    "\n",
    "En la pr√°ctica, \\(L_0\\) no es derivable con respecto a los par√°metros del modelo, por lo que se suele **aproximar** o **sustituir** por otras penalizaciones (como \\(L_1\\)), que permiten m√©todos de optimizaci√≥n basados en gradientes.\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary> Nota: Otras funciones de penalizacion comunes </summary>\n",
    "\n",
    "\n",
    "Otra opci√≥n muy usada es la **regularizaci√≥n L1 sobre las activaciones**, que consiste en sumar el valor absoluto de todas las activaciones de la capa oculta:\n",
    "\n",
    "$$\n",
    "\\text{L1} \n",
    "= \\sum_{i=1}^{M} \\sum_{j=1}^{H} \\bigl|\\,z_{i,j}\\bigr|.\n",
    "$$\n",
    "\n",
    "Esta penalizaci√≥n empuja directamente las activaciones a ser lo m√°s cercanas posible a cero, lo cual naturalmente genera *sparsity*.\n",
    "\n",
    "\n",
    "Tambi√©n existe la **regularizaci√≥n L2 sobre activaciones**, que en lugar de tomar el valor absoluto, eleva las activaciones al cuadrado:\n",
    "\n",
    "$$\n",
    "\\text{L2} \n",
    "= \\sum_{i=1}^{M} \\sum_{j=1}^{H} \\bigl(z_{i,j}\\bigr)^2.\n",
    "$$\n",
    "\n",
    "Esta penalizaci√≥n no genera sparsity tan fuerte como L1, pero ayuda a mantener las activaciones controladas.\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  **JumpReLU Sparse Autoencoder**\n",
    "\n",
    "El **JumpReLU Sparse Autoencoder** es una arquitectura dise√±ada para obtener representaciones dispersas e interpretables de las activaciones internas de modelos de lenguaje grandes, como los transformers. Su prop√≥sito es **decomponer vectores de activaci√≥n** en combinaciones lineales de un conjunto de direcciones base (features), de manera que solo unas pocas de estas se activen en cada caso, permitiendo an√°lisis m√°s interpretables y eficientes.\n",
    "JumpReLU SAE emplea una activaci√≥n modificada llamada **JumpReLU**, que introduce un umbral personalizado para cada feature. Esta peque√±a variaci√≥n permite mejorar tanto la **dispersi√≥n** como la **fidelidad de reconstrucci√≥n** sin sacrificar eficiencia computacional.\n",
    "\n",
    "\n",
    "## Estructura del modelo\n",
    "\n",
    "El autoencoder est√° compuesto por dos partes principales:\n",
    "\n",
    "### Encoder\n",
    "\n",
    "Toma como entrada una activaci√≥n $x \\in \\mathbb{R}^n$ (proveniente de un transformer) y la proyecta a una representaci√≥n dispersa $f(x) \\in \\mathbb{R}^M$, con $M \\gg n$:\n",
    "\n",
    "$$\n",
    "f(x) = \\text{JumpReLU}_\\theta(W_{\\text{enc}} x + b_{\\text{enc}})\n",
    "$$\n",
    "\n",
    "- $W_{\\text{enc}} \\in \\mathbb{R}^{M \\times n}$: matriz de pesos del encoder  \n",
    "- $b_{\\text{enc}} \\in \\mathbb{R}^M$: vector de sesgos (bias)  \n",
    "- $\\theta \\in \\mathbb{R}^M_+$: vector de umbrales positivos por feature  \n",
    "- $\\text{JumpReLU}_\\theta$: funci√≥n de activaci√≥n descrita m√°s abajo\n",
    "\n",
    "### Decoder\n",
    "\n",
    "Reconstruye la activaci√≥n original $x$ a partir de su representaci√≥n dispersa:\n",
    "\n",
    "$$\n",
    "\\hat{x} = W_{\\text{dec}} f(x) + b_{\\text{dec}}\n",
    "$$\n",
    "\n",
    "- $W_{\\text{dec}} \\in \\mathbb{R}^{n \\times M}$: matriz de pesos del decoder  \n",
    "- $b_{\\text{dec}} \\in \\mathbb{R}^n$: vector de sesgos\n",
    "\n",
    "## Funci√≥n de activaci√≥n: JumpReLU\n",
    "\n",
    "La funci√≥n **JumpReLU** act√∫a como un ReLU con un umbral desplazado hacia la derecha. Se define como:\n",
    "\n",
    "$$\n",
    "\\text{JumpReLU}_\\theta(z_i) = \n",
    "\\begin{cases}\n",
    "z_i & \\text{si } z_i > \\theta_i \\\\\n",
    "0 & \\text{en otro caso}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "O de forma compacta:\n",
    "\n",
    "$$\n",
    "\\text{JumpReLU}_\\theta(z) = z \\cdot H(z - \\theta)\n",
    "$$\n",
    "\n",
    "donde $H(\\cdot)$ es la funci√≥n escal√≥n de Heaviside:\n",
    "\n",
    "$$\n",
    "H(a) =\n",
    "\\begin{cases}\n",
    "1 & \\text{si } a > 0 \\\\\n",
    "0 & \\text{si } a \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Esta funci√≥n permite activar solo aquellas features cuya preactivaci√≥n supere cierto umbral, evitando as√≠ \"falsas activaciones\" y mejorando la selectividad del encoder.\n",
    "\n",
    "\n",
    "## Funci√≥n de p√©rdida (Loss function)\n",
    "\n",
    "El entrenamiento del JumpReLU SAE se realiza minimizando una funci√≥n de p√©rdida que combina dos objetivos:\n",
    "\n",
    " **Reconstrucci√≥n fiel** de la activaci√≥n original  \n",
    " **Representaci√≥n dispersa** de las features activas\n",
    "\n",
    "La funci√≥n de p√©rdida es:\n",
    "\n",
    "$$\n",
    "L(x) = \\underbrace{\\|x - \\hat{x}(f(x))\\|_2^2}_{\\text{Error de reconstrucci√≥n}} + \\lambda \\cdot \\underbrace{\\|f(x)\\|_0}_{\\text{Penalizaci√≥n de dispersi√≥n (L0)}}\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $x$ es la activaci√≥n original  \n",
    "- $\\hat{x}(f(x))$ es la reconstrucci√≥n  \n",
    "- $\\lambda$ es un hiperpar√°metro que controla el equilibrio entre fidelidad y dispersi√≥n  \n",
    "- $\\|f(x)\\|_0$ es la **norma L‚ÇÄ**, que cuenta cu√°ntas features est√°n activas ($\\neq 0$)\n",
    "\n",
    "\n",
    "## Entrenamiento con funciones no diferenciables\n",
    "\n",
    "La penalizaci√≥n $L_0$ y la funci√≥n escal√≥n $H(z - \\theta)$ **no son diferenciables**, lo que complica el uso de backpropagation est√°ndar. Para solucionarlo, se usa una t√©cnica llamada **Straight-Through Estimator (STE)**, que permite calcular **gradientes aproximados** y as√≠ entrenar el modelo usando m√©todos convencionales de optimizaci√≥n.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
