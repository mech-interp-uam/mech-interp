{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Formulación Matemática de Autoencoders**\n",
    "\n",
    "El estudio de los autoencoders, en general, es importante para poder interpretar lo que aprenden los modelos Transformer. Esta es una de las herramientas más \n",
    "utilizadas actualmente para la interpretación de dichos modelos. En esta sección se analizará la estructura matemática de los autoencoders simples y, posteriormente,\n",
    " de los autoencoders sparsos, los cuales incorporan pequeñas penalizaciones en la función de pérdida. Estas penalizaciones les otorgan propiedades particulares que \n",
    " pueden ser útiles para nuestros fines.\n",
    "\n",
    "\n",
    "## **Autoencoder Simple**\n",
    "\n",
    "\n",
    "### **Definición del Autoencoder**\n",
    "Un **autoencoder** es una función compuesta $ h: \\mathcal{X} \\to \\mathcal{X} $ definida por la composición de dos funciones diferenciables:\n",
    "\n",
    "$$\n",
    "h(\\mathbf{x}) = g_{\\phi}\\bigl(f_{\\theta}(\\mathbf{x})\\bigr)\n",
    "$$\n",
    "\n",
    "\n",
    "donde, $f_{\\theta}: \\mathcal{X} \\to \\mathbb{R}^m$ es la **función de codificación (encoder)** y $g_{\\phi}: \\mathbb{R}^m \\to \\mathcal{X}$ es la **función de decodificación (decoder)**.\n",
    "\n",
    "El objetivo del autoencoder es encontrar los parámetros $\\theta$ y $\\phi$ tales que $h(\\mathbf{x}) \\approx \\mathbf{x}$, minimizando una función de pérdida adecuada.\n",
    "\n",
    "\n",
    "### **Codificador (Encoder)**\n",
    "El encoder transforma la entrada $\\mathbf{x} \\in \\mathbb{R}^n$ en una representación latente $\\mathbf{z} \\in \\mathbb{R}^m$, con $m < n$ en el caso de reducción de dimensionalidad:\n",
    "\n",
    "$$\n",
    "\\mathbf{z} = f_{\\theta}(\\mathbf{x}) = \\sigma\\bigl(W_e \\,\\mathbf{x} + \\mathbf{b}_e\\bigr)\n",
    "$$\n",
    "\n",
    "donde, $W_e \\in \\mathbb{R}^{m \\times n}$ es la **matriz de pesos del encoder**, $\\mathbf{b}_e \\in \\mathbb{R}^{m}$ es el **vector de sesgo**, $\\sigma: \\mathbb{R} \\to \\mathbb{R}$ es una función de activación (**ReLU**, **Sigmoid**, **Tanh**) y $\\mathbf{z} \\in \\mathbb{R}^{m}$ es la representación latente.\n",
    "\n",
    "\n",
    "### **Decodificador (Decoder)**\n",
    "El decoder reconstruye la entrada original a partir de $\\mathbf{z}$:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{x}} = g_{\\phi}(\\mathbf{z}) = \\sigma'\\bigl(W_d \\,\\mathbf{z} + \\mathbf{b}_d\\bigr)\n",
    "$$\n",
    "\n",
    "donde, $W_d \\in \\mathbb{R}^{n \\times m}$ es la **matriz de pesos del decoder**, $\\mathbf{b}_d \\in \\mathbb{R}^{n}$ es el **vector de sesgo**, $\\sigma': \\mathbb{R} \\to \\mathbb{R}$ es una función de activación (puede diferir de $\\sigma$) y $\\hat{\\mathbf{x}} \\in \\mathbb{R}^n$ es la **reconstrucción de la entrada**.\n",
    "\n",
    "\n",
    "### **Función de Pérdida**\n",
    "\n",
    "Es importante considerar que la función de pérdida puede variar. En principio, el error cuadrático medio (Mean Squared Error, MSE) es una de las más utilizadas en autoencoders simples; sin embargo, dependiendo de la tarea a realizar, en algunos casos conviene más utilizar una u otra.\n",
    "\n",
    "Para un conjunto de datos \n",
    "\n",
    "$$\n",
    "\\mathcal{D} = \\{\\mathbf{x}_i\\}_{i=1}^{N},\n",
    "$$\n",
    "\n",
    "el entrenamiento del autoencoder minimiza la diferencia entre la entrada $\\mathbf{x}_i$ y la reconstrucción $\\hat{\\mathbf{x}}_i$. Usamos el **Error Cuadrático Medio (MSE)** promediado:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{MSE} = \\frac{1}{N} \\sum_{i=1}^{N}\n",
    "\\left(\n",
    "    \\frac{1}{n} \\sum_{j=1}^{n} \n",
    "    \\bigl(x_{i,j} - \\hat{x}_{i,j}\\bigr)^2\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "donde, $N$ es el número total de muestras, $d$ es la dimensión de cada muestra $\\mathbf{x}_i$, $x_{i,j}$ y $\\hat{x}_{i,j}$ representan la $j$-ésima componente de la muestra $\\mathbf{x}_i$ y de su reconstrucción, respectivamente.\n",
    "\n",
    "<br>\n",
    "<details>\n",
    "<summary>Nota: Sobre algunas otras funciones de pérdida. </summary>\n",
    "\n",
    "El **Error cuadrático medio** es ideal para datos continuos, como imágenes con valores reales. Es fácil de usar y da resultados estables, pero puede generar salidas borrosas porque penaliza fuertemente los errores grandes.\n",
    "\n",
    "**Binary Crossentropy** se usa cuando los datos están entre 0 y 1, como imágenes normalizadas. Funciona bien con activaciones como sigmoide, y modela la probabilidad de cada píxel o bit.\n",
    "\n",
    "La Binary Crossentropy se expresa como:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{BCE} = \n",
    "- \\frac{1}{N \\times d} \\sum_{i=1}^{N} \\sum_{j=1}^{d}\n",
    "\\Bigl[\n",
    "    x_{i,j} \\, \\log\\bigl(\\hat{x}_{i,j}\\bigr) \n",
    "    +\n",
    "    \\bigl(1 - x_{i,j}\\bigr) \\, \\log\\bigl(1 - \\hat{x}_{i,j}\\bigr)\n",
    "\\Bigr],\n",
    "$$\n",
    "\n",
    "donde $ d $ es la dimensión de cada muestra, $ x_{i,j} \\in \\{0,1\\} $ y $ \\hat{x}_{i,j} $ es la probabilidad estimada por el modelo para dicha componente. Esta función de pérdida castiga fuertemente aquellas predicciones en las que $ \\hat{x}_{i,j} $ difiere de $ x_{i,j} $ con alto grado de confianza (debido al uso del logaritmo).\n",
    "\n",
    "\n",
    "**Categorical Crossentropy** es más adecuada cuando la salida son categorías, como texto o etiquetas. \n",
    "Para cuando cada muestra $ \\mathbf{x}_i $ pertenece a una de $ K $ categorías y se representa en formato *one-hot* (solo una de sus $ K $ posiciones es 1, mientras que el resto son 0), tenemos la **Categorical Crossentropy**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{CCE} =\n",
    "- \\frac{1}{N}\n",
    "\\sum_{i=1}^{N}\n",
    "\\sum_{k=1}^{K}\n",
    "x_{i,k} \\,\\log\\bigl(\\hat{x}_{i,k}\\bigr),\n",
    "$$\n",
    "\n",
    "donde $ x_{i,k} $ es 1 si la clase $ k $ es la correcta para la muestra $ \\mathbf{x}_i $, y $ \\hat{x}_{i,k} $ es la probabilidad que el modelo asigna a la clase $ k $. El objetivo en este caso es alinear la distribución pronosticada con la verdadera, penalizando fuertemente cuando la probabilidad de la clase correcta resulta ser baja.\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "### **Optimización**\n",
    "El objetivo es encontrar los parámetros $\\theta$ y $\\phi$ que minimicen la función de pérdida:\n",
    "\n",
    "$$\n",
    "\\theta^*, \\phi^* = \\arg \\min_{\\theta, \\phi} \\,\\mathcal{L}_{MSE}.\n",
    "$$\n",
    "\n",
    "La optimización se resuelve mediante **descenso de gradiente**, por ejemplo usando una tasa de aprendizaje $\\eta$:\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta\\,\\nabla_{\\theta} \\,\\mathcal{L}_{MSE},\n",
    "\\quad\n",
    "\\phi \\leftarrow \\phi - \\eta\\,\\nabla_{\\phi} \\,\\mathcal{L}_{MSE}.\n",
    "$$\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>Nota: Sobre la optimizacion</summary>\n",
    "\n",
    "Hay diferentes formas de optimizacion. \n",
    "</details>\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sparse Autoencoder**\n",
    "\n",
    "Los sparse autoencoders son útiles para interpretar modelos Transformer porque permiten identificar representaciones latentes significativas y más fácilmente interpretables. Al forzar que solo una pequeña parte del espacio latente se active para cada ejemplo, el modelo tiende a representar conceptos más específicos y dispersos. Esto facilita analizar qué tipo de información se activa internamente en respuesta a una entrada, ayudando a entender mejor cómo el Transformer organiza y procesa los datos.\n",
    "\n",
    "Los sparse autoencoders respetan la estructura del \"autoencoder simple\" y simplemente se añade una **función de penalización** que fomenta activaciones promedio bajas en la capa latente, lo que provoca la dispersión. \n",
    "Además, es importante considerar que el **espacio latente** puede ser distinto al de los autoencoders simples. El espacio latente puede llegar a ser mayor que el tamaño de la entrada, pero esto dependerá del tipo de datos con los que se esté trabajando. Si los datos tienen una estructura común o baja complejidad, entonces requerirán un espacio latente más pequeño que la entrada. Por otro lado, si los datos son más complejos o variados, requerirán un espacio latente más grande.\n",
    "Aunque en la mayoría de los SAEs usados en interpretabilidad de LLMs (Large Language Models) el tamaño del espacio latente suele ser mayor que el de la entrada. \n",
    "\n",
    " **Nota: anotar el caso de la dimensionalidad para el ejemplo a abordar.** \n",
    "\n",
    "<details>\n",
    "<summary> Nota: Información extra sobre las función de ponalización </summary>\n",
    "\n",
    "Las funciones de penalización que inducen dispersión tienen en común que su objetivo es restringir el soporte efectivo del vector de representación latente, es decir, promover que la mayoría de sus componentes sean iguales o cercanos a cero. Para lograr esto, se introduce un término adicional en la función de pérdida que aumenta su valor cuando el número o magnitud de componentes diferentes de cero en la representación latente crece.\n",
    "\n",
    "La condición de diferenciabilidad que suelen tener las funciones de penalización utilizadas en aprendizaje automático no es una necesidad teórica, sino una conveniencia computacional. En teoría, se pueden utilizar funciones no diferenciables como la norma L₀ exacta para inducir dispersión, ya que definen perfectamente bien el problema de optimización. Sin embargo, en la práctica, los métodos estándar como el descenso de gradiente requieren calcular derivadas, por lo que se favorecen penalizaciones suaves y diferenciables que permitan aplicar este tipo de algoritmos de forma eficiente.Por lo tanto, es totalmente válido prescindir de esta restricción si se está dispuesto a trabajar con enfoques diferentes, aunque estos podrían no ser óptimos desde el punto de vista computacional.\n",
    "\n",
    "</details>\n",
    "\n",
    "### **Funciones de Penalización**\n",
    "\n",
    "\n",
    "**Divergencia KL**\n",
    "\n",
    "   Se define $\\rho$ como la activación deseada. La desviación de $\\hat{\\rho}_j$ respecto a $\\rho$ se mide con la **Divergencia KL**:\n",
    "\n",
    "   $$\n",
    "   \\mathrm{KL}\\bigl(\\rho \\,\\|\\, \\hat{\\rho}_j\\bigr)\n",
    "   =\n",
    "   \\rho \\,\\log \\frac{\\rho}{\\hat{\\rho}_j}\n",
    "   \\;+\\;\n",
    "   (1-\\rho)\\,\\log \\frac{1-\\rho}{\\,1-\\hat{\\rho}_j}.\n",
    "   $$\n",
    "\n",
    "   Donde la activación promedio de la neurona $j$ es:\n",
    "\n",
    "   $$\n",
    "   \\hat{\\rho}_j = \\frac{1}{N}\\sum_{i=1}^N z_{i,j}.\n",
    "   $$\n",
    "\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary> Nota: Sobre la divergencia KL </summary>\n",
    "\n",
    "\n",
    "**Definición General de la Divergencia KL**\n",
    "\n",
    "La divergencia KL entre dos distribuciones de probabilidad $P(x)$ y $Q(x)$ se define como:\n",
    "\n",
    "$$\n",
    "D_{KL}(P \\,\\|\\, Q) \\;=\\; \\sum_{x} P(x)\\,\\log\\!\\Bigl(\\tfrac{P(x)}{Q(x)}\\Bigr).\n",
    "$$\n",
    "\n",
    "Donde, $P(x)$ es la distribución de referencia, $Q(x)$ es la distribución que usamos para aproximar a $P(x)$, La divergencia KL mide cuánta información se pierde cuando usamos $Q(x)$ en lugar de $P(x)$.\n",
    "\n",
    "El principal objetivo de la divergencia de Kullback-Leibler (KL) es medir cuánta información se pierde cuando usamos una distribución de probabilidad 𝑄 para aproximar otra distribución P. En otras palabras, mide la diferencia entre dos distribuciones de probabilidad y nos dice cuánto nos alejamos de la distribución \"verdadera\" al usar una aproximación.\n",
    "\n",
    "\n",
    "-- Imagen del articulo pendiente:  Solomon Kullback y Richard A. Leibler en su artículo de 1951: \"On Information and Sufficiency\".\n",
    "\n",
    "\n",
    "**Divergencia KL con Bernoulli**\n",
    "\n",
    "Cuando $P$ y $Q$ son distribuciones de Bernoulli con parámetros $\\rho$ y $\\hat{\\rho}_j$, respectivamente, la variable aleatoria $X$ solo puede tomar los valores $0$ o $1$. Entonces:\n",
    "\n",
    "- Para $X = 1$:\n",
    "  \n",
    "  $$\n",
    "  P(1) = \\rho, \\quad Q(1) = \\hat{\\rho}_j.\n",
    "  $$\n",
    "  \n",
    "- Para $X = 0$:\n",
    "  \n",
    "  $$\n",
    "  P(0) = 1 - \\rho, \\quad Q(0) = 1 - \\hat{\\rho}_j.\n",
    "  $$\n",
    "\n",
    "Aplicamos la definición de la divergencia KL:\n",
    "\n",
    "$$\n",
    "D_{KL}(P \\,\\|\\, Q) \n",
    "= \\sum_{x \\in \\{0,1\\}} P(x) \\log \\frac{P(x)}{Q(x)}.\n",
    "$$\n",
    "\n",
    "entonces,\n",
    "\n",
    "$$\n",
    "D_{KL}(\\text{Bern}(\\rho) \\,\\|\\, \\text{Bern}(\\hat{\\rho}_j))\n",
    "=\n",
    "\\rho \\,\\log\\!\\Bigl(\\tfrac{\\rho}{\\hat{\\rho}_j}\\Bigr)\n",
    "\\;+\\;\n",
    "(1-\\rho)\\,\\log\\!\\Bigl(\\tfrac{1-\\rho}{1-\\hat{\\rho}_j}\\Bigr).\n",
    "$$\n",
    "\n",
    "Esto nos da la divergencia KL específica para dos distribuciones Bernoulli.\n",
    "\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "**Penalización $L_0$**\n",
    "\n",
    "Otra funcion de penalizacion en la que nos centraremos por su uso practico en la aplicación de este proyecto es la  regulacion **$L_0$**. La penalización $L_0$ (a veces denominada “norma $L_0$”) se define como el número de elementos distintos de cero en un vector.  \n",
    "Si $z_i \\in \\mathbb{R}^m$ es el vector de activaciones de la capa oculta para la muestra $i$, entonces la “norma” $L_0$ de $z_i$ se expresa como:\n",
    "\n",
    "$$\n",
    "\\| z_i \\|_0 \\;=\\; \\bigl|\\{\\,j : z_{i,j} \\neq 0\\,\\}\\bigr|.\n",
    "$$\n",
    "\n",
    "En otras palabras, $\\| z_i \\|_0$ es simplemente la cantidad de neuronas que están encendidas (activas) en la muestra $i$.  \n",
    "Para todo el conjunto de datos, con $N$ muestras, la penalización $L_0$ se puede escribir de forma compacta como:\n",
    "\n",
    "$$\n",
    "L_0 \\;=\\; \\sum_{i=1}^{N} \\| z_i \\|_0,\n",
    "$$\n",
    "\n",
    "o, de forma más explícita:\n",
    "\n",
    "$$\n",
    "L_0 \\;=\\; \\sum_{i=1}^{N} \\sum_{j=1}^{m} \\mathbf{1}(z_{i,j} \\neq 0),\n",
    "$$\n",
    "\n",
    "vale 1 si la condición se cumple y 0 en caso contrario, $z_{i,j}$ es la activación de la neurona $j$ en la muestra $i$, $N$ es el número de muestras de entrenamiento y $m$ es el número de neuronas en la capa oculta.\n",
    "En la práctica, $L_0$ no es derivable con respecto a los parámetros del modelo, por lo que se suele aproximar o sustituir por otras penalizaciones (como $L_1$), que permiten métodos de optimización basados en gradientes.\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary> Nota: Otras funciones de penalizacion comunes </summary>\n",
    "\n",
    "\n",
    "Otra opción muy usada es la **regularización L1 sobre las activaciones**, que consiste en sumar el valor absoluto de todas las activaciones de la capa oculta:\n",
    "\n",
    "$$\n",
    "\\text{L1} \n",
    "= \\sum_{i=1}^{N} \\sum_{j=1}^{m} \\bigl|\\,z_{i,j}\\bigr|.\n",
    "$$\n",
    "\n",
    "Esta penalización empuja directamente las activaciones a ser lo más cercanas posible a cero, lo cual naturalmente genera *dispersión*.\n",
    "\n",
    "\n",
    "También existe la **regularización L2 sobre activaciones**, que en lugar de tomar el valor absoluto, eleva las activaciones al cuadrado:\n",
    "\n",
    "$$\n",
    "\\text{L2} \n",
    "= \\sum_{i=1}^{N} \\sum_{j=1}^{m} \\bigl(z_{i,j}\\bigr)^2.\n",
    "$$\n",
    "\n",
    "Esta penalización no genera dispersión tan fuerte como L1, pero ayuda a mantener las activaciones controladas.\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "### **Implementacion de la función de penalización**\n",
    "\n",
    "La estructura general de la fución de perdida es la siguiente:\n",
    "$$\n",
    "   \\mathcal{L}_{Sparse}\n",
    "   =\n",
    "   \\mathcal{L}_{MSE}\n",
    "   \\;+\\;\n",
    "   \\text{Función de penalización}$$\n",
    "\n",
    "Basicamente se añade la funcion de penalizacion a la función de perdida.\n",
    "\n",
    "\n",
    "**Ejemplo de como implementar**  \n",
    "   El término de dispersión se agrega al MSE multiplicado por un factor $\\beta$:\n",
    "\n",
    "   $$\n",
    "   \\mathcal{L}_{Sparse}\n",
    "   =\n",
    "   \\mathcal{L}_{MSE}\n",
    "   \\;+\\;\n",
    "   \\beta \\sum_{j=1}^{m} \n",
    "   \\mathrm{KL}\\bigl(\\rho \\,\\|\\, \\hat{\\rho}_j\\bigr).\n",
    "   $$\n",
    "\n",
    "Este término adicional obliga a que la **activación promedio** de cada neurona $\\hat{\\rho}_j$ se acerque a $\\rho$, convirtiendo así un autoencoder normal en un **autoencoder *disperso***.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  **JumpReLU Sparse Autoencoder**\n",
    "\n",
    "El JumpReLU Sparse Autoencoder es una arquitectura diseñada para obtener representaciones dispersas e interpretables de las activaciones internas de modelos de lenguaje grandes, como los transformers. Su propósito es decomponer vectores de activación en combinaciones lineales de un conjunto de direcciones base (features), de manera que solo unas pocas de estas se activen en cada caso, permitiendo análisis más interpretables y eficientes.\n",
    "JumpReLU SAE emplea una activación modificada llamada JumpReLU, que introduce un umbral personalizado para cada feature. Esta pequeña variación permite mejorar tanto la dispersión como la fidelidad de reconstrucción sin sacrificar eficiencia computacional.\n",
    "La importancia esta estructura de Autoencoder para este proyecto es que es la que se utilizara para los experimentos y sobre el modelo Gemma 2 (modelo Trasformer).\n",
    "\n",
    "\n",
    "### **Estructura del modelo**\n",
    "\n",
    "La estructura del modelo JumpReLU Sparse Autoencoder difiere poco del modelo de los Autoencoders Dispersos, y por lo tanto, también de los Autoencoders simples. Hay que notar la similitud entre este modelo y los demás. De esta forma, podemos observar que la diferencia principal está en el tipo de **función de activación del encoder**, su particular **función de penalización** que se integra, y los desafíos que implica incorporar dicha función de penalización. Igualmente, en este caso no se implementará una función de activación en la parte del decoder.\n",
    "\n",
    "\n",
    "### **Codificador**\n",
    "\n",
    "Toma como entrada una activación $x \\in \\mathbb{R}^n$ (proveniente de un transformer) y la proyecta a una representación dispersa $f(x) \\in \\mathbb{R}^m$, con $m > n$:\n",
    "\n",
    "$$\n",
    "\\mathbf{z} = f_{\\theta}(\\mathbf{x}) = \\sigma\\bigl(W_e \\,\\mathbf{x} + \\mathbf{b}_e\\bigr) = \\text{JumpReLU}_\\theta(W_{\\text{e}} x + b_{\\text{e}})\n",
    "$$\n",
    "\n",
    "$\\text{JumpReLU}_\\theta$: función de activación descrita más abajo\n",
    "\n",
    "### **Decodificador**\n",
    "\n",
    "Reconstruye la activación original $x$ a partir de su representación dispersa:\n",
    "\n",
    "$$\n",
    "\\hat{x} = W_{\\text{d}} f(x) + b_{\\text{d}}\n",
    "$$\n",
    "\n",
    "$W_{\\text{d}} \\in \\mathbb{R}^{n \\times M}$: matriz de pesos del decoder y $b_{\\text{d}} \\in \\mathbb{R}^n$: vector de sesgos.\n",
    "\n",
    "La razón por la que se decide evitar el uso de una función de activación en el decodificador es principalmente porque, en la aplicación de este autoencoder a nuestro modelo de lenguaje, la entrada y la salida del autoencoder son vectores reales, ya que representan activaciones internas del modelo. Estas activaciones pueden ser positivas o negativas, y el objetivo del SAE es reconstruir exactamente esas activaciones internas, no convertirlas en otra cosa.\n",
    "\n",
    "### **Función de activación JumpReLU**\n",
    "\n",
    "La función **JumpReLU** actúa como un ReLU con un umbral desplazado hacia la derecha. Se define como:\n",
    "\n",
    "$$\n",
    "\\text{JumpReLU}_\\theta(z_i) = \n",
    "\\begin{cases}\n",
    "z_i & \\text{si } z_i > \\theta_i \\\\\n",
    "0 & \\text{en otro caso}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "O de forma compacta:\n",
    "\n",
    "$$\n",
    "\\text{JumpReLU}_\\theta(z) = z \\cdot H(z - \\theta)\n",
    "$$\n",
    "\n",
    "donde $H(\\cdot)$ es la función escalón de Heaviside:\n",
    "\n",
    "$$\n",
    "H(a) =\n",
    "\\begin{cases}\n",
    "1 & \\text{si } a > 0 \\\\\n",
    "0 & \\text{si } a \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Esta función permite activar solo aquellos razgos cuya preactivación supere cierto umbral, evitando así \"falsas activaciones\" y mejorando la selectividad del encoder.\n",
    "\n",
    "\n",
    "### **Función de pérdida**\n",
    "\n",
    "El entrenamiento del JumpReLU SAE se realiza minimizando una función de pérdida.\n",
    "La función de pérdida es:\n",
    "\n",
    "$$\n",
    "L(x) = \\underbrace{\\|x - \\hat{x}(f(x))\\|_2^2}_{\\text{Error de reconstrucción}} + \\lambda \\cdot \\underbrace{\\|f(x)\\|_0}_{\\text{Penalización de dispersión (L0)}}\n",
    "$$\n",
    "\n",
    "Esta parte ya está más desallada en el apartado de las funciones de perdida. \n",
    "\n",
    "\n",
    "### **Entrenamiento con funciones no diferenciables**\n",
    "\n",
    "La penalización $L_0$ y la función escalón $H(z - \\theta)$ **no son diferenciables**, lo que complica el uso de backpropagation estándar. Para solucionarlo, se usa una técnica llamada **Straight-Through Estimator (STE)**, que permite calcular **gradientes aproximados** y así entrenar el modelo usando métodos convencionales de optimización.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
