{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Danos una introducción a los fenómenos que la interpretabilidad mecanicista trata de explicar, en concreto cosas como\n",
    "\n",
    "    Las neuronas monosemánticas como\n",
    "        La neurona de sentimientos que mencionaron Hinton\n",
    "        La neurona de Donal Trump que Chris Olah mencionó en el podcast de Lex es común en muchas inteligencias artificiales de la época\n",
    "\n",
    "    Los resultados de word2vec\n",
    "\n",
    "    Las definiciones de:\n",
    "\n",
    "    Características\n",
    "\n",
    "    Monosemanticidad\n",
    "\n",
    "    Polisemanticidad\n",
    "\n",
    "    La teoría de la representación lineal\n",
    "\n",
    "    Superposición\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretabilidad Mecanicista\n",
    "Las redes neironales tienen la habilidad (bastante asaombrosa) para convertir el\n",
    "significado en numeros. Los datos fluyen desde la entrada hasta la salida, \n",
    "siendo empujados a través de una serie de transformaciones que procesan los \n",
    "datos en vectores cada vez más complejos. Estos números, las activaciones de la\n",
    "red, llevan información útil de una capa a la red siguiente, y se cree que\n",
    "representan los datos en diferentes capas de abstracción. Pero los vectores en \n",
    "sí han desafiado hasta ahora la interpretación.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La interpretabilidad trata de explicar las relación causs y efecto del algoritmo\n",
    "y saber cómo variará la salida ante cambios en la entrada o en los parámetros. \n",
    "La explicabilidad proporciona un conocimiento del funcionamiento interno del \n",
    "algoritmo y aclara el porqué de su comportamiento. \n",
    "\n",
    "Con los modelos tradicionales de deep learning  como las RNNs, LSTMs o CNNs es \n",
    "difícil predecir la salida del modelo ante cambios en la entrada y también \n",
    "explicar con detalle el funcionamiento interno, más allá de la función \n",
    "proporcionada por cada una de las capas y las simetrías del modelo.\n",
    "\n",
    "En los mecanismos diferenciables de atención permite al modelo seq2seq (modelo \n",
    "de aprendizaje profundo usado particularmente en modelos de traducción \n",
    "automática y procesamiento de lenguaje natural) suele centrarse en unos \n",
    "elementos de la secuencia de entrada más que en otros.\n",
    "\n",
    "Como se puede ver en la ecuación, los coeficientes de atención relacionan cada \n",
    "elemento de la entrada codificada por el encoder con la salida en el instante t.\n",
    "$$\n",
    "c(t) = \\sum_{i=1}^T \\alpha_{ti} h(i)\n",
    "$$\n",
    "donde: \n",
    "- $c(t)$ es el contexto en el instante t, que se calcula como una combinación \n",
    "ponderada de los estados ocultos.  \n",
    "- $\\alpha_{ti}$ son los coeficientes de atención que indican la importancia \n",
    "relativa de cada estado oculto en el instante t. Estos valores se calculan con \n",
    "una función de atención y suman 1, es decir, son probabilidades.  \n",
    "- $h(i)$ representa los estados ocultos del encoder en el instante i. Contienen \n",
    "información de la secuencia de entrada.  \n",
    "- $T$ es la longitud de la secuencia de entrada.  \n",
    "\n",
    "Los coeficientes de atención aportan cierta interpretabilidad al modelo, ya que \n",
    "permiten analizar la relación entre los elementos de la entrada y la salida. \n",
    "Basándose en esta característica, se han realizado estudios para evaluar si el \n",
    "mecanismo de atención es completamente interpretable. Los resultados indican que,\n",
    "si bien la atención puede ayudar a identificar parcialmente la importancia de \n",
    "cada elemento de la entrada, su interpretabilidad presenta ciertas limitaciones.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Superposición \n",
    "La superposición es cuando un modelo representa mása de $n$ características en \n",
    "un espacio de activación dimensional. Es decir, las características todavía \n",
    "corresponden a direcciones, pero el conjunto de direcciones interpretables es \n",
    "mayor que el número de dimensiones. Intutivamente, es un modelo que simula un\n",
    "modelo más grande.\n",
    "Una representación lineal exhibe superposición si $W^TW$ no es invertible. Si\n",
    "$W^TW$ es invertible, no representa superposición.\n",
    "\n",
    "Hay dos tipos de superposición que vale la pena preocuparse en un Transformer:\n",
    "(estos son cómo pienso en ello, pero no son notación estándar)\n",
    "\n",
    "* Superposición de cuello embotellado. Esto es cuando una dimensión de cuello \n",
    "de botella experimenta superposición( i.e keys, queries y flujo residual, etc).\n",
    "    * Esto no es muy sorprendente. Si hay 50,000 fichasa en el vocabulario y 768\n",
    "     dimensiones en la corriente residual, casi tiene que haber más\n",
    "     características que dimensiones, y por lo tanto la superposición.\n",
    "    * La superposición de cuello de botella se utiliza para el almacenamiento, \n",
    "    las dimensiones del cuello de borella son estados intermedios de mapas \n",
    "    lineales y no esperamos que estén haciendo cálculos significativos.\n",
    "* Superposición de la neurona: Esto es cuando las activaciones de neuronas \n",
    "experimentan superposición. Hay más características representadas en el espacio \n",
    "de activación de la neurona que en las neuronas.\n",
    "    * Intuitivamente, la superposición de neuronas representa hacer computación \n",
    "    en la superposición - usando $n$ no linealidades para hacer algún \n",
    "    procesamiento que produce $n$ características.\n",
    "\n",
    "## Polisemanticidad\n",
    "La polisemanticidad es idea de que la activación de una sola neurona corresponde\n",
    "a múltiples características. Empíricamente podríamos observar que, una neurona\n",
    "se activa en múltiples cúmulos de cosas aparentemente no relacionadas como \n",
    "imágenes de dados y fotos  de poetas.\n",
    "* La superposición de una neurona implica polisemanticidad (ya que hay más \n",
    "características que las neuronas), pero no al revés. Podría haber una base \n",
    "interpretable de características simplemente no la base estándar- esto crea \n",
    "polisemanticidad, pero no superposición.\n",
    "* Por el contrario una nuerona es **monosemántica** si corresponde a una \n",
    "sola función.\n",
    "En la práctica, los estándares para llamar a una neurona monosemántica son algo\n",
    "difusos y no es una elección binaria, pues si una neurona se activa fuertemente\n",
    "para una sola función, pero se activa un poco en un montón de otras \n",
    "características, probablemente se le llame monosemántico.\n",
    "\n",
    "\n",
    "Entonces, la superposición es una forma de comprensión perdida. El modelo es \n",
    "capaz de representar más características, pero a costa de añadir ruido e \n",
    "interferencia entre características. Los modelos necesitan encontrar un equilibrio\n",
    "óptimo de puntos  entre los dos, y es verosímil que el punto óptmo no será la\n",
    "superposición cero.\n",
    "\n",
    "## Representaciones de características\n",
    "\n",
    "El problema de la interpretabilidad  es la idea de que las cosas pueden ponerse extrañas y confusas al examinar los sistemas de alta dimensión pues las\n",
    "activaciones de la red neuronal viven en un espacio dimensional muy alto y los \n",
    "pesos se encuentran en un espacio dimensional aín más alto por lo que se \n",
    "necesita una forma de romper el objeto de alta dimensión en pedazos de dimensiones\n",
    "inferiores.\n",
    "La principal manera es encontrar una manera de descomponer las activaciones \n",
    "internas del modelo en **características** y utilizar esto para descomponer los\n",
    "pesos en circuitos que conenctan las caracteristicas.\n",
    "Hay dos aspectos claves de una **característica**:\n",
    "* Importancia:  ¿qué tan útil es para lograr una pérdida más baja? Las \n",
    "características importantes son más útiles para representar, y la interferencia \n",
    "con ellos es más cara.\n",
    "* **Dispersión:** es con qué frecuencia está en la entrada.  Controlar la \n",
    "importancia, si una característica es dispersa interferirá con otras características\n",
    "menos.\n",
    "Como las características son igualmente importantes, se asume que $I_i=1$.\n",
    "Una forma conveniente de medir el número de características que el modelo ha \n",
    "aprendido es mirar la norma de Frobenius $ \\|W\\|_{F}^{2}$. Si $\\|W_{i}\\|^{2} \\simeq 1$\n",
    "entonces la característica está representada  y si $\\|W_{i}\\|^{2} \\simeq 0$ no \n",
    "lo está.\n",
    "\n",
    "## Hipotesis de la representación lineal\n",
    "Es la hipotesis de que las características se representan en el modelo como\n",
    "direcciones en el espacio de activación. Esto significa que se pueden recuperar \n",
    "proyectándolas sobre la dirección pertinente. La intuición detras de esto\n",
    "es que lo principal que un modelo es capaz de hacer es el álgebra lineal.\n",
    "$$\\mathrm{Proj}_{\\vec{f}}(\\vec{v}) = \\frac{\\mathbf{v} \\cdot \\mathbf{f}}{\\|\\mathbf{f}\\|^2}\\mathbf{f}$$\n",
    "\n",
    "Dada esta capacidad, si una capa posterior quiere acceder a una característica puede\n",
    "proyectar en esa dirección de característica, una neurona puede acceder fácilmente\n",
    "y combinar múltiples características, las características pueden variar independientemente\n",
    "y el componente de esa caracteristica representa la fuerza de es característica.\n",
    "Esto nos introduce al concepto de **base interpretable** que es un conjunto\n",
    "de direcciones en el espacio de activación donde cada dirección corresponde a alguna\n",
    "característica interpretable. En el sentido débil, esto significa un conjunto de\n",
    "direcciones donde esperamos que cada uno sea una característica interpretable, \n",
    "pero no necesariamente sabemos lo que es. \n",
    "Por otro lado una base privilegiada es aquella cuyas coordenadas tienen algún\n",
    "significado que las coordenadas de manera arbitraria no tienen. No significa \n",
    "necesariamente que se trate de una base interpretable.\n",
    "\n",
    "## Características como neurona\n",
    "Es la hipótesis más especifica de que, no sólo las características corresponden a \n",
    "direcciones, sino que cada característica corresponde a una neurona especifica, \n",
    "y que la activación de la neurona es la fuerza de esa característica en esa \n",
    "entrada.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
