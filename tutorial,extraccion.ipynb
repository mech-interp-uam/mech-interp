{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción de activaciones de la MLP en la capa 8 de LLaMA 3.2\n",
    "\n",
    "Este tutorial explica detalladamente el proceso para extraer activaciones de \n",
    "la capa **MLP 8** del modelo **LLaMA 3.2**, simulando el modo de \n",
    "preentrenamiento autoregresivo y guardando dichas activaciones como un dataset \n",
    "en Hugging Face.\n",
    "\n",
    "---\n",
    "\n",
    "##  Objetivo general\n",
    "\n",
    "Capturar las activaciones (vectores de salida) de la **capa MLP 8** de LLaMA \n",
    "3.2 para utilizarlas como dataset de entrada en el entrenamiento de un \n",
    "**autoencoder**.\n",
    "\n",
    "---\n",
    "\n",
    "##  Fundamento teórico\n",
    "\n",
    "En lugar de generar tokens uno a uno, alimentamos el modelo con secuencias \n",
    "completas y extraemos las salidas de la capa MLP correspondiente a todos los \n",
    "tokens en una pasada por lote. Esta técnica simula el comportamiento del \n",
    "modelo durante su preentrenamiento autoregresivo.\n",
    "\n",
    "Para una secuencia de tokens $t_0, t_1, \\dots, t_n$, se generan internamente \n",
    "los logits para $t_1, t_2, \\dots, t_{n+1}$, y podemos interceptar estos \n",
    "valores con *hooks*.\n",
    "\n",
    "---\n",
    "\n",
    "##  Requisitos técnicos\n",
    "\n",
    "- GPU con al menos **24 GB de VRAM** (ej. RTX 3090 / 4090 / A6000), \n",
    "provenientes de la plataforma vast.ai, debidamente configurago con su llave\n",
    "- Acceso a Hugging Face con token autorizado al modelo y con permiso de write.\n",
    "- Paquetes instalados: `transformers`, `datasets`, `torch`, `numpy`, \n",
    "`huggingface_hub`, entre otros.\n",
    "\n",
    "---\n",
    "\n",
    "##  Pasos\n",
    "\n",
    "### 1. Autenticación y carga del modelo\n",
    "\n",
    "```python\n",
    "from huggingface_hub import login\n",
    "# Importa la función login para autenticarte con Hugging Face.\n",
    "login(\"TU_TOKEN_AQUI\") \n",
    "# Inicia sesión con tu token personal (reemplaza \"TU_TOKEN_AQUI\").\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# Importa:\n",
    "# AutoModelForCausalLM: carga modelos de lenguaje autoregresivo.\n",
    "# AutoTokenizer: carga el tokenizador correspondiente.\n",
    "import torch\n",
    "# Importa PyTorch para manejar tensores y modelos.\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "# Define el nombre del modelo que se va a usar.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "# Carga el tokenizador desde Hugging Face usando autenticación.\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Establece el token de padding como el token de fin de secuencia.\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    use_auth_token=True\n",
    ")\n",
    "#Carga el modelo LLaMA 3.2-1B desde Hugging Face con los siguientes argumentos:\n",
    "# torch_dtype=torch.float16: utiliza media precisión para reducir el uso de \n",
    "# memoria y acelerar los cálculos en GPU.\n",
    "#device_map=\"auto\": distribuye automáticamente las partes del modelo entre los \n",
    "# dispositivos disponibles (por ejemplo, GPU si existe).\n",
    "# use_auth_token=True: permite acceder al modelo si está protegido o es \n",
    "# privado, utilizando el token ya cargado.\n",
    "model.eval()\n",
    "# Coloca el modelo en modo evaluación, lo que desactiva componentes como \n",
    "# Dropout. Esto garantiza que los resultados sean deterministas durante la \n",
    "# inferencia.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Inserción del hook\n",
    "\n",
    "```python\n",
    "mlp_outputs = []\n",
    "# Inicializa una lista vacía donde se almacenarán las salidas capturadas \n",
    "# del módulo MLP de una capa específica del modelo.\n",
    "def capture_mlp_output(module, input, output):\n",
    "    mlp_outputs.append(output.detach().cpu())\n",
    "# Define una función hook que captura la salida del módulo MLP:\n",
    "# output.detach() evita que se mantenga el grafo computacional.\n",
    "# .cpu() mueve el tensor a la memoria de CPU para facilitar el análisis y \n",
    "# reducir uso de memoria en GPU.\n",
    "layer_idx = 8\n",
    "# Selecciona el índice de la capa del modelo de la cual se desea capturar \n",
    "# la salida del MLP. En este caso, es la capa 8.\n",
    "mlp_module = model.model.layers[layer_idx].mlp\n",
    "# Accede directamente al módulo MLP de la capa especificada.\n",
    "hook_handle = mlp_module.register_forward_hook(capture_mlp_output)\n",
    "# Registra un hook de avance (forward_hook) en el MLP de la capa seleccionada. \n",
    "# Este hook ejecutará la función capture_mlp_output cada vez que se realice \n",
    "# una pasada hacia adelante por esa capa.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Descarga y preparación del dataset\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "# Importa la función load_dataset de la librería datasets de Hugging Face, \n",
    "# que permite cargar datasets públicos con una sola línea de código.\n",
    "dataset = load_dataset(\n",
    "    \"oscar\",\n",
    "    \"unshuffled_deduplicated_es\",\n",
    "    split=\"train[:1%]\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "# Carga una porción del dataset OSCAR (una colección masiva de textos web \n",
    "# multilingües). Se especifica:\n",
    "# \"oscar\": el nombre del dataset.\n",
    "# \"unshuffled_deduplicated_es\": la versión en español sin duplicados.\n",
    "# split=\"train[:1%]\": se selecciona solo el 1% del conjunto de entrenamiento \n",
    "# para reducir el tamaño.\n",
    "# trust_remote_code=True: permite ejecutar código personalizado del dataset si \n",
    "# existe.\n",
    "texts = [item[\"text\"] for item in dataset if item[\"text\"].strip() != \"\"]\n",
    "# Extrae todos los textos del dataset asegurándose de filtrar entradas vacías \n",
    "# (aquellas donde \"text\" es una cadena vacía o solo contiene espacios).\n",
    "\n",
    "import json\n",
    "# Importa el módulo json para trabajar con archivos en formato JSON.\n",
    "with open(\"textos_oscar_1porciento.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for text in texts:\n",
    "        f.write(json.dumps({\"text\": text}) + \"\\n\")\n",
    "# Crea un archivo en formato .jsonl (JSON Lines), donde cada línea contiene un \n",
    "# texto independiente en formato JSON:\n",
    "# open(..., \"w\"): abre el archivo en modo escritura.\n",
    "# encoding=\"utf-8\": asegura compatibilidad con caracteres especiales del \n",
    "# español.\n",
    "# json.dumps({\"text\": text}): convierte cada texto a una estructura JSON.\n",
    "# f.write(... + \"\\n\"): escribe cada texto como una línea separada en el archivo \n",
    "# de salida.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Tokenización y procesamiento por lotes\n",
    "\n",
    "```python\n",
    "from torch.utils.data import DataLoader\n",
    "# Importa DataLoader desde PyTorch, que permite procesar los datos en pequeños \n",
    "# lotes (batches) de manera eficiente durante el entrenamiento o inferencia.\n",
    "import os\n",
    "import numpy as np\n",
    "# Importa los módulos estándar os para manipulación de archivos y numpy para \n",
    "# operaciones numéricas si se requieren más adelante.\n",
    "batch_size = 4\n",
    "max_length = 256\n",
    "output_dir = \"activaciones_mlp8\"\n",
    "# Se define:\n",
    "# batch_size: número de textos que se procesarán simultáneamente.\n",
    "# max_length: longitud máxima de tokens por texto.\n",
    "# output_dir: nombre del directorio donde se guardarán los archivos de salida.\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "# Crea el directorio de salida si aún no existe, evitando errores si ya está \n",
    "# presente.\n",
    "texts = []\n",
    "with open(\"textos_oscar_1porciento.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        texts.append(json.loads(line)[\"text\"])\n",
    "# Lee el archivo .jsonl creado previamente, línea por línea, cargando cada \n",
    "# texto desde su formato JSON y agregándolo a la lista texts.\n",
    "\n",
    "def collate_fn(batch_texts): # # Función de tokenización por lote\n",
    "    return tokenizer(batch_texts, return_tensors=\"pt\", padding=True, \n",
    "    truncation=True, max_length=max_length)\n",
    "# Define una función personalizada collate_fn para preparar cada batch:\n",
    "# Convierte una lista de textos en tensores PyTorch (return_tensors=\"pt\").\n",
    "# Aplica padding para igualar longitud dentro del batch.\n",
    "# Aplica truncamiento si un texto excede max_length.\n",
    "loader = DataLoader(texts, batch_size=batch_size, collate_fn=collate_fn)\n",
    "# Crea el DataLoader, que generará lotes de textos tokenizados utilizando \n",
    "# la función collate_fn y el tamaño de batch especificado.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Extracción de activaciones\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "#Inicia un contexto en el que se desactiva el cálculo del gradiente. Esto \n",
    "# reduce el consumo de memoria y mejora la velocidad, ya que no es necesario el \n",
    "# entrenamiento del modelo.\n",
    "    for i, batch in enumerate(loader):\n",
    "    # Itera sobre los lotes del DataLoader, extrayendo el índice i del lote y \n",
    "    # su contenido batch.\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "        # Envía todos los tensores del lote al mismo dispositivo en el que \n",
    "        # está el modelo (por ejemplo, la GPU) para asegurar compatibilidad.\n",
    "        mlp_outputs.clear()\n",
    "        # Limpia la lista mlp_outputs para evitar que se acumulen las salidas \n",
    "        # de lotes anteriores.\n",
    "        _ = model(**batch)\n",
    "        # Ejecuta una pasada hacia adelante del modelo con el lote actual. \n",
    "        # Gracias al hook, se captura la salida del MLP de la capa 8 en \n",
    "        # mlp_outputs.\n",
    "\n",
    "        for j, tensor in enumerate(mlp_outputs):\n",
    "            # Itera sobre cada tensor capturado del lote actual, junto con su \n",
    "            # índice j.\n",
    "            np.save(os.path.join(output_dir, f\"lote_{i*batch_size + j:06d}.npy\")\n",
    "            , tensor.numpy())\n",
    "            # Guarda cada tensor como un archivo .npy en el directorio \n",
    "            # output_dir. El nombre del archivo incluye el índice global del \n",
    "            # texto procesado (i * batch_size + j) con relleno de ceros para \n",
    "            # mantener el orden.\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\" Procesados y guardados {(i+1)*batch_size:,} textos\")\n",
    "        # Cada 10 lotes, imprime un mensaje indicando cuántos textos han sido \n",
    "        # procesados y guardados hasta ese momento.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##  Subida del dataset a Hugging Face Hub\n",
    "\n",
    "```python\n",
    "from huggingface_hub import HfApi\n",
    "# Crea una instancia de HfApi para acceder a los métodos de la API de Hugging Face.\n",
    "\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"ruta/del/archivo.zip\",\n",
    "    path_in_repo=\"archivo.zip\",\n",
    "    repo_id=\"naraca/mi-dataset-activaciones-llama3_2\",\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "# Sube un archivo ZIP al repositorio en Hugging Face. Parámetros:\n",
    "# path_or_fileobj: ruta local al archivo .zip que se desea subir.\n",
    "# path_in_repo: nombre que tendrá el archivo dentro del repositorio.\n",
    "# repo_id: identificador del repositorio en Hugging Face (en formato \n",
    "# usuario/repositorio).\n",
    "# repo_type=\"dataset\": indica que se trata de un repositorio de tipo dataset.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##  Preguntas frecuentes \n",
    "\n",
    "- ¿Por qué se eligió la **capa MLP 8**?\n",
    "Por tratarse de una capa intermedia donde se suelen observar representaciones \n",
    "lingüísticas generalizadas útiles para tareas posteriores como compresión \n",
    "(autoencoders).\n",
    "\n",
    "- ¿Qué representan los vectores?\n",
    "Las activaciones internas de la MLP para cada token, una codificación \n",
    "semántica útil para analizar y comprimir el conocimiento interno del modelo.\n",
    "\n",
    "- ¿Qué simula este proceso?\n",
    "El modo de preentrenamiento, donde el modelo es alimentado con secuencias \n",
    "completas para predecir el próximo token sin usar generación paso a paso.\n",
    "\n",
    "---\n",
    "\n",
    "## Checklist de implementación\n",
    "\n",
    "- [x] Carga de modelo/tokenizador\n",
    "- [x] Descarga y preprocesamiento del corpus\n",
    "- [x] Inserción del hook en la capa MLP 8\n",
    "- [x] Extracción por lotes y guardado\n",
    "- [x] Dataset con `.npy` por texto\n",
    "- [x] Subida al repositorio Hugging Face\n",
    "\n",
    "---\n",
    "\n",
    "##  Estructura del dataset generado\n",
    "\n",
    "```bash\n",
    "activaciones_mlp8/\n",
    "├── lote_000000.npy\n",
    "├── lote_000001.npy\n",
    "├── ...\n",
    "```\n",
    "\n",
    "Cada archivo `.npy` contiene un tensor de forma `(longitud_secuencia, 4096)`.\n",
    "\n",
    "---\n",
    "\n",
    "## Enlace al repositorio del dataset (scrip de mas abajo)\n",
    "\n",
    "https://huggingface.co/datasets/naraca/mi-dataset-activaciones-llama3_2/tree/\n",
    "main\n",
    "\n",
    "---\n",
    "\n",
    "## Recomendaciones finales\n",
    "\n",
    "- Correr con GPU potente (ideal A100/4090).\n",
    "- Usar `torch_dtype=torch.float16` para optimizar VRAM.\n",
    "- Comprimir los archivos `.npy` periódicamente para subirlos como `.zip`.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
