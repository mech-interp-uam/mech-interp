{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de un SAE sobre las salidad y activaciones de la MLP intermedia de llama3.2 1B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Costo de entrenamiento\n",
    "\n",
    "Generalmente, el costo computacional de un LLM está dominado por la evaluación\n",
    "de sus MLPs (referencia a el seminario en una universidad de un ex empleado\n",
    "de anthropic)\n",
    "\n",
    "Una aprocimación simple del costo de entrenamiento de un modelo es\n",
    "\n",
    "$$\n",
    "  6ND \n",
    "$$\n",
    "esto en términos de FLOPs (operaciones de punto flotante).\n",
    "(citar a chinchilla scaling laws)\n",
    "\n",
    "donde $N$ es el número de parámetros y $D$ la cantidad de muestras en el\n",
    "conjunto de entrenamiento.\n",
    "\n",
    "Esto se debe a que, generalmente, cada parámetro actua en una multiplicaciónl\n",
    "y en una suma de punto flotante, dandonos un costo de $2ND$ tan solo en el\n",
    "forward pass. Tipicamente, el costo de el backward pass es el doble del forward\n",
    "pass, haciendo que su costo sea $4ND$. Sumando tenemos el resultado previamente\n",
    "mencionado.\n",
    "\n",
    "Sea $N_l$ el número de parámetros de llama.\n",
    "Como nosotros vamos a entrenar un autoencoder sobre un MLP a la mitad de llama,\n",
    "solo necesitamos evaluar esa primera mitad. Además, no correremos el backward\n",
    "pass sobre los parámetros de llama, pues no buscamos modificarlos, es decir, los\n",
    "mantendrémos fijos. Por esto, tenemos que el FLOPs realizados por tal mitad del\n",
    "modelo llama es\n",
    "\n",
    "$$\n",
    "  N_l D\n",
    "$$\n",
    "\n",
    "En cuanto al SAE, solo considerando el costo de aplicar sus matrices, tenemos\n",
    "\n",
    "$$\n",
    "  6 (2d_\\text{in}d_\\text{sae})D\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de gemmascope, entre todos los SAEs que entrenaron, los más pequeños\n",
    "entrenados en las salidas de las MLPs, se entrenaron en 4 mil millones de\n",
    "vectores de activaciones, con la dimencion de los vectores en el stream\n",
    "recidual (y por lo tanto, de la salida de las capas MLP), es 2048, con\n",
    "$d_\\text{sae} = 2028 * 8$.\n",
    "\n",
    "Deseamos encontrar hiperparámetros para entrenar SAEs, para esto:\n",
    "- Usamos una primera aproximación razonable modificando los hiperparámetros para\n",
    "  los autoencoders más pequeños entrenados en salidas de las MLPs de gemma 2\n",
    "  2 B\n",
    "- Ajustamos una power law en base a 2 entrenamientos de SAEs más pequeñas,\n",
    "  usando el mismo learning rate.\n",
    "- Ajustamos una power law para el learning rate con los hiperparámetos optimos\n",
    "  que estimó el paso anterior.\n",
    "\n",
    "Si ignoramos la relación posicional de los tokens y asumimos una distribución\n",
    "uniforme, tenemos que la entropía por token es\n",
    "\n",
    "$$\n",
    "  \\log_2 (\\text{tamaño del vocabulario})\n",
    "$$\n",
    "ya que el vocabulario de gemma2 2B es 256000 y el de llama es 128000, obtenemos\n",
    "que el número de tokens equivalente sería 4.2 mil millones. Ya que nuestro\n",
    "modelo es la mitad de tamaño de gemma2 2B, una primera cantidad de datos\n",
    "razonable para entrenar nuesto sae más grande es $2.1B$\n",
    "\n",
    "En el caso de llama3.2 1B, eso resultaría en\n",
    "\n",
    "$$\n",
    "  N_l D = (2.1 \\times 10^9)(1.2 \\times 10^9) \\approx 2.5 \\times 10^{18}\n",
    "$$\n",
    "Una RTX 4090 puede realisar cada segundo un máximo de $165 \\times 10^{12}$\n",
    "operaciones con tensores de 16 bits y acumulador de 32 bits (referencia\n",
    "al reporte técnico v1.0.1), luego, estimamos 4.2 horas de entrenamiento\n",
    "tan solo considerando la computación en el modelo llama.\n",
    "\n",
    "Ahora, para estimar las horas-RTX4090 para el autoencoder, en el caso de\n",
    "entrenarlo en la salida de la MLP intermedia, tendríamos\n",
    "\n",
    "$$\n",
    "    6(2.1 \\times 10^9)(2048^2)(8)(2) = 8.5 \\times 10^{17}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\julyv\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: jaxtyping in c:\\users\\julyv\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\julyv\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\julyv\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (4.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\julyv\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\julyv\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\julyv\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\julyv\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (78.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\julyv\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\julyv\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: wadler-lindig>=0.1.3 in c:\\users\\julyv\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jaxtyping) (0.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\julyv\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torch jaxtyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import jaxtyping\n",
    "import dataclasses\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JumpReLU(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, threshold):\n",
    "        ctx.save_for_backward(x, threshold)\n",
    "        return torch.where(x > threshold, x, 0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        bandwidth = 0.001\n",
    "        x, threshold = ctx.saved_tensors\n",
    "        grad_x = torch.where(x > threshold,\n",
    "            torch.ones_like(x), torch.zeros_like(x))\n",
    "        grad_threshold = torch.where(\n",
    "            abs(x - threshold) < bandwidth/2,\n",
    "            - threshold/bandwidth, 0)\n",
    "\n",
    "        return grad_x * grad_output, grad_threshold * grad_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Step(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, threshold):\n",
    "        ctx.save_for_backward(x, threshold)\n",
    "        return (x > threshold).to(x.dtype)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        bandwidth = 0.001\n",
    "        x, threshold = ctx.saved_tensors\n",
    "\n",
    "        grad_threshold = torch.where(\n",
    "            abs(F.relu(x) - threshold) < bandwidth/2,\n",
    "            -1.0/bandwidth, 0)\n",
    "        \n",
    "        return torch.zeros_like(x), grad_threshold * grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sae(nn.Module):\n",
    "    def __init__(self, d_in, d_sae, use_pre_enc_bias=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.enc = nn.Linear(d_in, d_sae, dtype=torch.float16)\n",
    "        self.dec = nn.Linear(d_sae, d_in, dtype=torch.float16)\n",
    "        with torch.no_grad():\n",
    "            # normalize each of the d_sae dictonary vectors\n",
    "            self.dec.weight /= self.dec.weight.norm(dim=0, keepdim=True)\n",
    "        self.enc.weight = self.dec.weight.clone().t()\n",
    "        self.enc.bias = torch.zeros_like(self.enc.bias)\n",
    "        self.dec.bias = torch.zeros_like(self.dec.bias)\n",
    "        self.log_threshold = nn.Parameter(\n",
    "            torch.log(torch.full((d_sae,), 0.001, dtype=torch.float16)))\n",
    "        self.use_pre_enc_bias = use_pre_enc_bias\n",
    "        def project_out_parallel_grad(dim, tensor):\n",
    "            @torch.no_grad\n",
    "            def hook(grad_in):\n",
    "                # norm along dim=dim of the tensor is assumed to be 1 as we\n",
    "                # are going to normalize it after every grad update\n",
    "                dot = (tensor * grad_in).sum(dim=dim, keepdim=True)\n",
    "                return grad_in - dot * tensor\n",
    "            return hook\n",
    "\n",
    "        self.dec.weight.register_hook(\n",
    "            project_out_parallel_grad(0, self.dec.weight))\n",
    "                \n",
    "\n",
    "    def forward(self,\n",
    "        x,\n",
    "        return_mask=False,\n",
    "        return_reconstruction=False,\n",
    "        return_l0=True,\n",
    "        return_reconstruction_loss=True,\n",
    "    ):\n",
    "        \"We compute this much here so that compile() can do its magic\"\n",
    "        # as per train_gpt2.py on karpathy's llm.c repo, there are performance\n",
    "        # reasons not to return stuff\n",
    "        d = {}\n",
    "        original_input = x\n",
    "        if self.use_pre_enc_bias:\n",
    "            x = x - self.dec.bias\n",
    "        \n",
    "        x = self.enc(x)\n",
    "        threshold = torch.exp(self.log_threshold)\n",
    "        s = Step.apply(x, threshold)\n",
    "        if return_mask:\n",
    "            d['mask'] = s\n",
    "        if return_l0:\n",
    "            d['l0'] = s.sum(-1).mean()\n",
    "        x = x*s\n",
    "        x = self.dec(x)\n",
    "        if return_reconstruction:\n",
    "            d['reconstruction'] = ((x - original_input)**2).sum(-1).mean()\n",
    "\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_schedule_with_warmup(\n",
    "    current_step: int,\n",
    "    warmup_steps: int,\n",
    "    total_steps: int\n",
    "    ):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot assign 'torch.HalfTensor' as parameter 'weight' (torch.nn.Parameter or None expected)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m16\u001b[39m\n\u001b[0;32m      2\u001b[0m max_lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7e-5\u001b[39m\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSae\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mmax_lr)\n\u001b[0;32m      6\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mLambdaLR(\n\u001b[0;32m      7\u001b[0m     optimizer,\n\u001b[0;32m      8\u001b[0m     lr_lambda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m step: cosine_schedule_with_warmup(step, warmup_steps, total_tr)\n\u001b[0;32m      9\u001b[0m )\n",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m, in \u001b[0;36mSae.__init__\u001b[1;34m(self, d_in, d_sae, use_pre_enc_bias, **kwargs)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# normalize each of the d_sae dictonary vectors\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mt()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc\u001b[38;5;241m.\u001b[39mbias)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32mc:\\Users\\julyv\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1956\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   1954\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m   1955\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1956\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   1957\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot assign \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mtypename(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as parameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1958\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(torch.nn.Parameter or None expected)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1959\u001b[0m         )\n\u001b[0;32m   1960\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(name, value)\n\u001b[0;32m   1961\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot assign 'torch.HalfTensor' as parameter 'weight' (torch.nn.Parameter or None expected)"
     ]
    }
   ],
   "source": [
    "steps = 2**16\n",
    "max_lr = 7e-5\n",
    "model = Sae(2048, 2048*4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lr_lambda=lambda step: cosine_schedule_with_warmup(step, warmup_steps, total_tr)\n",
    ")\n",
    "for batch in tqdm(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    d = model(batch)\n",
    "    reconstruction_loss, l0 = d['reconstuction_loss'], d['l0']\n",
    "    loss = reconstruction_loss + sparsity_coefficient * l0\n",
    "    # log losses, compute stats, etc\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # TODO: sparsity_coefficient scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # normalize\n",
    "    with torch.no_grad():\n",
    "        model.dec.weight /= model.dec.weight.norm(dim=0, keepdim=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretación de latentes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=API_KEY) if API_KEY else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_activating_examples_gpt(api_key, n=10) -> list[tuple[int, list[str]]]:\n",
    "    \"\"\"\n",
    "    Simula ejemplos de activación máxima en frases relacionadas con comunicación.\n",
    "    Regresa el mismo formato que fetch_max_activating_examples().\n",
    "    \"\"\"\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    system_prompt = \"You are a helpful assistant that writes natural sentences related to the concept of communication.\"\n",
    "    user_prompt = (\n",
    "        f\"Please generate {n} short sentences (10 to 15 words each) about communication. \"\n",
    "        \"Mark exactly one key word per sentence — related to communication — by wrapping it with double angle brackets like this: <<talk>>. \"\n",
    "        \"Do not explain or list anything, just return the marked sentences as plain text, one per line.\"\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        max_tokens=300,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    raw_sentences = response.choices[0].message.content.strip().split(\"\\n\")\n",
    "    raw_sentences = [s.strip().lstrip(\"0123456789. \") for s in raw_sentences if s.strip()]\n",
    "\n",
    "    parsed = []\n",
    "    for i, sentence in enumerate(raw_sentences):\n",
    "        tokens = sentence.split()\n",
    "        for j, tok in enumerate(tokens):\n",
    "            if tok.startswith(\"<<\") and tok.endswith(\">>\"):\n",
    "                tokens[j] = f\"<<{tok.strip('<>')}>>\"\n",
    "                break\n",
    "        parsed.append((i, tokens))\n",
    "\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_gpt4o_from_simulated(examples: list[tuple[int, list[str]]], use_chain_of_thought=True) -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    Construye un prompt de GPT-4o a partir de ejemplos simulados con tokens activadores marcados.\n",
    "    \"\"\"\n",
    "    formatted = \"\\n\".join(f\"{i+1}. {' '.join(tokens)}\" for i, tokens in examples)\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You're analyzing a latent neuron in a neural network trained on text.\\n\"\n",
    "        \"Each example below highlights the token that activates this neuron with << >>.\\n\"\n",
    "        \"Your job is to explain what this neuron responds to, using 20 words or fewer.\\n\"\n",
    "        \"Avoid punctuation, formatting, and long lists. Be specific but not too narrow.\\n\"\n",
    "    )\n",
    "\n",
    "    assistant_prompt = (\n",
    "        \"First, look at the highlighted tokens and try to generalize what they represent.\\n\"\n",
    "        \"Then, give a short final interpretation.\\n\\nThis neuron fires on\"\n",
    "        if use_chain_of_thought else\n",
    "        \"this neuron fires on\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"system\": system_prompt,\n",
    "        \"user\": f\"The activating examples are:\\n\\n{formatted}\",\n",
    "        \"assistant\": assistant_prompt,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt4o_explanation_from_prompt(prompts: dict, n_completions=3, max_tokens=100) -> list[str]:\n",
    "    \"\"\"\n",
    "    Llama a la API de GPT-4o con un prompt ya generado y devuelve las interpretaciones.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompts[\"system\"]},\n",
    "            {\"role\": \"user\", \"content\": prompts[\"user\"]},\n",
    "            {\"role\": \"assistant\", \"content\": prompts[\"assistant\"]},\n",
    "        ],\n",
    "        n=n_completions,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return [choice.message.content.strip() for choice in response.choices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if API_KEY:\n",
    "    simulated_examples = simulate_activating_examples_gpt(api_key=API_KEY, n=10)\n",
    "    prompt = create_prompt_gpt4o_from_simulated(simulated_examples, use_chain_of_thought=True)\n",
    "    explanations = get_gpt4o_explanation_from_prompt(prompt, n_completions=3)\n",
    "\n",
    "    for i, exp in enumerate(explanations):\n",
    "        print(f\"[Explicación {i+1}]: {exp}\")\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY no está configurada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enviar los datos en grupo batching para obtener descuento en el uso de la API"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
